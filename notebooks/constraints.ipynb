{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb0775b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Adding constraints to L96\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1a857-c84c-4fe6-9494-22cc0aeffefd",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Outline\n",
    "\n",
    "The goal of this notebook is to show how a physical or mathematical constraint can be built in to the learning process, to try and nudge the ML model towards more realistic solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c32a3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Problems with constraints in L96\n",
    "\n",
    "The **$1^{st}$ order equation L96 model**, \n",
    "\\begin{equation}\n",
    "\\frac{d}{dt} X_k\n",
    "= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F \n",
    "\\end{equation}\n",
    "conserves energy, \n",
    "$$E_X = \\frac{1}{2}<X_K^2>,$$ \n",
    "in the unforced and undamped form (last 2 terms on the RHS are zero), which results due to the form of the non-linearity. \n",
    "\n",
    "However, the **$2^{nd}$ order equation L96 model**\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F - \\left( \\frac{hc}{b} \\right) \\sum_{j=0}^{J-1} Y_{j,k}\n",
    "\\\\\n",
    "\\frac{d}{dt} Y_{j,k}\n",
    "&= - cbY_{j+1,k} \\left( Y_{j+2,k} - Y_{j-1,k} \\right) - c Y_{j,k} + \\frac{hc}{b} X_k\n",
    "\\end{align}\n",
    "does not have the same conservation property. Instead, for the unforced and undamped system the total energy, \n",
    "$$E = E_X + E_Y = \\frac{1}{2}(<X_K^2> + <Y_j^2>),$$ \n",
    "is conserved, with the \"subgrid tendencies\" being the exchange terms ($<X_kY_{j,k}>$) between the large scale ($\\frac{1}{2}<X_K^2>$) and small scale energy reservoirs $\\frac{1}{2}<Y_j^2>$. \n",
    "\n",
    "Using these to contrain a parameterization is non-trivial for the following reasons: \n",
    "- **We need some more physical insight to use the conservation laws**: It is not obvious how to use the knowledge that there is a total energy conservation to help the parameterization. Some more work into coming up with some physically justified contraint on the exchange terms might be required; e.g. the way the Gent-McWilliams **possibly cite** parameterization forces the subgrid to be an APE sink. \n",
    "- **Track subgrid energy**: Since the parameterized model does not keep track of the subgrid energy, so it is hard to do something when we only know part of the energy term. All these constrains are for the unforced-undamped system. How do we translate them to the forced-damped problem? \n",
    "- **Train based on the evolution of the model state (X)**: The energetic constrains involve both X and Y, but the simple loss functions we are used to at the moment are usually only built in terms of the predicted Ys. It is not clear to me how we would add a constrain saying that \"predict the Ys such that the total future X does not increase\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acaa8ea",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Solution: L96 model with momentum conserving term\n",
    "We created a modified L96, where the coupling term is momentum conserving. \n",
    "This is a bit silly to do, as the L96 model does not conserve momentum (but no one can stop you!). This also changes/breaks the energetics of the original L96. \n",
    "The main reason for this choice was because it was the easiest to implement. \n",
    "\n",
    "The equations of the new model are: \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F - \\left( \\frac{hc}{b} \\right) \\left(\\sum_{j=0}^{J-1} Y_{j,k-1} - \\sum_{j=0}^{J-1} Y_{j,k+1}\\right)\n",
    "\\\\\n",
    "\\frac{d}{dt} Y_{j,k}\n",
    "&= - cbY_{j+1,k} \\left( Y_{j+2,k} - Y_{j-1,k} \\right) - c Y_{j,k} + \\frac{hc}{b} X_k\n",
    "\\end{align}\n",
    "\n",
    "The small scale forcing from the small scale to the large scale is:\n",
    "\\begin{equation}\n",
    "G_K = S_{k-1} - S_{k+1} = - \\left( \\frac{hc}{b} \\right) \\left(\\sum_{j=0}^{J-1} Y_{j,k-1} - \\sum_{j=0}^{J-1} Y_{j,k+1}\\right)\n",
    "\\end{equation}\n",
    " \n",
    "It is clear that summing $G_k$ over the full domain is identically zero, since it is in a flux form. This term does not input any net momentum into the system, $<G_k>=0$. We will use this as a contrain in this exercise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd5a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules for the model\n",
    "from L96_model import (\n",
    "    L96,\n",
    "    L96_eq1_xdot,\n",
    "    integrate_L96_2t,\n",
    ")  # L96_model_XYtend Adds the option to ouptput the subgrid tendencies (effect of Y on X)\n",
    "from L96_model import EulerFwd, RK2, RK4\n",
    "\n",
    "# Note: L96_model_XYtend_conservative2 and L96_model_XYtend_conservative are the same models, but L96_model_XYtend_conservative2 outputs S_k while L96_model_XYtend_conservative outputs G_k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e603f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams[\"font.size\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982eda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the world\n",
    "\n",
    "time_steps = 20000\n",
    "Forcing, dt, T = 12, 0.01, 0.01 * time_steps\n",
    "\n",
    "# Create a \"real world\" with K=8 and J=32\n",
    "K = 8\n",
    "J = 32\n",
    "W = L96(K, J, F=Forcing)\n",
    "\n",
    "# Get training data for the neural network.\n",
    "# - Run the true state and output subgrid tendencies (the effect of Y on X is xytrue):\n",
    "X_true, y, T_true, S_true = W.run(dt, T, store=True, return_coupling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_k = S_(k-1) - S_(k+1)\n",
    "G_true = np.roll(S_true, 1, axis=1) - np.roll(S_true, -1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca46561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to xarray for convenience\n",
    "\n",
    "X_true_xr = xr.DataArray(\n",
    "    X_true, dims=[\"time\", \"K\"], coords={\"time\": T_true, \"K\": np.arange(K)}\n",
    ").rename(\"X\")\n",
    "Y_xr = xr.DataArray(\n",
    "    y, dims=[\"time\", \"JK\"], coords={\"time\": T_true, \"JK\": np.arange(K * J)}\n",
    ").rename(\"Y\")\n",
    "S_true_xr = xr.DataArray(\n",
    "    S_true, dims=[\"time\", \"K\"], coords={\"time\": T_true, \"K\": np.arange(K)}\n",
    ").rename(\"S\")\n",
    "G_true_xr = xr.DataArray(\n",
    "    G_true, dims=[\"time\", \"K\"], coords={\"time\": T_true, \"K\": np.arange(K)}\n",
    ").rename(\"G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(19, 10), dpi=150)\n",
    "\n",
    "plt.subplot(141)\n",
    "X_true_xr.sel(time=slice(0, 30)).plot.contourf(vmin=-16, levels=21)\n",
    "\n",
    "plt.subplot(142)\n",
    "Y_xr.sel(time=slice(0, 30)).plot.contourf(vmin=-2, levels=21)\n",
    "\n",
    "plt.subplot(143)\n",
    "G_true_xr.sel(time=slice(0, 30)).plot.contourf(levels=21)\n",
    "\n",
    "plt.subplot(144)\n",
    "S_true_xr.sel(time=slice(0, 30)).plot.contourf(levels=21)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ac2b8",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The modified system looks fairly organized and chaotic. So the essence of the original L96 still remains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d2aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 4), dpi=150)\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(X_true_xr.sel(K=0), S_true_xr.sel(K=1), \".\", markersize=1)\n",
    "plt.xlabel(\"$X_{k-1}$\")\n",
    "plt.ylabel(\"$S_{k}$\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(X_true_xr.sel(K=1), S_true_xr.sel(K=1), \".\", markersize=1)\n",
    "plt.xlabel(\"$X_{k}$\")\n",
    "plt.ylabel(\"$S_{k}$\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(X_true_xr.sel(K=2), S_true_xr.sel(K=1), \".\", markersize=1)\n",
    "plt.xlabel(\"$X_{k+1}$\")\n",
    "plt.ylabel(\"$S_{k}$\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 4), dpi=150)\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(X_true_xr.sel(K=0), G_true_xr.sel(K=1), \".\", markersize=1)\n",
    "plt.xlabel(\"$X_{k-1}$\")\n",
    "plt.ylabel(\"$G_{k}$\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(X_true_xr.sel(K=1), G_true_xr.sel(K=1), \".\", markersize=1)\n",
    "plt.xlabel(\"$X_{k}$\")\n",
    "plt.ylabel(\"$G_{k}$\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(X_true_xr.sel(K=2), G_true_xr.sel(K=1), \".\", markersize=1)\n",
    "plt.xlabel(\"$X_{k+1}$\")\n",
    "plt.ylabel(\"$G_{k}$\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc3a04e",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Since the dependence of Y on X remains the same, G now depends on the neighbouring points (and not the K itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5), dpi=150)\n",
    "plt.subplot(121)\n",
    "S_true_xr.mean(\"K\").plot()\n",
    "plt.ylabel(\"<S>\")\n",
    "# plt.title('Averaged subgrid term')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(122)\n",
    "G_true_xr.mean(\"K\").plot()\n",
    "plt.ylabel(\"<G>\")\n",
    "# plt.title('Averaged subgrid term')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29380dc3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The momentum input from the subgrid term is essentially zero (to numerical precision), as should be. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa6cd0",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**What should we learn?**  \n",
    "We will do three types of learning (all fully non-local):  \n",
    "a) Learn $G_k$ directly as a function of $X_k$s by minimizing a MSE loss.  \n",
    "b) Learn $G_k$ directly as a function of $X_k$s by minimizing a MSE loss + minimizing $<G_k>$.   \n",
    "c) Learn $S_k$ as a function of $X_k$s, and then $<G_k>$ will be zero by design.  \n",
    "d) Learn $S_k$ but with $G_k$ being the goal. (This will introduce the Gauge problem).\n",
    "\n",
    "*We chose MSE as a loss function because as shown earlier, the learning for L96 is not sensitive to the choice of loss function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split into Training and testing data\n",
    "\n",
    "val_size = 4000  # number of time steps for validation\n",
    "\n",
    "# train:\n",
    "X_true_train = X_true[\n",
    "    :-val_size, :\n",
    "]  # Flatten because we first use single input as a sample\n",
    "subgrid_G_tend_train = G_true[:-val_size, :]\n",
    "subgrid_S_tend_train = S_true[:-val_size, :]\n",
    "\n",
    "# test:\n",
    "X_true_test = X_true[-val_size:, :]\n",
    "subgrid_G_tend_test = G_true[-val_size:, :]\n",
    "subgrid_S_tend_test = S_true[-val_size:, :]\n",
    "\n",
    "# Create non local training data\n",
    "# Define a data loader (8 inputs, 8 outputs)\n",
    "\n",
    "### For the cases where will learn G_k\n",
    "\n",
    "# Define our X,Y pairs (state, subgrid tendency) for the linear regression local network.local_torch_dataset = Data.TensorDataset(\n",
    "torch_dataset = Data.TensorDataset(\n",
    "    torch.from_numpy(X_true_train).double(),\n",
    "    torch.from_numpy(subgrid_G_tend_train).double(),\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 1024  # Number of sample in each batch\n",
    "\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define a test dataloader (8 inputs, 8 outputs)\n",
    "\n",
    "torch_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(X_true_test).double(),\n",
    "    torch.from_numpy(subgrid_G_tend_test).double(),\n",
    ")\n",
    "\n",
    "loader_test = Data.DataLoader(\n",
    "    dataset=torch_dataset_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "### For the cases where will learn S_k\n",
    "\n",
    "# Define our X,Y pairs (state, subgrid tendency) for the linear regression local network.local_torch_dataset = Data.TensorDataset(\n",
    "torch_dataset = Data.TensorDataset(\n",
    "    torch.from_numpy(Xtrue_train).double(),\n",
    "    torch.from_numpy(subgrid_S_tend_train).double(),\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 1024  # Number of sample in each batch\n",
    "\n",
    "loader_S = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define a test dataloader (8 inputs, 8 outputs)\n",
    "\n",
    "torch_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(Xtrue_test).double(),\n",
    "    torch.from_numpy(subgrid_S_tend_test).double(),\n",
    ")\n",
    "\n",
    "loader_S_test = Data.DataLoader(\n",
    "    dataset=torch_dataset_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7554df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class for the neural network architecture\n",
    "class Net_ANN(nn.Module):\n",
    "    def __init__(self, name=\"Default\"):\n",
    "        super(Net_ANN, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs, 16 neurons for first hidden layer\n",
    "        self.linear2 = nn.Linear(16, 16)  # 16 neurons for second hidden layer\n",
    "        self.linear3 = nn.Linear(16, 8)  # 8 outputs\n",
    "        self.name = name\n",
    "\n",
    "    #         self.lin_drop = nn.Dropout(0.1) #regularization method to prevent overfitting.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c136578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup function to take one step in the learning\n",
    "\n",
    "\n",
    "def train_model(net, criterion, trainloader, optimizer):\n",
    "    # set model in to training mode\n",
    "    net.train()\n",
    "\n",
    "    # Loop through all the training subsets (notice we batched in size of 1024)\n",
    "    for step, (batch_x, batch_y) in enumerate(trainloader):  # for each training step\n",
    "        b_x = Variable(batch_x)  # Inputs\n",
    "        b_y = Variable(batch_y)  # outputs\n",
    "\n",
    "        if (\n",
    "            len(b_x.shape) == 1\n",
    "        ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "            prediction = torch.squeeze(\n",
    "                net(torch.unsqueeze(b_x, 1))\n",
    "            )  # input x and predict based on x\n",
    "        else:\n",
    "            prediction = net(b_x)\n",
    "\n",
    "        loss = criterion(prediction, b_y)  # Calculating loss\n",
    "        optimizer.zero_grad()  # clear gradients for next train\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients to update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e24fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup function that evaluates a number of metrics at each training step\n",
    "\n",
    "\n",
    "def test_model(\n",
    "    net, criterion, metric_fun, trainloader, optimizer, text=\"validation\", print_flag=0\n",
    "):\n",
    "    net.eval()  # Evaluation mode (important when having dropout layers)\n",
    "\n",
    "    test_loss = 0\n",
    "    test_metric = 0\n",
    "    test_metric_2 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(\n",
    "            trainloader\n",
    "        ):  # for each training step\n",
    "            b_x = Variable(batch_x)  # Inputs\n",
    "            b_y = Variable(batch_y)  # outputs\n",
    "            if (\n",
    "                len(b_x.shape) == 1\n",
    "            ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "                prediction = torch.squeeze(\n",
    "                    net(torch.unsqueeze(b_x, 1))\n",
    "                )  # input x and predict based on x\n",
    "            else:\n",
    "                prediction = net(b_x)\n",
    "            loss = criterion(prediction, b_y)  # Calculating loss\n",
    "            metric = metric_fun(prediction, b_y)\n",
    "\n",
    "            test_loss = test_loss + loss.data.numpy()  # Keep track of the loss\n",
    "            test_metric = test_metric + metric.data.numpy()  # Keep track of the loss\n",
    "            test_metric_2 = (\n",
    "                test_metric_2\n",
    "                + torch.mean(torch.abs(torch.mean(prediction, axis=1))).data.numpy()\n",
    "            )\n",
    "\n",
    "        test_loss /= len(trainloader)  # dividing by the number of batches\n",
    "        test_metric /= len(trainloader)  # dividing by the number of batches\n",
    "        test_metric_2 /= len(trainloader)\n",
    "    #         print(len(trainloader))\n",
    "    # print(test_loss)\n",
    "\n",
    "    if print_flag == 1:\n",
    "        print(\n",
    "            net.name + \" \" + text + \" loss:\",\n",
    "            str(test_loss) + \", metric_1:\",\n",
    "            str(test_metric) + \", metric_2:\" + str(test_metric_2),\n",
    "        )\n",
    "\n",
    "    return test_loss, test_metric, test_metric_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ML models\n",
    "\n",
    "lr = 0.001  # the learning rate\n",
    "\n",
    "# (a) Learn G_k using MSE\n",
    "criterion_a = torch.nn.MSELoss()\n",
    "nn_a = Net_ANN(\"Learn Gk\").double()\n",
    "optimizer_a = optim.Adam(nn_a.parameters(), lr=lr)\n",
    "\n",
    "# (b) learn G_k using MSE + condition of reducing momentum forcing\n",
    "\n",
    "alpha = 100  # This is the regularization parameter\n",
    "\n",
    "\n",
    "def custom_loss(Ypred, Ytrue):\n",
    "    # loss = MSE + net momentum forcing penalty\n",
    "    loss = torch.mean((Ypred - Ytrue) ** 2) + alpha * torch.mean(\n",
    "        torch.abs(torch.mean(Ypred, axis=1))\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "criterion_b = custom_loss\n",
    "nn_b = Net_ANN(\"Learn Gk w/ constrain\").double()\n",
    "optimizer_b = optim.Adam(nn_b.parameters(), lr=lr)\n",
    "\n",
    "# (c) Learn S_k using MSE\n",
    "criterion_c = torch.nn.MSELoss()\n",
    "nn_c = Net_ANN(\"Learn Sk\").double()\n",
    "optimizer_c = optim.Adam(nn_c.parameters(), lr=lr)\n",
    "\n",
    "# (d) Learn S_k but with G_k as the objective\n",
    "\n",
    "\n",
    "def custom_loss2(Spred, Gtrue):\n",
    "    # Gtrue = np.roll(Strue, 1, axis=1) - np.roll(Strue, -1, axis=1)\n",
    "    Gpred = torch.roll(Spred, 1, dims=1) - torch.roll(Spred, -1, dims=1)\n",
    "    loss = torch.mean((Gpred - Gtrue) ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "criterion_d = custom_loss2\n",
    "nn_d = Net_ANN(\"Learn Sk from Gk\").double()\n",
    "optimizer_d = optim.Adam(nn_d.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4555c47",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The custom loss function: \n",
    "$$\n",
    "LOSS = (G_{pred} - G_{true})^2 + \\alpha |<G_k>|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa56c74",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "In model (b) we have to pick a value for the degree of regularization $\\alpha$. Here we use a constant value, but this value itself needed to be determined. We determined it using some rough testing with the L-curve criterion (read about it more here - https://www.sintef.no/globalassets/project/evitameeting/2005/lcurve.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the training\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "# Containers\n",
    "validation_stat_a = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "train_stat_a = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "\n",
    "validation_stat_b = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "train_stat_b = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "\n",
    "validation_stat_c = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "train_stat_c = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "\n",
    "validation_stat_d = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "train_stat_d = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "\n",
    "\n",
    "def helper_fun(nn_sel, criterion, metric, loader, loader_test, optimizer):\n",
    "    train_model(nn_sel, criterion, loader, optimizer)\n",
    "\n",
    "    return test_model(\n",
    "        nn_sel, criterion, metric, loader, optimizer, text=\"train\"\n",
    "    ), test_model(nn_sel, criterion, metric, loader_test, optimizer, print_flag=1)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"--------------\")\n",
    "    print(\"Epoch num : \" + str(epoch))\n",
    "\n",
    "    # (a)\n",
    "    (\n",
    "        validation_stat_a[\"loss\"][epoch],\n",
    "        validation_stat_a[\"metric\"][epoch],\n",
    "        validation_stat_a[\"metric2\"][epoch],\n",
    "    ), (\n",
    "        train_stat_a[\"loss\"][epoch],\n",
    "        train_stat_a[\"metric\"][epoch],\n",
    "        train_stat_a[\"metric2\"][epoch],\n",
    "    ) = helper_fun(\n",
    "        nn_a, criterion_a, criterion_a, loader, loader_test, optimizer_a\n",
    "    )\n",
    "\n",
    "    # (b)\n",
    "    (\n",
    "        validation_stat_b[\"loss\"][epoch],\n",
    "        validation_stat_b[\"metric\"][epoch],\n",
    "        validation_stat_b[\"metric2\"][epoch],\n",
    "    ), (\n",
    "        train_stat_b[\"loss\"][epoch],\n",
    "        train_stat_b[\"metric\"][epoch],\n",
    "        train_stat_b[\"metric2\"][epoch],\n",
    "    ) = helper_fun(\n",
    "        nn_b, criterion_b, criterion_a, loader, loader_test, optimizer_b\n",
    "    )\n",
    "\n",
    "    # (c)\n",
    "    (\n",
    "        validation_stat_c[\"loss\"][epoch],\n",
    "        validation_stat_c[\"metric\"][epoch],\n",
    "        validation_stat_c[\"metric2\"][epoch],\n",
    "    ), (\n",
    "        train_stat_c[\"loss\"][epoch],\n",
    "        train_stat_c[\"metric\"][epoch],\n",
    "        train_stat_c[\"metric2\"][epoch],\n",
    "    ) = helper_fun(\n",
    "        nn_c, criterion_c, criterion_a, loader_S, loader_S_test, optimizer_c\n",
    "    )\n",
    "\n",
    "    # (d)\n",
    "    # notice that this learns Sk (what is predicted), but the MSE is relative to Gk.\n",
    "    (\n",
    "        validation_stat_d[\"loss\"][epoch],\n",
    "        validation_stat_d[\"metric\"][epoch],\n",
    "        validation_stat_d[\"metric2\"][epoch],\n",
    "    ), (\n",
    "        train_stat_d[\"loss\"][epoch],\n",
    "        train_stat_d[\"metric\"][epoch],\n",
    "        train_stat_d[\"metric2\"][epoch],\n",
    "    ) = helper_fun(\n",
    "        nn_d, criterion_d, criterion_d, loader, loader_test, optimizer_d\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba797430",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "ax.plot(range(n_epochs), train_stat_a[\"metric\"], label=nn_a.name)\n",
    "ax.plot(range(n_epochs), train_stat_b[\"metric\"], label=nn_b.name)\n",
    "ax.plot(range(n_epochs), train_stat_c[\"metric\"], label=nn_c.name)\n",
    "ax.plot(range(n_epochs), train_stat_d[\"metric\"], label=nn_d.name)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "# ax.set_xlim([0, 20])\n",
    "ax.set_ylabel(\"MSE in G or S\")\n",
    "ax.set_xlabel(\"Epoch number\")\n",
    "ax.set_title(\"Training\")\n",
    "\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "ax.plot(range(n_epochs), validation_stat_a[\"metric\"], label=nn_a.name)\n",
    "ax.plot(range(n_epochs), validation_stat_b[\"metric\"], label=nn_b.name)\n",
    "ax.plot(range(n_epochs), validation_stat_c[\"metric\"], label=nn_c.name)\n",
    "ax.plot(range(n_epochs), validation_stat_d[\"metric\"], label=nn_d.name)\n",
    "plt.yscale(\"log\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Epoch number\")\n",
    "# ax.set_xlim([0, 20])\n",
    "ax.set_title(\"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3fea6f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "- The training went well for all the cases.\n",
    "- Don't compare the MSE of the cases where G_k is learnt with the cases where S_k is learnt. \n",
    "- Adding a constrain to minimize net momentum input when learning G_k, made the MSE for the model a bit higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ef53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do the models that directly learn Gk do at mom cons of that term.\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(range(n_epochs), train_stat_a[\"metric2\"], label=nn_a.name + \" Train\")\n",
    "ax.plot(range(n_epochs), train_stat_b[\"metric2\"], label=nn_b.name + \" Train\")\n",
    "\n",
    "ax.plot(range(n_epochs), validation_stat_a[\"metric2\"], \"--\", label=nn_a.name + \" Test\")\n",
    "ax.plot(range(n_epochs), validation_stat_b[\"metric2\"], \"--\", label=nn_a.name + \" Test\")\n",
    "ax.legend()\n",
    "plt.yscale(\"log\")\n",
    "# ax.set_xlim([0, 20])\n",
    "ax.set_ylabel(\"<G_k>\")\n",
    "ax.set_xlabel(\"Epoch number\")\n",
    "ax.set_title(\"Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0451e419",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "A few more take aways from the training: \n",
    "- The model with the explicit constrain for keeping forcing momentum conserving ends up with slightly higher MSE, as it is trying to reduce two things at once. \n",
    "- Adding the constraint for momentum conservation tries to keep the momentum input small, but the training behavior along the axis is quite sporadic (maybe will require a 2D learning rates?)\n",
    "- Interesting to note that even the model without the constraint is trying to reduce the momentum input over training epochs, which shows that it is something that the learning is trying to learn without help. (there might be situations with limited data where this help is important, but not super helpful for L96)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at predictions\n",
    "nn_a.eval()\n",
    "nn_b.eval()\n",
    "nn_c.eval()\n",
    "nn_d.eval()\n",
    "\n",
    "predG_a = nn_a(torch.from_numpy(Xtrue_test)).detach().numpy()\n",
    "predG_b = nn_b(torch.from_numpy(Xtrue_test)).detach().numpy()\n",
    "predS_c = nn_c(torch.from_numpy(Xtrue_test)).detach().numpy()\n",
    "predG_c = np.roll(predS_c, 1, axis=1) - np.roll(\n",
    "    predS_c, -1, axis=1\n",
    ")  # Since the last model predicts the S, we need to calculate G.\n",
    "predS_d = nn_d(torch.from_numpy(Xtrue_test)).detach().numpy()\n",
    "predG_d = np.roll(predS_d, 1, axis=1) - np.roll(\n",
    "    predS_d, -1, axis=1\n",
    ")  # Since the last model predicts the S, we need to calculate G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "k = 2\n",
    "\n",
    "Ttest = Ttrue[-val_size:]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(Ttest[:n], subgrid_G_tend_test[:n, k], label=\"True\")\n",
    "plt.plot(Ttest[:n], predG_a[:n, k], label=nn_a.name)\n",
    "plt.plot(Ttest[:n], predG_b[:n, k], label=nn_b.name)\n",
    "plt.plot(Ttest[:n], predG_c[:n, k], label=nn_b.name)\n",
    "plt.plot(Ttest[:n], predG_d[:n, k], label=nn_b.name)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"G_k\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "# plt.plot(Xtrue_test[:,k],  subgrid_tend_test[:,k], '.', markersize=1)\n",
    "plt.plot(subgrid_G_tend_test[:, k], predG_a[:, k], \".\", markersize=1.5, label=nn_a.name)\n",
    "plt.plot(subgrid_G_tend_test[:, k], predG_b[:, k], \".\", markersize=1.5, label=nn_b.name)\n",
    "plt.plot(subgrid_G_tend_test[:, k], predG_c[:, k], \".\", markersize=1.5, label=nn_c.name)\n",
    "plt.plot(subgrid_G_tend_test[:, k], predG_d[:, k], \".\", markersize=1.5, label=nn_c.name)\n",
    "\n",
    "plt.xlabel(\"True G\")\n",
    "plt.ylabel(\"Predicted G\")\n",
    "plt.legend()\n",
    "# plt.plot(Xtrue_test[:,k],  pred_2.detach().numpy()[:,k], '.', markersize=1.5)\n",
    "# plt.plot(Xtrue_test[:,k],  pred_3.detach().numpy()[:,k], '.', markersize=1.5)\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_a = np.mean((subgrid_G_tend_test - predG_a) ** 2)\n",
    "MSE_b = np.mean((subgrid_G_tend_test - predG_b) ** 2)\n",
    "MSE_c = np.mean((subgrid_G_tend_test - predG_c) ** 2)\n",
    "MSE_d = np.mean((subgrid_G_tend_test - predG_d) ** 2)\n",
    "\n",
    "MSE_S_c = np.mean((subgrid_S_tend_test - predS_c) ** 2)\n",
    "MSE_S_d = np.mean((subgrid_S_tend_test - predS_d) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_names = [nn_a.name, nn_b.name, nn_c.name, nn_d.name]\n",
    "MSEs = [MSE_a, MSE_b, MSE_c, MSE_d]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(nn_names, MSEs)\n",
    "plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment=\"right\")\n",
    "plt.ylabel(\"MSE in G_k\")\n",
    "# plt.xlabel('Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c9696",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_names = [nn_c.name, nn_d.name]\n",
    "MSEs = [MSE_S_c, MSE_S_d]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(nn_names, MSEs)\n",
    "plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment=\"right\")\n",
    "plt.ylabel(\"MSE in S_k\")\n",
    "# plt.xlabel('Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc58bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(Ttest, np.mean(subgrid_G_tend_test, axis=1), \"--\", label=\"True\")\n",
    "plt.plot(Ttest, np.mean(predG_a, axis=1), label=nn_a.name)\n",
    "plt.plot(Ttest, np.mean(predG_b, axis=1), label=nn_b.name)\n",
    "plt.plot(Ttest, np.mean(predG_c, axis=1), \"-.\", label=nn_c.name)\n",
    "plt.plot(Ttest, np.mean(predG_d, axis=1), \"-.\", label=nn_c.name)\n",
    "plt.xlim([160, 175])\n",
    "plt.legend()\n",
    "plt.ylabel(\"<G_k>\")\n",
    "plt.xlabel(\"Time step\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(0)\n",
    "plt.hist(np.mean(predG_a, axis=1), density=True, label=nn_a.name)\n",
    "plt.hist(np.mean(predG_b, axis=1), density=True, label=nn_b.name)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"<G_k>\")\n",
    "plt.ylabel(\"PDF\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad595b3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "- The models do quite a good job (visually) at capturing the patterns in the subgrid forcing. \n",
    "- The model with the constrain of momentum conservation does a better job of it (pdf width is much smaller), BUT some bias is introduced (median is shifted from 0). \n",
    "- From our experimenting this bias is not always the same, and can vary if trainined multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4842bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(Ttest, np.mean(subgrid_S_tend_test, axis=1), \"--\", label=\"True\")\n",
    "plt.plot(Ttest, np.mean(predS_c, axis=1), \"-.\", label=nn_c.name)\n",
    "plt.plot(Ttest, np.mean(predS_d, axis=1), \"-.\", label=nn_d.name)\n",
    "plt.xlim([160, 175])\n",
    "plt.legend()\n",
    "plt.ylabel(\"<S_k>\")\n",
    "plt.xlabel(\"Time step\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(0, 0)\n",
    "plt.hist(np.mean(predS_c, axis=1), density=True, label=nn_c.name)\n",
    "plt.hist(np.mean(predS_d, axis=1), density=True, label=nn_d.name)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"<S_k>\")\n",
    "plt.ylabel(\"PDF\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c44fc3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**When the model learns Sk from Gk, without knowing anything about Gk. It produces the right Gk, but a Sk that looks very different from the real Sk.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aee82d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Skill of different ML models in prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d1e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - a GCM class including a neural network parameterization in rhs of equation for tendency\n",
    "\n",
    "time_method = RK4\n",
    "\n",
    "# - a GCM class without any parameterization\n",
    "\n",
    "\n",
    "class GCM_no_param:\n",
    "    def __init__(self, F, time_stepping=time_method):\n",
    "        self.F = F\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        return L96_eq1_xdot(X, self.F)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time\n",
    "\n",
    "\n",
    "class GCM_network:\n",
    "    def __init__(self, F, network, time_stepping=time_method):\n",
    "        self.F = F\n",
    "        self.network = network\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        if self.network.linear1.in_features == 1:\n",
    "            X_torch = torch.from_numpy(X).double()\n",
    "            X_torch = torch.unsqueeze(X_torch, 1)\n",
    "        else:\n",
    "            X_torch = torch.from_numpy(np.expand_dims(X, 0)).double()\n",
    "\n",
    "        return L96_eq1_xdot(X, self.F) + np.squeeze(\n",
    "            self.network(X_torch).data.numpy()\n",
    "        )  # Adding NN parameterization\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, histX, histB, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        histX[0] = X\n",
    "        histB[0] = 0.0\n",
    "\n",
    "        for n in range(nt):\n",
    "            # this next if statement is being called twice\n",
    "            if self.network.linear1.in_features == 1:\n",
    "                X_torch = torch.from_numpy(X).double()\n",
    "                X_torch = torch.unsqueeze(X_torch, 1)\n",
    "            else:\n",
    "                X_torch = torch.from_numpy(np.expand_dims(X, 0)).double()\n",
    "\n",
    "            histB[n + 1] = np.squeeze(self.network(X_torch).data.numpy())\n",
    "\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            histX[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return histX, time, histB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ebac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCM_networkS:\n",
    "    def __init__(self, F, network, time_stepping=time_method):\n",
    "        self.F = F\n",
    "        self.network = network\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        if self.network.linear1.in_features == 1:\n",
    "            X_torch = torch.from_numpy(X).double()\n",
    "            X_torch = torch.unsqueeze(X_torch, 1)\n",
    "        else:\n",
    "            X_torch = torch.from_numpy(np.expand_dims(X, 0)).double()\n",
    "\n",
    "            predS = self.network(X_torch).data.numpy()\n",
    "            predG = np.roll(predS, 1, axis=1) - np.roll(predS, -1, axis=1)\n",
    "        # return L96_eq1_xdot(X, self.F) + np.squeeze(self.network(X_torch).data.numpy()) # Adding NN parameterization\n",
    "        return L96_eq1_xdot(X, self.F) + np.squeeze(predG)  # Adding NN parameterization\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, histX, histB, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        histX[0] = X\n",
    "        histB[0] = 0.0\n",
    "\n",
    "        for n in range(nt):\n",
    "            # this next if statement is being called twice\n",
    "            if self.network.linear1.in_features == 1:\n",
    "                X_torch = torch.from_numpy(X).double()\n",
    "                X_torch = torch.unsqueeze(X_torch, 1)\n",
    "            else:\n",
    "                X_torch = torch.from_numpy(np.expand_dims(X, 0)).double()\n",
    "\n",
    "            predS = self.network(X_torch).data.numpy()\n",
    "            predG = np.roll(predS, 1, axis=1) - np.roll(predS, -1, axis=1)\n",
    "            histB[n + 1] = predG  # np.squeeze(self.network(X_torch).data.numpy())\n",
    "\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            histX[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return histX, time, histB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59080546",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test = 20\n",
    "\n",
    "X_full, _, t, S_full = W.run(dt, T_test, return_coupling=True)  # Full model\n",
    "G_full = np.roll(S_full, 1, axis=1) - np.roll(S_full, -1, axis=1)\n",
    "init_cond = Xtrue[-1, :]\n",
    "\n",
    "gcm_a = GCM_network(Forcing, nn_a)\n",
    "X_a, t, G_a = gcm_a(init_cond, dt, int(T_test / dt))\n",
    "\n",
    "gcm_b = GCM_network(Forcing, nn_b)\n",
    "X_b, t, G_b = gcm_b(init_cond, dt, int(T_test / dt))\n",
    "\n",
    "gcm_c = GCM_networkS(Forcing, nn_c)\n",
    "X_c, t, G_c = gcm_c(init_cond, dt, int(T_test / dt))\n",
    "\n",
    "gcm_d = GCM_networkS(Forcing, nn_d)\n",
    "X_d, t, G_d = gcm_d(init_cond, dt, int(T_test / dt))\n",
    "\n",
    "\n",
    "gcm_no_param = GCM_no_param(Forcing)\n",
    "X_no_param, t = gcm_no_param(init_cond, dt, int(T_test / dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ac884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to xarray for convenience\n",
    "X_full_xr = xr.DataArray(\n",
    "    X_full, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"X_true\")\n",
    "G_full_xr = xr.DataArray(\n",
    "    G_full, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"G_true\")\n",
    "\n",
    "X_a_xr = xr.DataArray(\n",
    "    X_a, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"X_a\")\n",
    "G_a_xr = xr.DataArray(\n",
    "    G_a, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"G_a\")\n",
    "\n",
    "X_b_xr = xr.DataArray(\n",
    "    X_b, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"X_b\")\n",
    "G_b_xr = xr.DataArray(\n",
    "    G_b, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"G_b\")\n",
    "\n",
    "X_c_xr = xr.DataArray(\n",
    "    X_c, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"X_c\")\n",
    "G_c_xr = xr.DataArray(\n",
    "    G_c, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"G_c\")\n",
    "\n",
    "X_d_xr = xr.DataArray(\n",
    "    X_d, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"X_d\")\n",
    "G_d_xr = xr.DataArray(\n",
    "    G_d, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"G_d\")\n",
    "\n",
    "X_noparam_xr = xr.DataArray(\n",
    "    X_no_param, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"X_noparam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46274421",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22, 10))\n",
    "\n",
    "plt.subplot(161)\n",
    "X_full_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-15, 15, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Full model\")\n",
    "\n",
    "plt.subplot(162)\n",
    "X_a_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-15, 15, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Learn Gk\")\n",
    "\n",
    "plt.subplot(163)\n",
    "X_b_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-15, 15, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Lrn Gk w/ con\")\n",
    "\n",
    "plt.subplot(164)\n",
    "X_c_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-15, 15, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Lrn Sk\")\n",
    "\n",
    "plt.subplot(165)\n",
    "X_d_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-15, 15, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Lrn Sk f/ Gk\")\n",
    "\n",
    "plt.subplot(166)\n",
    "X_noparam_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-15, 15, 11),\n",
    "    extend=\"both\",\n",
    "    cbar_kwargs={\"aspect\": 20, \"shrink\": 0.2},\n",
    ")\n",
    "plt.title(\"No param\")\n",
    "\n",
    "plt.suptitle(\"X\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6649bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22, 10))\n",
    "\n",
    "plt.subplot(161)\n",
    "G_full_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-20, 20, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Full model\")\n",
    "\n",
    "plt.subplot(162)\n",
    "G_a_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-20, 20, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Learn Gk\")\n",
    "\n",
    "plt.subplot(163)\n",
    "G_b_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-20, 20, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Lrn Gk w/ con\")\n",
    "\n",
    "plt.subplot(164)\n",
    "G_c_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-20, 20, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Lrn Sk\")\n",
    "\n",
    "plt.subplot(165)\n",
    "G_d_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-20, 20, 11),\n",
    "    extend=\"both\",\n",
    "    add_colorbar=True,\n",
    "    cbar_kwargs={\"aspect\": 20, \"shrink\": 0.2},\n",
    ")\n",
    "plt.title(\"Lrn Sk\")\n",
    "\n",
    "plt.subplot(166)\n",
    "# X_noparam_xr.sel(time=slice(0, 15)).plot.contourf(levels=np.linspace(-15, 15, 9), extend='both', cbar_kwargs={'aspect':5})\n",
    "plt.title(\"No param\")\n",
    "plt.suptitle(\"G\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17401577",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(X_full_xr.data.reshape(-1), bins=np.linspace(-15, 21, 51), label=\"True\")\n",
    "plt.hist(\n",
    "    X_a_xr.data.reshape(-1), bins=np.linspace(-15, 21, 51), alpha=0.5, label=nn_a.name\n",
    ")\n",
    "plt.hist(\n",
    "    X_b_xr.data.reshape(-1), bins=np.linspace(-15, 21, 51), alpha=0.5, label=nn_b.name\n",
    ")\n",
    "plt.hist(\n",
    "    X_c_xr.data.reshape(-1), bins=np.linspace(-15, 21, 51), alpha=0.5, label=nn_c.name\n",
    ")\n",
    "plt.hist(\n",
    "    X_d_xr.data.reshape(-1), bins=np.linspace(-15, 21, 51), alpha=0.5, label=nn_d.name\n",
    ")\n",
    "plt.hist(\n",
    "    X_noparam_xr.data.reshape(-1),\n",
    "    bins=np.linspace(-15, 21, 51),\n",
    "    alpha=0.5,\n",
    "    label=\"No param\",\n",
    ")\n",
    "\n",
    "plt.legend(fontsize=\"x-small\")\n",
    "plt.xlabel(\"X\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(G_full_xr.data.reshape(-1), bins=np.linspace(-21, 21, 51), label=\"True\")\n",
    "plt.hist(\n",
    "    G_a_xr.data.reshape(-1), bins=np.linspace(-21, 21, 51), alpha=0.5, label=nn_a.name\n",
    ")\n",
    "plt.hist(\n",
    "    G_b_xr.data.reshape(-1), bins=np.linspace(-21, 21, 51), alpha=0.5, label=nn_b.name\n",
    ")\n",
    "plt.hist(\n",
    "    G_c_xr.data.reshape(-1), bins=np.linspace(-15, 21, 51), alpha=0.5, label=nn_c.name\n",
    ")\n",
    "plt.hist(\n",
    "    G_d_xr.data.reshape(-1), bins=np.linspace(-15, 21, 51), alpha=0.5, label=nn_d.name\n",
    ")\n",
    "# plt.hist(X_noparam_xr.data.reshape(-1), bins = np.linspace(-15, 21, 51), alpha=0.5, label='No param');\n",
    "\n",
    "# plt.legend()\n",
    "plt.xlabel(\"G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536de4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0ee7ea5",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Discussion points\n",
    "- Do people have examples where the choice of loss function or constrains really mattered and helped, please share? (e.g. Laure’s work on adding momentum conservation)\n",
    "- Has anybody else used custom loss functions in their work?\n",
    "- Anyone has experience with this bias/variance trade-off that we see here?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014a41f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
