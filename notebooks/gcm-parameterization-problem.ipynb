{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9645ec",
   "metadata": {},
   "source": [
    "*The objective of this notebook is to introduce some of the key aspects of parameterizations in GCMs, illustrating the deterministic vs stochastic approaches, the interplay with numerical errors and how to measure the skill of a parameterization.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c17d19",
   "metadata": {},
   "source": [
    "## 1. Introducing the need for GCM parameterizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4905750",
   "metadata": {},
   "source": [
    "Starting from where we stopped last time, let's assume from now on that we are all familiar  with the [Lorenz (1996)](https://www.ecmwf.int/en/elibrary/10829-predictability-problem-partly-solved) two-time scale model and its pratical numerical implementation in the `L96_model` module.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8255144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# L96 provides the \"real world\", L96_eq1_xdot is the beginning of rhs of X tendency\n",
    "from L96_model import L96, RK2, RK4, EulerFwd, L96_eq1_xdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ba242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed gives us reproducible results\n",
    "np.random.seed(13)\n",
    "\n",
    "# Create a \"real world\" with K=8 and J=32\n",
    "W = L96(8, 32, F=18)\n",
    "\n",
    "# Run \"real world\" for 3 days to forget initial conditons\n",
    "# (store=True save the final state as an initial condition for the next run)\n",
    "W.run(0.05, 3.0, store=True);\n",
    "\n",
    "# From here on we can use W.X as perfect initial conditions for a model\n",
    "# and sample the real world using W.run(dt,T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1c62ff",
   "metadata": {},
   "source": [
    "Let's call $Z(t)$ the trajectory of the full complexity physical system (say planet earth). Because in practice, for computational or observational reasons, we cannot afford describing and predicting $Z(t)$, we will only focus on a projection of $Z(t)$ in some lower dimension space. Let's call this reduced dimension state $X(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e04ccf0",
   "metadata": {},
   "source": [
    "In our toy model,  $Z(t)=(X(t),Y(t))$ is the full complexity physical system while $X(t)$ is the lower dimension reduction. In real case situations, the lower dimension representation of the real system is usually a coarse-grained or a subsampled description of the full-scale system but this is not exclusive. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66dd9fa",
   "metadata": {},
   "source": [
    "Now, a GCM is simply a numerical machine which intends to predict the trajectory $X(t)$ from knowledge of $X(t=0)$ only. A GCM is generally built from first principle physical laws, by discretizing partial differential equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364ca7d",
   "metadata": {},
   "source": [
    "In what follows, we therefore assume that we know a fraction of the terms that govern the evolution of $X$. We also assume that we do not know what governs the evolution of $Y$ nor how $Y$ may affect $X$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53afa120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "def GCMnoparam(X0, F, dt, nt):\n",
    "    time, hist, X = (\n",
    "        dt * np.arange(nt + 1),\n",
    "        np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "        X0.copy(),\n",
    "    )\n",
    "    hist[0] = X\n",
    "\n",
    "    for n in range(nt):\n",
    "        X = X + dt * (L96_eq1_xdot(X, F))\n",
    "        hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "    return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f113734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This GCM is unstable due to Euler forward time stepping scheme,\n",
    "# so we integrate for not too long...\n",
    "F, dt, T = 18, 0.01, 3.0\n",
    "X, t = GCMnoparam(W.X, F, dt, int(T / dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f3c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and compare to the real world with the same time interval as \"dt\" used by the model\n",
    "Xtrue, _, _ = W.run(dt, T)\n",
    "\n",
    "plt.plot(t, Xtrue[:, 4], label=\"Truth ie L96\")\n",
    "plt.plot(t, X[:, 4], label=\"Model ie GCM\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9cbd6",
   "metadata": {},
   "source": [
    "There are several reasons why the above Model (ie GCM) differs from Truth (ie L96), cf section 5 of this notebook. \n",
    "\n",
    "One possibility to reduce differences in between Model and Truth, is to add a *parameterization* : an extra term to the rhs of the Model evolution operator in order to reduce the Model error as compared to Truth. It may account for missing processes, acting in Truth but not included in Model, eventually relating to subgrid physics, which is the case here, but not automatically. *Parameterizations* are also commonly refered to as *closures*, in particular when they encode explicit physical assumptions on how non-represented variables (e.g. $Y$) impact represented variables (e.g. $X$). \n",
    "\n",
    "Parameterizations usually involve free parameters that need to be adjusted. The form of the parameterization may be dictated by physical laws, but generally is unknown as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e94c31",
   "metadata": {},
   "source": [
    "## 2. GCM with parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - a GCM class including a parameterization in rhs of equation for tendency\n",
    "class GCM:\n",
    "    def __init__(self, F, parameterization, time_stepping=EulerFwd):\n",
    "        self.F = F\n",
    "        self.parameterization = parameterization\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        return L96_eq1_xdot(X, self.F) - self.parameterization(param, X)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d08038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a first step, we illustrate introducing a polynomial parameterization to GCM\n",
    "naive_parameterization = lambda param, X: np.polyval(param, X)\n",
    "F, dt, T = 18, 0.01, 5.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "X, t = gcm(W.X, dt, int(T / dt), param=[0.85439536, 1.75218026])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca979ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing model and true trajectories.\n",
    "\n",
    "# This samples the real world with the same time interval as \"dt\" used by the model\n",
    "Xtrue, _, _ = W.run(dt, T)\n",
    "\n",
    "plt.plot(t, Xtrue[:, 4], label=\"Truth\")\n",
    "plt.plot(t, X[:, 4], label=\"Model\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a97034",
   "metadata": {},
   "source": [
    "Obviously, at this stage our GCM is still not very good at reproducing the true evolution of the full system. It also remains to find the most appropriate coefficients of the polynomial parameterization to make the Model as close as possible to the Truth.\n",
    "\n",
    "The parameterization problem boils down to defining the formulation and finding the best parameters in order to minimize the distance between the true trajectory and the model trajectory.\n",
    "\n",
    "Within M2LINES we are approaching this problem as a Machine Learning problem, namely we want to learn parameterizations from objective measures of their skills through an optimization procedure. \n",
    "\n",
    "But we are not only interested in learning the parameters of existing formulations. More generally, we would like to learn the formulation too.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca8ac3",
   "metadata": {},
   "source": [
    "## 3. Should parameterizations  be deterministic or stochastic ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36678885",
   "metadata": {},
   "source": [
    "The `naive_parameterization` above has no particular physical nor mathematical justification. Most importantly, it relies on a very strong assumption, that the time rate of change of $X$ at time $t$ is a function of $X(t)$. This assumption implies that the future evolution of the reduced dimension system $X(t)$ is *deterministically* related to the initial reduced dimension state $X(0)$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6083799",
   "metadata": {},
   "source": [
    "But this is not a good assumption because two identical reduced dimension state ($X$, macro-state) can be associated with very different fine scale states ($Y$, micro-state). Given the non-linearity of the evolution equation for $Z$, the two large scale trajectories will diverge at some point. Let's illustrate that with L96 alone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd825114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - randomising the initial Ys\n",
    "np.random.seed(13)\n",
    "\n",
    "# Duplicating L96 to create perturbed versions that include random perturbations in Y\n",
    "Wp1 = W.copy()\n",
    "Yp1 = W.Y.std() * np.random.rand(Wp1.Y.size)\n",
    "Wp1.set_state(W.X, Yp1)\n",
    "\n",
    "Wp2 = W.copy()\n",
    "Yp2 = W.Y + 0.0001 * np.random.rand(Wp2.Y.size)\n",
    "Wp2.set_state(W.X, Yp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a856077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running L96 and perturbed versions to compare results\n",
    "Xtrue, _, _ = W.run(dt, T)\n",
    "Xpert1, _, _ = Wp1.run(dt, T)\n",
    "Xpert2, _, _ = Wp2.run(dt, T)\n",
    "plt.plot(t, Xtrue[:, 4], label=\"Truth\")\n",
    "plt.plot(t, Xpert1[:, 4], label=\"Perturbed\")\n",
    "plt.plot(t, Xpert2[:, 4], label=\"Perturbed\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b73260",
   "metadata": {},
   "source": [
    "So even very small uncertainties in the micro-state ($Y$) of L96 can lead to large scale changes (ie of the variable $X$) over short time... \n",
    "\n",
    "In a Model that does not know anything about micro-state $Y$, it is possible to introduce this uncertainty through a stochastic form in the parameterization.\n",
    "\n",
    "In addition, with this illustration, we also highlight that there is a horizon after which pointwise comparisons of the model with the truth are meaningless, hence there is some needed discussion on how to measure the skill of a parameterization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf8c10",
   "metadata": {},
   "source": [
    "## 4. How to measure how good is a given parameterization ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f4cd9",
   "metadata": {},
   "source": [
    "So, we would like to build our closure by systematically measuring their skills so that we can formulate inverse problems based on objective \"skill scores\". The skill scores we are talking about here will measure the distance between  the evolution of the true state $X_{true}$ and the simulated state $X_{gcm}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35609fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Let's define again our GCM\n",
    "F, dt, T = 18, 0.01, 100.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "Xgcm, t = gcm(W.X, dt, int(T / dt), param=[0.85439536, 1.75218026])\n",
    "\n",
    "# - ... the true state\n",
    "Xtrue, _, _ = W.run(dt, T)\n",
    "\n",
    "## and plot the results\n",
    "plt.plot(t[:500], Xtrue[:500, 4], label=\"Truth\")\n",
    "plt.plot(t[:500], Xgcm[:500, 4], label=\"GCM\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa78beb",
   "metadata": {},
   "source": [
    "A first simple choice is to start with a point wise mean square error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - A first simple choice :\n",
    "def pointwise(X1, X2, L=1.0):  # computed over some window t<L.\n",
    "    D = (X1 - X2)[np.where(t < L)]\n",
    "    return np.sqrt(D**2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c31b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_dist = lambda l: pointwise(Xtrue, Xgcm, L=l)\n",
    "dist = np.array([measure_dist(l) for l in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49636d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - plotting how this distance grows with the length of the window\n",
    "#  for all the components of X\n",
    "plt.plot(t, dist)\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$||X_{true}-X_{gcm}||_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea7896",
   "metadata": {},
   "source": [
    "Knowing from the above discussion that the system is not predictable after some time, we may as well decide to measure how well the model captures the mean states over some time window. With this we expect to measure the \"climate\" of the system instead of the \"weather\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbbd3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - mean state metric :\n",
    "def dist_mean(X1, X2, L=1.0):\n",
    "    _X1 = X1[np.where(t < L)]\n",
    "    _X2 = X2[np.where(t < L)]\n",
    "    return np.sqrt((_X1.mean(axis=0) - _X2.mean(axis=0)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60570e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_dist = lambda l: dist_mean(Xtrue, Xgcm, L=l)\n",
    "dist = np.array([measure_dist(l) for l in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9998b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - plotting how this distance grows with the length of the window\n",
    "#  for all the components of X\n",
    "plt.plot(t, dist)\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$||\\overline{X_{true}}-\\overline{X_{gcm}}||_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769ec8e",
   "metadata": {},
   "source": [
    "After some time, this metric seems to converge to some non-trivial values which are probably indicative of how well our model captures the \"climate\" of the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d68b1",
   "metadata": {},
   "source": [
    "It is also very common to formulate closures based on databases of initial tendencies.  In the Large Eddy Simulation  community, this is refered to as *a priori* skill, because you don't need to run the full model to compute it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb760c",
   "metadata": {},
   "source": [
    "This is the sort of game that several of us have been playing, trying for instance to estimate subgrid fluxes from knowledge of the large scale quantities  $$ \\nabla\\cdot \\mathbf{s} =   \\nabla\\cdot\\big(\\overline{\\mathbf{u}\\,\\Phi} - \\overline{\\mathbf{u}}\\,\\overline{\\Phi}\\big) \\simeq f(\\overline{\\mathbf{u}},\\overline{\\Phi})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fd549",
   "metadata": {},
   "source": [
    "(Note that this is not exactly the same problem as the *a priori* LES problem, because of the interplay with time-discretization. Let's neglect that for the moment.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_initial_tendency(X1, X2):\n",
    "    T1 = X1[1, :] - X1[0, :]\n",
    "    T2 = X2[1, :] - X2[0, :]\n",
    "    return np.sqrt((T1 - T2) ** 2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a52dd0",
   "metadata": {},
   "source": [
    "Because this metric is cheap to evaluate, as we do not need to integrate the GCM more than 1 time-step, we can start a sensitivity analysis in order to identify good optimal parameters for the specific formulation `naive_parameterization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d331c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "F, dt, T = 18, 0.01, 0.01\n",
    "\n",
    "\n",
    "# - Let's define again the true state\n",
    "Xtrue, _, _ = W.run(dt, T)\n",
    "\n",
    "#  and an ensemble of trajectories :\n",
    "\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "\n",
    "n = 100\n",
    "\n",
    "_p1 = np.linspace(-20, 20, n + 1)\n",
    "_p2 = np.linspace(-20, 20, n + 1)\n",
    "xp1, yp2 = np.meshgrid(_p1, _p2)\n",
    "\n",
    "score = np.zeros((n + 1, n + 1))\n",
    "\n",
    "for i, p1 in enumerate(_p1):\n",
    "    for j, p2 in enumerate(_p2):\n",
    "        Xgcm, t = gcm(W.X, dt, int(T / dt), param=[p1, p2])\n",
    "        score[i, j] = norm_initial_tendency(Xtrue, Xgcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ac3775",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(xp1, yp2, score)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"$p_2$\")\n",
    "plt.ylabel(\"$p_1$\")\n",
    "plt.title(\"Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65357a",
   "metadata": {},
   "source": [
    "From this analysis, we see that the optimisation problem is probably well posed as the cost function appears pretty smooth. One can also see that the parameter $p_1$ is more important than $p_2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d9ad08",
   "metadata": {},
   "source": [
    "## 5. How other sources of error affect the parameterization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c8635",
   "metadata": {},
   "source": [
    "Here we illustrate how the parameterization problem depends on our a priori knowledge of the other sources of errors in our GCMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8644d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Generic GCM parameters\n",
    "F, dt, T = 18, 0.01, 20.0\n",
    "X0 = W.X\n",
    "parameterization = naive_parameterization\n",
    "params = [0.85439536, 1.75218026]\n",
    "\n",
    "# - sampling real world over a longer period of time\n",
    "Xtrue, _, _ = W.run(dt, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e6cb41",
   "metadata": {},
   "source": [
    "### 5.1 Comparing GCMs with small errors in the forcing F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_relative_error = 0.01\n",
    "Fs = F + max_relative_error * (np.random.rand(40) - 0.5)  # array of perturbed forcings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c71ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCMs = [GCM(Fp, parameterization) for Fp in Fs]\n",
    "Xs = []  # list of trajectories for individual perturbed forcings\n",
    "for gcm in GCMs:\n",
    "    X, t = gcm(X0, dt, int(T / dt), param=params)\n",
    "    Xs.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68560d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, Xs[0][:, 4], label=\"Model with perturbed forcings\", color=\"grey\", lw=1)\n",
    "for X in Xs[1:]:\n",
    "    plt.plot(t, X[:, 4], color=\"grey\", lw=1)\n",
    "plt.plot(t, Xtrue[:, 4], label=\"Truth\", lw=2)\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e69d2",
   "metadata": {},
   "source": [
    "### 5.2 Comparing GCM solutions with slight errors in the (resolved) initial condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c059fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_error = 0.01\n",
    "X0s = X0 + max_relative_error * (\n",
    "    np.random.rand(50, 8) - 0.5\n",
    ")  # array of perturbed initial conditions\n",
    "gcm = GCM(F, parameterization)\n",
    "Xs = []  # list of trajectories for individual preturbed initial conditions\n",
    "for X0 in X0s:\n",
    "    X, t = gcm(X0, dt, int(T / dt), param=params)\n",
    "    Xs.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adef508",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, Xs[0][:, 4], label=\"Model with perturbed ICs\", color=\"grey\", lw=1)\n",
    "for X in Xs[1:]:\n",
    "    plt.plot(t, X[:, 4], color=\"grey\", lw=1)\n",
    "plt.plot(t, Xtrue[:, 4], label=\"Truth\", lw=2)\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ebe8ba",
   "metadata": {},
   "source": [
    "The above two experiments illustrate that the definition of our distance metrics should take into account the other sources of errors in our GCMs : errors on the parameters, on the forcing and on the initial conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3cb8f",
   "metadata": {},
   "source": [
    "### 5.3 Comparing GCMs with identical parameters but different time discretization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5648a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "F, dt, T = 18, 0.01, 10.0\n",
    "X0 = W.X\n",
    "#\n",
    "euler_GCM = GCM(F, naive_parameterization, time_stepping=EulerFwd)\n",
    "rk2_GCM = GCM(F, naive_parameterization, time_stepping=RK2)\n",
    "rk4_GCM = GCM(F, naive_parameterization, time_stepping=RK4)\n",
    "#\n",
    "euler_X, t = euler_GCM(X0, dt, int(T / dt), param=[0.85439536, 1.75218026])\n",
    "rk2_X, _ = rk2_GCM(X0, dt, int(T / dt), param=[0.85439536, 1.75218026])\n",
    "rk4_X, _ = rk4_GCM(X0, dt, int(T / dt), param=[0.85439536, 1.75218026])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3909f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, euler_X[:, 3], label=\"euler\")\n",
    "plt.plot(t, rk2_X[:, 3], label=\"RK2\")\n",
    "plt.plot(t, rk4_X[:, 3], label=\"RK4\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_3(t)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c2b3ac",
   "metadata": {},
   "source": [
    "This last experiment illustrates that depending in the numerical schemes of the GCM, the distance between the model prediction and the true state can be substantial. There again, we have an additional constraint on the definition of our distance metrics.\n",
    "\n",
    "More generally, this also suggests that the optimal closures we will find will probably depend on the numerical schemes that are used in our GCMs, or more generally on the structural uncertainties of our GCMs, which are difficult to quantify..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
