
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural networks &#8212; Learning Machine Learning with Lorenz-96</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Learning Data Assimilation Increments" href="Learning-DA-increments.html" />
    <link rel="prev" title="Using neural networks for L96 parameterization" href="Neural_network_for_Lorenz96.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/newlogo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning Machine Learning with Lorenz-96</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L96-two-scale-description.html">
   The Lorenz-96 Two-Timescale System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="presentation-model-setup.html">
   The Lorenz-96 GCM Analog
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gcm-parameterization-problem.html">
   Key aspects of GCMs parameterizations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="estimating-gcm-parameters.html">
   Tuning GCM Parameterizations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DA_demo_L96.html">
   Data Assimilation demo in the Lorenz 96 (L96) two time-scale model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Neural_network_for_Lorenz96.html">
   Using neural networks for L96 parameterization
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Learning-DA-increments.html">
   Learning Data Assimilation Increments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="random_forest_parameterization.html">
   Random Forest
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LRP-L96.html">
   LRP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Neural-Network-Advection.html">
   Using neural networks to parameterize advection in L96
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bibliography.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/gradient_decent.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/m2lines/L96_demo"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/m2lines/L96_demo/issues/new?title=Issue%20on%20page%20%2Fnotebooks/gradient_decent.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/m2lines/L96_demo/main?urlpath=tree/notebooks/gradient_decent.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#universal-approximation-theorem-nns-can-approximate-any-continuous-function">
   Universal approximation theorem - NNs can approximate any continuous function.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-gradient-descent-in-linear-regression">
   Using gradient descent in Linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression-from-scratch">
   Linear regression from scratch
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-pytorch-as-defined-at-https-pytorch-org">
     What is pytorch? (as defined at https://pytorch.org/)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chose-the-true-parameters-we-want-to-learn">
     Chose the true parameters we want to learn.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-some-data-points-x-and-y-which-lie-on-the-line">
     Create some data points x and y which lie on the line
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2">
     <span class="math notranslate nohighlight">
      \(L = \frac{1}{n}\sum_{i=1}^n [y_i - (w_0 + w_1 x_i)]^2\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression">
       So far, we have specified the
       <em>
        model
       </em>
       (linear regression) and the
       <em>
        evaluation criteria
       </em>
       (or
       <em>
        loss function
       </em>
       ). Now we need to handle
       <em>
        optimization
       </em>
       ; that is, how do we find the best values for weights (
       <span class="math notranslate nohighlight">
        \(w_0\)
       </span>
       ,
       <span class="math notranslate nohighlight">
        \(w_1\)
       </span>
       )? How do we find the best
       <em>
        fitting
       </em>
       linear regression.
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate">
       If we know those we can iteratively take little steps down the gradient to reduce the loss, aka,
       <em>
        gradient descent
       </em>
       . How big our steps are is determined by the
       <em>
        learning rate
       </em>
       .
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent">
       In Deep learning, we use a variation of gradient descent called mini-batch gradient descent:
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Neural networks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#universal-approximation-theorem-nns-can-approximate-any-continuous-function">
   Universal approximation theorem - NNs can approximate any continuous function.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-gradient-descent-in-linear-regression">
   Using gradient descent in Linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression-from-scratch">
   Linear regression from scratch
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-pytorch-as-defined-at-https-pytorch-org">
     What is pytorch? (as defined at https://pytorch.org/)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chose-the-true-parameters-we-want-to-learn">
     Chose the true parameters we want to learn.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-some-data-points-x-and-y-which-lie-on-the-line">
     Create some data points x and y which lie on the line
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2">
     <span class="math notranslate nohighlight">
      \(L = \frac{1}{n}\sum_{i=1}^n [y_i - (w_0 + w_1 x_i)]^2\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression">
       So far, we have specified the
       <em>
        model
       </em>
       (linear regression) and the
       <em>
        evaluation criteria
       </em>
       (or
       <em>
        loss function
       </em>
       ). Now we need to handle
       <em>
        optimization
       </em>
       ; that is, how do we find the best values for weights (
       <span class="math notranslate nohighlight">
        \(w_0\)
       </span>
       ,
       <span class="math notranslate nohighlight">
        \(w_1\)
       </span>
       )? How do we find the best
       <em>
        fitting
       </em>
       linear regression.
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate">
       If we know those we can iteratively take little steps down the gradient to reduce the loss, aka,
       <em>
        gradient descent
       </em>
       . How big our steps are is determined by the
       <em>
        learning rate
       </em>
       .
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent">
       In Deep learning, we use a variation of gradient descent called mini-batch gradient descent:
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="neural-networks">
<h1>Neural networks<a class="headerlink" href="#neural-networks" title="Permalink to this headline">¶</a></h1>
<div class="section" id="universal-approximation-theorem-nns-can-approximate-any-continuous-function">
<h2>Universal approximation theorem - NNs can approximate any continuous function.<a class="headerlink" href="#universal-approximation-theorem-nns-can-approximate-any-continuous-function" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>A visual demonstration that neural nets can compute any function: http://neuralnetworksanddeeplearning.com/chap4.html</p></li>
<li><p>Like any ML algorithm, training a neural netwoek requires minimizing some loss function (for a given structure that maps inputs to outputs).</p></li>
<li><p>The minimization is done using an algorithm called gradient descent, or a variation called stochastic/minibach gradient descent.</p></li>
</ul>
</div>
<div class="section" id="using-gradient-descent-in-linear-regression">
<h2>Using gradient descent in Linear regression<a class="headerlink" href="#using-gradient-descent-in-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>The simplest machine learning algorithm is linear regression. We will code up linear regression from scratch with a twist: we will use gradient descent, which is also how neural networks learn. Most of this lesson is pretty much stolen from Jeremy Howard’s fast.ai <a class="reference external" href="https://www.youtube.com/watch?v=ACU-T9L4_lI">lesson zero</a></p>
<ul class="simple">
<li><p>In linear regression, we assume that <span class="math notranslate nohighlight">\(y = w_0 + w_1 x \)</span>.</p></li>
<li><p>We look for the <span class="math notranslate nohighlight">\(w\)</span> coefficients that give the ‘best’ prediction for the output (<span class="math notranslate nohighlight">\(y\)</span>). The best prediction is defined by minimizing some cost function. For linear regression in machine learning task it is usually the mean square error.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;figs/linear_regression_as_neural_network2.png&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="c1"># Image is taken from https://blog.insightdatascience.com/a-quick-introduction-to-vanilla-neural-networks-b0998c6216a1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_5_0.png" src="../_images/gradient_decent_5_0.png" />
</div>
</div>
</div>
<div class="section" id="linear-regression-from-scratch">
<h2>Linear regression from scratch<a class="headerlink" href="#linear-regression-from-scratch" title="Permalink to this headline">¶</a></h2>
<p>We will learn the parameters <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> of a line.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import plotting packages</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Import machine-learning packages</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">Data</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>

<span class="c1"># from fastai.basics import *</span>
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Set random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="what-is-pytorch-as-defined-at-https-pytorch-org">
<h3>What is pytorch? (as defined at https://pytorch.org/)<a class="headerlink" href="#what-is-pytorch-as-defined-at-https-pytorch-org" title="Permalink to this headline">¶</a></h3>
<p>It’s a Python-based scientific computing package targeted at two sets of audiences:</p>
<ul class="simple">
<li><p>A replacement for NumPy to use the power of GPUs</p></li>
<li><p>A deep learning research platform that provides flexibility and speed</p></li>
</ul>
</div>
<div class="section" id="chose-the-true-parameters-we-want-to-learn">
<h3>Chose the true parameters we want to learn.<a class="headerlink" href="#chose-the-true-parameters-we-want-to-learn" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">w</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([3., 2.])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="create-some-data-points-x-and-y-which-lie-on-the-line">
<h3>Create some data points x and y which lie on the line<a class="headerlink" href="#create-some-data-points-x-and-y-which-lie-on-the-line" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span>
    <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">1</span>
<span class="p">)</span>  <span class="c1"># Underscore functions in pytorch means replace the value (update)</span>
<span class="n">x</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 1.0000,  0.9171],
        [ 1.0000, -0.3941],
        [ 1.0000, -0.1826],
        [ 1.0000, -0.4808],
        [ 1.0000, -0.3250]])
</pre></div>
</div>
</div>
</div>
<p>Tensor is a data structure which is a fundamental building block of PyTorch. Tensors are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU
and more importantly for us - they can keep track of its gradients.</p>
<p>For further reading, see <a class="reference external" href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">here</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># @ is a matrix product (similar to matmul)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_15_0.png" src="../_images/gradient_decent_15_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_real</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>If we could find a way to fit our guess for the coefficients the weights (<span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span>), we could use the exact same method for very complicated tasks (as in image recognition).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Written in terms of <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(w_1\)</span>, our <strong>loss function</strong> is:</p>
</div>
<div class="section" id="l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2">
<h3><span class="math notranslate nohighlight">\(L = \frac{1}{n}\sum_{i=1}^n [y_i - (w_0 + w_1 x_i)]^2\)</span><a class="headerlink" href="#l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_real</span>
<span class="n">mse</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># Initial mean-squared error</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(56.2617)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_hat</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_21_0.png" src="../_images/gradient_decent_21_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w_real</span><span class="p">)</span>
<span class="n">w</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([-3., -5.], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<div class="section" id="so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression">
<h4>So far, we have specified the <em>model</em> (linear regression) and the <em>evaluation criteria</em> (or <em>loss function</em>). Now we need to handle <em>optimization</em>; that is, how do we find the best values for weights (<span class="math notranslate nohighlight">\(w_0\)</span>, <span class="math notranslate nohighlight">\(w_1\)</span>)? How do we find the best <em>fitting</em> linear regression.<a class="headerlink" href="#so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression" title="Permalink to this headline">¶</a></h4>
<p>To know how to change <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(w_1\)</span> to reduce the loss, we compute the derivatives (or gradients).</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial L}{\partial w_0} = \frac{1}{n}\sum_i -2[y_i - (w_0 + w_1 x_i)]\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial L}{\partial w_1} = \frac{1}{n}\sum_i -2[y_i - (w_0 + w_1 x_i)]x_i\)</span></p>
</div>
<div class="section" id="if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate">
<h4>If we know those we can iteratively take little steps down the gradient to reduce the loss, aka, <em>gradient descent</em>. How big our steps are is determined by the <em>learning rate</em>.<a class="headerlink" href="#if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate" title="Permalink to this headline">¶</a></h4>
<p><span class="math notranslate nohighlight">\(w_0^{new} = w_0^{old}\)</span> - Learning-Rate <span class="math notranslate nohighlight">\(*  \frac{\partial L}{\partial w_0}\)</span></p>
<p><span class="math notranslate nohighlight">\(w_1^{new} = w_1^{old}\)</span> - Learning-Rate <span class="math notranslate nohighlight">\(*  \frac{\partial L}{\partial w_1}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update2</span><span class="p">(</span><span class="n">iteration</span><span class="p">):</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mse</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># calculate the gradient of a tensor! It is now stored at w.grad</span>

    <span class="c1"># To prevent tracking history and using memory</span>
    <span class="c1"># (code block where we don&#39;t need to track the gradients but only modify the values of tensors)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">w</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
        <span class="c1"># Under score means inplace.</span>
        <span class="c1"># lr is the learning rate. Good learning rate is a key part of Neural Networks.</span>

        <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="c1"># We want to zero the gradient before we are re-evaluate it.</span>

    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<p>In PyTorch, we need to set the gradients to zero before starting to do back propragation because PyTorch accumulates the gradients on subsequent backward passes. This is convenient while training RNNs. So, the default action is to accumulate (i.e. sum) the gradients on every loss.backward() call.
Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the minimum (or maximum, in case of maximization objectives).</p>
<p>Explanations about how PyTorch calculates the gradients can be found here (and in many other sources) - https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lin</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">w0</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="p">(</span><span class="n">line</span><span class="p">,)</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">lin</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">]),</span>
    <span class="n">c</span><span class="o">=</span><span class="s2">&quot;firebrick&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># line, = ax.plot(x[:,0], y, c=&#39;firebrick&#39;)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Loss = 0.00&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">update2</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Loss = </span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">l</span><span class="p">)</span>
    <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span>
        <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">lin</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">]),</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">line</span><span class="p">,)</span>


<span class="n">anim</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># You might have some difficulties running this cell without importing certain packages.</span>
<span class="c1"># might need to install: conda install -c conda-forge ffmpeg</span>
<span class="n">HTML</span><span class="p">(</span><span class="n">anim</span><span class="o">.</span><span class="n">to_html5_video</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><video width="720" height="432" controls autoplay loop>
  <source type="video/mp4" src="data:video/mp4;base64,AAAAIGZ0eXBNNFYgAAACAE00ViBpc29taXNvMmF2YzEAAAAIZnJlZQAAIvFtZGF0AAACrgYF//+q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">
  Your browser does not support the video tag.
</video></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;figs/Gradient_descent2.png&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">700</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_29_0.png" src="../_images/gradient_decent_29_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">update2</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">losses</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iternation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Loss&#39;)
</pre></div>
</div>
<img alt="../_images/gradient_decent_30_1.png" src="../_images/gradient_decent_30_1.png" />
</div>
</div>
</div>
<div class="section" id="in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent">
<h4>In Deep learning, we use a variation of gradient descent called <a class="reference external" href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/">mini-batch gradient descent</a>:<a class="headerlink" href="#in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent" title="Permalink to this headline">¶</a></h4>
<p>Instead of calculating the gradient over the whole training data before changing model weights (coefficients), we take a subset (batch) of our data, and change the values of the weights after we calculated the gradient over this subset.</p>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Neural_network_for_Lorenz96.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Using neural networks for L96 parameterization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Learning-DA-increments.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Learning Data Assimilation Increments</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By The M2LinES Community<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>