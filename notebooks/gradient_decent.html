
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural networks &#8212; Lorenz 1996 two time-scale model for learning machine learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Learning Data Assimilation Increments" href="Learning-DA-increments.html" />
    <link rel="prev" title="Using neural networks for L96 parameterization" href="Neural_network_for_Lorenz96.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/newlogo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Lorenz 1996 two time-scale model for learning machine learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Lorenz 1996 two time-scale model for learning machine learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Intro
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="presentation-model-setup.html">
   L96 analogs for this project: the real world and the GCM with parameterization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L96-two-scale-description.html">
   L96 two-timescale model description
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Type of Parametrization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="estimating-gcm-parameters.html">
   1. Copy / pasting from gcm-parameterization-problem notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gcm-parameterization-problem.html">
   Key aspects of GCMs parameterizations
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Assimilation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="DA_demo_L96.html">
   Data Assimilation demo in the Lorenz 96 (L96) two time-scale model
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Subgrid Patermetrization
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Neural_network_for_Lorenz96.html">
   Using neural networks for L96 parameterization
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Offline DA Increments
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Learning-DA-increments.html">
   Learning Data Assimilation Increments
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Different ML Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="random_forest_parameterization.html">
   Random Forest
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Implementation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Neural-Network-Advection.html">
   Using neural networks to parameterize advection in L96
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bibliography.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/gradient_decent.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/m2lines/L96_demo"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/m2lines/L96_demo/issues/new?title=Issue%20on%20page%20%2Fnotebooks/gradient_decent.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/m2lines/L96_demo/main?urlpath=tree/notebooks/gradient_decent.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#universal-approximation-theorem-nns-can-approximate-any-continuous-function">
   Universal approximation theorem - NNs can approximate any continuous function.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-gradient-descent-in-linear-regression">
   Using gradient descent in Linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression-from-scratch">
   Linear regression from scratch
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-pytorch-as-defined-at-https-pytorch-org">
     What is pytorch? (as defined at https://pytorch.org/)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chose-the-true-parameters-we-want-to-learn">
     Chose the true parameters we want to learn.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-some-data-points-x-and-y-which-lie-on-the-line">
     Create some data points x and y which lie on the line
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2">
     <span class="math notranslate nohighlight">
      \(L = \frac{1}{n}\sum_{i=1}^n [y_i - (w_0 + w_1 x_i)]^2\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression">
       So far, we have specified the
       <em>
        model
       </em>
       (linear regression) and the
       <em>
        evaluation criteria
       </em>
       (or
       <em>
        loss function
       </em>
       ). Now we need to handle
       <em>
        optimization
       </em>
       ; that is, how do we find the best values for weights (
       <span class="math notranslate nohighlight">
        \(w_0\)
       </span>
       ,
       <span class="math notranslate nohighlight">
        \(w_1\)
       </span>
       )? How do we find the best
       <em>
        fitting
       </em>
       linear regression.
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate">
       If we know those we can iteratively take little steps down the gradient to reduce the loss, aka,
       <em>
        gradient descent
       </em>
       . How big our steps are is determined by the
       <em>
        learning rate
       </em>
       .
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent">
       In Deep learning, we use a variation of gradient descent called mini-batch gradient descent:
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Neural networks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#universal-approximation-theorem-nns-can-approximate-any-continuous-function">
   Universal approximation theorem - NNs can approximate any continuous function.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-gradient-descent-in-linear-regression">
   Using gradient descent in Linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression-from-scratch">
   Linear regression from scratch
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-pytorch-as-defined-at-https-pytorch-org">
     What is pytorch? (as defined at https://pytorch.org/)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chose-the-true-parameters-we-want-to-learn">
     Chose the true parameters we want to learn.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-some-data-points-x-and-y-which-lie-on-the-line">
     Create some data points x and y which lie on the line
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2">
     <span class="math notranslate nohighlight">
      \(L = \frac{1}{n}\sum_{i=1}^n [y_i - (w_0 + w_1 x_i)]^2\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression">
       So far, we have specified the
       <em>
        model
       </em>
       (linear regression) and the
       <em>
        evaluation criteria
       </em>
       (or
       <em>
        loss function
       </em>
       ). Now we need to handle
       <em>
        optimization
       </em>
       ; that is, how do we find the best values for weights (
       <span class="math notranslate nohighlight">
        \(w_0\)
       </span>
       ,
       <span class="math notranslate nohighlight">
        \(w_1\)
       </span>
       )? How do we find the best
       <em>
        fitting
       </em>
       linear regression.
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate">
       If we know those we can iteratively take little steps down the gradient to reduce the loss, aka,
       <em>
        gradient descent
       </em>
       . How big our steps are is determined by the
       <em>
        learning rate
       </em>
       .
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent">
       In Deep learning, we use a variation of gradient descent called mini-batch gradient descent:
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="neural-networks">
<h1>Neural networks<a class="headerlink" href="#neural-networks" title="Permalink to this headline">¶</a></h1>
<div class="section" id="universal-approximation-theorem-nns-can-approximate-any-continuous-function">
<h2>Universal approximation theorem - NNs can approximate any continuous function.<a class="headerlink" href="#universal-approximation-theorem-nns-can-approximate-any-continuous-function" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>A visual demonstration that neural nets can compute any function: http://neuralnetworksanddeeplearning.com/chap4.html</p></li>
<li><p>Like any ML algorithm, training a neural netwoek requires minimizing some loss function (for a given structure that maps inputs to outputs).</p></li>
<li><p>The minimization is done using an algorithm called gradient descent, or a variation called stochastic/minibach gradient descent.</p></li>
</ul>
</div>
<div class="section" id="using-gradient-descent-in-linear-regression">
<h2>Using gradient descent in Linear regression<a class="headerlink" href="#using-gradient-descent-in-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>The simplest machine learning algorithm is linear regression. We will code up linear regression from scratch with a twist: we will use gradient descent, which is also how neural networks learn. Most of this lesson is pretty much stolen from Jeremy Howard’s fast.ai <a class="reference external" href="https://www.youtube.com/watch?v=ACU-T9L4_lI">lesson zero</a></p>
<ul class="simple">
<li><p>In linear regression, we assume that <span class="math notranslate nohighlight">\(y = w_0 + w_1 x \)</span>.</p></li>
<li><p>We look for the <span class="math notranslate nohighlight">\(w\)</span> coefficients that give the ‘best’ prediction for the output (<span class="math notranslate nohighlight">\(y\)</span>). The best prediction is defined by minimizing some cost function. For linear regression in machine learning task it is usually the mean square error.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;figs/linear_regression_as_neural_network2.png&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="c1"># Image is taken from https://blog.insightdatascience.com/a-quick-introduction-to-vanilla-neural-networks-b0998c6216a1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_5_0.png" src="../_images/gradient_decent_5_0.png" />
</div>
</div>
</div>
<div class="section" id="linear-regression-from-scratch">
<h2>Linear regression from scratch<a class="headerlink" href="#linear-regression-from-scratch" title="Permalink to this headline">¶</a></h2>
<p>We will learn the parameters <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> of a line.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import plotting packages</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Import machine-learning packages</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">Data</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>

<span class="c1"># from fastai.basics import *</span>
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Set random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="what-is-pytorch-as-defined-at-https-pytorch-org">
<h3>What is pytorch? (as defined at https://pytorch.org/)<a class="headerlink" href="#what-is-pytorch-as-defined-at-https-pytorch-org" title="Permalink to this headline">¶</a></h3>
<p>It’s a Python-based scientific computing package targeted at two sets of audiences:</p>
<ul class="simple">
<li><p>A replacement for NumPy to use the power of GPUs</p></li>
<li><p>A deep learning research platform that provides flexibility and speed</p></li>
</ul>
</div>
<div class="section" id="chose-the-true-parameters-we-want-to-learn">
<h3>Chose the true parameters we want to learn.<a class="headerlink" href="#chose-the-true-parameters-we-want-to-learn" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">w</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([3., 2.])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="create-some-data-points-x-and-y-which-lie-on-the-line">
<h3>Create some data points x and y which lie on the line<a class="headerlink" href="#create-some-data-points-x-and-y-which-lie-on-the-line" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span>
    <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">1</span>
<span class="p">)</span>  <span class="c1"># Underscore functions in pytorch means replace the value (update)</span>
<span class="n">x</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 1.0000,  0.6582],
        [ 1.0000, -0.6586],
        [ 1.0000,  0.6990],
        [ 1.0000,  0.6540],
        [ 1.0000,  0.8058]])
</pre></div>
</div>
</div>
</div>
<p>Tensor is a data structure which is a fundamental building block of PyTorch. Tensors are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU
and more importantly for us - they can keep track of its gradients.</p>
<p>For further reading, see <a class="reference external" href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">here</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># @ is a matrix product (similar to matmul)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_15_0.png" src="../_images/gradient_decent_15_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_real</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>If we could find a way to fit our guess for the coefficients the weights (<span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span>), we could use the exact same method for very complicated tasks (as in image recognition).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Written in terms of <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(w_1\)</span>, our <strong>loss function</strong> is:</p>
</div>
<div class="section" id="l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2">
<h3><span class="math notranslate nohighlight">\(L = \frac{1}{n}\sum_{i=1}^n [y_i - (w_0 + w_1 x_i)]^2\)</span><a class="headerlink" href="#l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_real</span>
<span class="n">mse</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># Initial mean-squared error</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(60.6016)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_hat</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_21_0.png" src="../_images/gradient_decent_21_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w_real</span><span class="p">)</span>
<span class="n">w</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([-3., -5.], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<div class="section" id="so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression">
<h4>So far, we have specified the <em>model</em> (linear regression) and the <em>evaluation criteria</em> (or <em>loss function</em>). Now we need to handle <em>optimization</em>; that is, how do we find the best values for weights (<span class="math notranslate nohighlight">\(w_0\)</span>, <span class="math notranslate nohighlight">\(w_1\)</span>)? How do we find the best <em>fitting</em> linear regression.<a class="headerlink" href="#so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression" title="Permalink to this headline">¶</a></h4>
<p>To know how to change <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(w_1\)</span> to reduce the loss, we compute the derivatives (or gradients).</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial L}{\partial w_0} = \frac{1}{n}\sum_i -2[y_i - (w_0 + w_1 x_i)]\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial L}{\partial w_1} = \frac{1}{n}\sum_i -2[y_i - (w_0 + w_1 x_i)]x_i\)</span></p>
</div>
<div class="section" id="if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate">
<h4>If we know those we can iteratively take little steps down the gradient to reduce the loss, aka, <em>gradient descent</em>. How big our steps are is determined by the <em>learning rate</em>.<a class="headerlink" href="#if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate" title="Permalink to this headline">¶</a></h4>
<p><span class="math notranslate nohighlight">\(w_0^{new} = w_0^{old}\)</span> - Learning-Rate <span class="math notranslate nohighlight">\(*  \frac{\partial L}{\partial w_0}\)</span></p>
<p><span class="math notranslate nohighlight">\(w_1^{new} = w_1^{old}\)</span> - Learning-Rate <span class="math notranslate nohighlight">\(*  \frac{\partial L}{\partial w_1}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update2</span><span class="p">(</span><span class="n">iteration</span><span class="p">):</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mse</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># calculate the gradient of a tensor! It is now stored at w.grad</span>

    <span class="c1"># To prevent tracking history and using memory</span>
    <span class="c1"># (code block where we don&#39;t need to track the gradients but only modify the values of tensors)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">w</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
        <span class="c1"># Under score means inplace.</span>
        <span class="c1"># lr is the learning rate. Good learning rate is a key part of Neural Networks.</span>

        <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="c1"># We want to zero the gradient before we are re-evaluate it.</span>

    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<p>In PyTorch, we need to set the gradients to zero before starting to do back propragation because PyTorch accumulates the gradients on subsequent backward passes. This is convenient while training RNNs. So, the default action is to accumulate (i.e. sum) the gradients on every loss.backward() call.
Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the minimum (or maximum, in case of maximization objectives).</p>
<p>Explanations about how PyTorch calculates the gradients can be found here (and in many other sources) - https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lin</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">w0</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="p">(</span><span class="n">line</span><span class="p">,)</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">lin</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">]),</span>
    <span class="n">c</span><span class="o">=</span><span class="s2">&quot;firebrick&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># line, = ax.plot(x[:,0], y, c=&#39;firebrick&#39;)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Loss = 0.00&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">update2</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Loss = </span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">l</span><span class="p">)</span>
    <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span>
        <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">lin</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">]),</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">line</span><span class="p">,)</span>


<span class="n">anim</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># You might have some difficulties running this cell without importing certain packages.</span>
<span class="c1"># might need to install: conda install -c conda-forge ffmpeg</span>
<span class="n">HTML</span><span class="p">(</span><span class="n">anim</span><span class="o">.</span><span class="n">to_html5_video</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><video width="720" height="432" controls autoplay loop>
  <source type="video/mp4" src="data:video/mp4;base64,AAAAIGZ0eXBNNFYgAAACAE00ViBpc29taXNvMmF2YzEAAAAIZnJlZQAAJDttZGF0AAACrgYF//+q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">
  Your browser does not support the video tag.
</video></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;figs/Gradient_descent2.png&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">700</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_29_0.png" src="../_images/gradient_decent_29_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">update2</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">losses</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iternation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Loss&#39;)
</pre></div>
</div>
<img alt="../_images/gradient_decent_30_1.png" src="../_images/gradient_decent_30_1.png" />
</div>
</div>
</div>
<div class="section" id="in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent">
<h4>In Deep learning, we use a variation of gradient descent called <a class="reference external" href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/">mini-batch gradient descent</a>:<a class="headerlink" href="#in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent" title="Permalink to this headline">¶</a></h4>
<p>Instead of calculating the gradient over the whole training data before changing model weights (coefficients), we take a subset (batch) of our data, and change the values of the weights after we calculated the gradient over this subset.</p>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Neural_network_for_Lorenz96.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Using neural networks for L96 parameterization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Learning-DA-increments.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Learning Data Assimilation Increments</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By The M2LinES Community<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>