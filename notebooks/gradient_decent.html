
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural networks &#8212; Learning Machine Learning with Lorenz-96</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Learning Data Assimilation Increments" href="Learning-DA-increments.html" />
    <link rel="prev" title="Using neural networks for L96 parameterization" href="Neural_network_for_Lorenz96.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/newlogo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning Machine Learning with Lorenz-96</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L96-two-scale-description.html">
   The Lorenz-96 Two-Timescale System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="presentation-model-setup.html">
   The Lorenz-96 GCM Analog
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gcm-parameterization-problem.html">
   Key aspects of GCMs parameterizations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="estimating-gcm-parameters.html">
   Tuning GCM Parameterizations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DA_demo_L96.html">
   Data Assimilation demo in the Lorenz 96 (L96) two time-scale model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Neural_network_for_Lorenz96.html">
   Using neural networks for L96 parameterization
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Learning-DA-increments.html">
   Learning Data Assimilation Increments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="random_forest_parameterization.html">
   Random Forest
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Neural-Network-Saliency-Maps.html">
   Generating saliency maps for neural networks trained on L96
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Neural-Network-Advection.html">
   Using neural networks to parameterize advection in L96
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bibliography.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/gradient_decent.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/m2lines/L96_demo"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/m2lines/L96_demo/issues/new?title=Issue%20on%20page%20%2Fnotebooks/gradient_decent.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/m2lines/L96_demo/main?urlpath=tree/notebooks/gradient_decent.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#universal-approximation-theorem-nns-can-approximate-any-continuous-function">
   Universal approximation theorem - NNs can approximate any continuous function.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-gradient-descent-in-linear-regression">
   Using gradient descent in Linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression-from-scratch">
   Linear regression from scratch
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-pytorch-as-defined-at-https-pytorch-org">
     What is pytorch? (as defined at https://pytorch.org/)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chose-the-true-parameters-we-want-to-learn">
     Chose the true parameters we want to learn.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-some-data-points-x-and-y-which-lie-on-the-line">
     Create some data points x and y which lie on the line
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2">
     <span class="math notranslate nohighlight">
      \(L = \frac{1}{n}\sum_{i=1}^n [y_i - (w_0 + w_1 x_i)]^2\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression">
       So far, we have specified the
       <em>
        model
       </em>
       (linear regression) and the
       <em>
        evaluation criteria
       </em>
       (or
       <em>
        loss function
       </em>
       ). Now we need to handle
       <em>
        optimization
       </em>
       ; that is, how do we find the best values for weights (
       <span class="math notranslate nohighlight">
        \(w_0\)
       </span>
       ,
       <span class="math notranslate nohighlight">
        \(w_1\)
       </span>
       )? How do we find the best
       <em>
        fitting
       </em>
       linear regression.
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate">
       If we know those we can iteratively take little steps down the gradient to reduce the loss, aka,
       <em>
        gradient descent
       </em>
       . How big our steps are is determined by the
       <em>
        learning rate
       </em>
       .
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent">
       In Deep learning, we use a variation of gradient descent called mini-batch gradient descent:
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Neural networks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#universal-approximation-theorem-nns-can-approximate-any-continuous-function">
   Universal approximation theorem - NNs can approximate any continuous function.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-gradient-descent-in-linear-regression">
   Using gradient descent in Linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression-from-scratch">
   Linear regression from scratch
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-pytorch-as-defined-at-https-pytorch-org">
     What is pytorch? (as defined at https://pytorch.org/)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chose-the-true-parameters-we-want-to-learn">
     Chose the true parameters we want to learn.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-some-data-points-x-and-y-which-lie-on-the-line">
     Create some data points x and y which lie on the line
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2">
     <span class="math notranslate nohighlight">
      \(L = \frac{1}{n}\sum_{i=1}^n [y_i - (w_0 + w_1 x_i)]^2\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression">
       So far, we have specified the
       <em>
        model
       </em>
       (linear regression) and the
       <em>
        evaluation criteria
       </em>
       (or
       <em>
        loss function
       </em>
       ). Now we need to handle
       <em>
        optimization
       </em>
       ; that is, how do we find the best values for weights (
       <span class="math notranslate nohighlight">
        \(w_0\)
       </span>
       ,
       <span class="math notranslate nohighlight">
        \(w_1\)
       </span>
       )? How do we find the best
       <em>
        fitting
       </em>
       linear regression.
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate">
       If we know those we can iteratively take little steps down the gradient to reduce the loss, aka,
       <em>
        gradient descent
       </em>
       . How big our steps are is determined by the
       <em>
        learning rate
       </em>
       .
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent">
       In Deep learning, we use a variation of gradient descent called mini-batch gradient descent:
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="neural-networks">
<h1>Neural networks<a class="headerlink" href="#neural-networks" title="Permalink to this headline">¶</a></h1>
<div class="section" id="universal-approximation-theorem-nns-can-approximate-any-continuous-function">
<h2>Universal approximation theorem - NNs can approximate any continuous function.<a class="headerlink" href="#universal-approximation-theorem-nns-can-approximate-any-continuous-function" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>A visual demonstration that neural nets can compute any function: http://neuralnetworksanddeeplearning.com/chap4.html</p></li>
<li><p>Like any ML algorithm, training a neural netwoek requires minimizing some loss function (for a given structure that maps inputs to outputs).</p></li>
<li><p>The minimization is done using an algorithm called gradient descent, or a variation called stochastic/minibach gradient descent.</p></li>
</ul>
</div>
<div class="section" id="using-gradient-descent-in-linear-regression">
<h2>Using gradient descent in Linear regression<a class="headerlink" href="#using-gradient-descent-in-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>The simplest machine learning algorithm is linear regression. We will code up linear regression from scratch with a twist: we will use gradient descent, which is also how neural networks learn. Most of this lesson is pretty much stolen from Jeremy Howard’s fast.ai <a class="reference external" href="https://www.youtube.com/watch?v=ACU-T9L4_lI">lesson zero</a></p>
<ul class="simple">
<li><p>In linear regression, we assume that <span class="math notranslate nohighlight">\(y = w_0 + w_1 x \)</span>.</p></li>
<li><p>We look for the <span class="math notranslate nohighlight">\(w\)</span> coefficients that give the ‘best’ prediction for the output (<span class="math notranslate nohighlight">\(y\)</span>). The best prediction is defined by minimizing some cost function. For linear regression in machine learning task it is usually the mean square error.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;figs/linear_regression_as_neural_network2.png&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="c1"># Image is taken from https://blog.insightdatascience.com/a-quick-introduction-to-vanilla-neural-networks-b0998c6216a1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_5_0.png" src="../_images/gradient_decent_5_0.png" />
</div>
</div>
</div>
<div class="section" id="linear-regression-from-scratch">
<h2>Linear regression from scratch<a class="headerlink" href="#linear-regression-from-scratch" title="Permalink to this headline">¶</a></h2>
<p>We will learn the parameters <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> of a line.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import plotting packages</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Import machine-learning packages</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">Data</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>

<span class="c1"># from fastai.basics import *</span>
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Set random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="what-is-pytorch-as-defined-at-https-pytorch-org">
<h3>What is pytorch? (as defined at https://pytorch.org/)<a class="headerlink" href="#what-is-pytorch-as-defined-at-https-pytorch-org" title="Permalink to this headline">¶</a></h3>
<p>It’s a Python-based scientific computing package targeted at two sets of audiences:</p>
<ul class="simple">
<li><p>A replacement for NumPy to use the power of GPUs</p></li>
<li><p>A deep learning research platform that provides flexibility and speed</p></li>
</ul>
</div>
<div class="section" id="chose-the-true-parameters-we-want-to-learn">
<h3>Chose the true parameters we want to learn.<a class="headerlink" href="#chose-the-true-parameters-we-want-to-learn" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">w</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([3., 2.])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="create-some-data-points-x-and-y-which-lie-on-the-line">
<h3>Create some data points x and y which lie on the line<a class="headerlink" href="#create-some-data-points-x-and-y-which-lie-on-the-line" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span>
    <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">1</span>
<span class="p">)</span>  <span class="c1"># Underscore functions in pytorch means replace the value (update)</span>
<span class="n">x</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 1.0000, -0.4223],
        [ 1.0000, -0.2875],
        [ 1.0000, -0.6397],
        [ 1.0000,  0.7178],
        [ 1.0000, -0.1071]])
</pre></div>
</div>
</div>
</div>
<p>Tensor is a data structure which is a fundamental building block of PyTorch. Tensors are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU
and more importantly for us - they can keep track of its gradients.</p>
<p>For further reading, see <a class="reference external" href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">here</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># @ is a matrix product (similar to matmul)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_15_0.png" src="../_images/gradient_decent_15_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_real</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>If we could find a way to fit our guess for the coefficients the weights (<span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span>), we could use the exact same method for very complicated tasks (as in image recognition).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Written in terms of <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(w_1\)</span>, our <strong>loss function</strong> is:</p>
</div>
<div class="section" id="l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2">
<h3><span class="math notranslate nohighlight">\(L = \frac{1}{n}\sum_{i=1}^n [y_i - (w_0 + w_1 x_i)]^2\)</span><a class="headerlink" href="#l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_real</span>
<span class="n">mse</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># Initial mean-squared error</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(58.5756)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_hat</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_21_0.png" src="../_images/gradient_decent_21_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w_real</span><span class="p">)</span>
<span class="n">w</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([-3., -5.], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<div class="section" id="so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression">
<h4>So far, we have specified the <em>model</em> (linear regression) and the <em>evaluation criteria</em> (or <em>loss function</em>). Now we need to handle <em>optimization</em>; that is, how do we find the best values for weights (<span class="math notranslate nohighlight">\(w_0\)</span>, <span class="math notranslate nohighlight">\(w_1\)</span>)? How do we find the best <em>fitting</em> linear regression.<a class="headerlink" href="#so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression" title="Permalink to this headline">¶</a></h4>
<p>To know how to change <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(w_1\)</span> to reduce the loss, we compute the derivatives (or gradients).</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial L}{\partial w_0} = \frac{1}{n}\sum_i -2[y_i - (w_0 + w_1 x_i)]\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial L}{\partial w_1} = \frac{1}{n}\sum_i -2[y_i - (w_0 + w_1 x_i)]x_i\)</span></p>
</div>
<div class="section" id="if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate">
<h4>If we know those we can iteratively take little steps down the gradient to reduce the loss, aka, <em>gradient descent</em>. How big our steps are is determined by the <em>learning rate</em>.<a class="headerlink" href="#if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate" title="Permalink to this headline">¶</a></h4>
<p><span class="math notranslate nohighlight">\(w_0^{new} = w_0^{old}\)</span> - Learning-Rate <span class="math notranslate nohighlight">\(*  \frac{\partial L}{\partial w_0}\)</span></p>
<p><span class="math notranslate nohighlight">\(w_1^{new} = w_1^{old}\)</span> - Learning-Rate <span class="math notranslate nohighlight">\(*  \frac{\partial L}{\partial w_1}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update2</span><span class="p">(</span><span class="n">iteration</span><span class="p">):</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mse</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># calculate the gradient of a tensor! It is now stored at w.grad</span>

    <span class="c1"># To prevent tracking history and using memory</span>
    <span class="c1"># (code block where we don&#39;t need to track the gradients but only modify the values of tensors)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">w</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
        <span class="c1"># Under score means inplace.</span>
        <span class="c1"># lr is the learning rate. Good learning rate is a key part of Neural Networks.</span>

        <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="c1"># We want to zero the gradient before we are re-evaluate it.</span>

    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<p>In PyTorch, we need to set the gradients to zero before starting to do back propragation because PyTorch accumulates the gradients on subsequent backward passes. This is convenient while training RNNs. So, the default action is to accumulate (i.e. sum) the gradients on every loss.backward() call.
Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the minimum (or maximum, in case of maximization objectives).</p>
<p>Explanations about how PyTorch calculates the gradients can be found here (and in many other sources) - https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lin</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">w0</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="p">(</span><span class="n">line</span><span class="p">,)</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">lin</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">]),</span>
    <span class="n">c</span><span class="o">=</span><span class="s2">&quot;firebrick&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># line, = ax.plot(x[:,0], y, c=&#39;firebrick&#39;)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Loss = 0.00&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">update2</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Loss = </span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">l</span><span class="p">)</span>
    <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span>
        <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">lin</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">]),</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">line</span><span class="p">,)</span>


<span class="n">anim</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># You might have some difficulties running this cell without importing certain packages.</span>
<span class="c1"># might need to install: conda install -c conda-forge ffmpeg</span>
<span class="n">HTML</span><span class="p">(</span><span class="n">anim</span><span class="o">.</span><span class="n">to_html5_video</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><video width="720" height="432" controls autoplay loop>
  <source type="video/mp4" src="data:video/mp4;base64,AAAAIGZ0eXBNNFYgAAACAE00ViBpc29taXNvMmF2YzEAAAAIZnJlZQAAIs5tZGF0AAACrgYF//+q
3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE2MSByMzAzME0gOGJkNmQyOCAtIEguMjY0L01Q
RUctNCBBVkMgY29kZWMgLSBDb3B5bGVmdCAyMDAzLTIwMjAgLSBodHRwOi8vd3d3LnZpZGVvbGFu
Lm9yZy94MjY0Lmh0bWwgLSBvcHRpb25zOiBjYWJhYz0xIHJlZj0zIGRlYmxvY2s9MTowOjAgYW5h
bHlzZT0weDM6MHgxMTMgbWU9aGV4IHN1Ym1lPTcgcHN5PTEgcHN5X3JkPTEuMDA6MC4wMCBtaXhl
ZF9yZWY9MSBtZV9yYW5nZT0xNiBjaHJvbWFfbWU9MSB0cmVsbGlzPTEgOHg4ZGN0PTEgY3FtPTAg
ZGVhZHpvbmU9MjEsMTEgZmFzdF9wc2tpcD0xIGNocm9tYV9xcF9vZmZzZXQ9LTIgdGhyZWFkcz0z
IGxvb2thaGVhZF90aHJlYWRzPTEgc2xpY2VkX3RocmVhZHM9MCBucj0wIGRlY2ltYXRlPTEgaW50
ZXJsYWNlZD0wIGJsdXJheV9jb21wYXQ9MCBjb25zdHJhaW5lZF9pbnRyYT0wIGJmcmFtZXM9MyBi
X3B5cmFtaWQ9MiBiX2FkYXB0PTEgYl9iaWFzPTAgZGlyZWN0PTEgd2VpZ2h0Yj0xIG9wZW5fZ29w
PTAgd2VpZ2h0cD0yIGtleWludD0yNTAga2V5aW50X21pbj02IHNjZW5lY3V0PTQwIGludHJhX3Jl
ZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAu
NjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAAL
LmWIhAAT//73sY+BTcgADZc6inof4RWx9JBRerHZoGTqAAADAAADAAADABCOhyi2JfVxcZaAAAAK
aMYJOI+wxmPgAC7uAxKfClhXdEiiDs5SKd9S/4jjH0k4KTcA7K/Dv/ZKYTDB9HsK035GbnDB1dAJ
zCHLEByzWJ6o3Zj8q282zzjIp6irzKq1/t6RzYB/eFkvtToaWT+GaISV3vMU6Orpp3ap8CUKuNP2
fQhNuIgvY4ywAJWpoD519FMOTfg8dsmmwBWtwMVdlW+v02OzSQMDOhhBBlmK/3oxkI9B4d8278L5
k6UbxeQS0VfVo4ubehFvJE6p1VGQx5OFoo+lxARKEUJVTLQMb7QZEtt+dzXVwQW9Gfq+sbwHgjCB
R9wGjfM5FBhbmu997OVcTYEh+3N59SIta8jLHyBfyVGAtcbij9p2p+2J/eKc+h9nd1YN8FKThYtZ
UmACD+Zug4L3MuXfPsWYs7km66DPj2wwqxH4yUfDZWySbHmXnQutUy/FEACHlNM1Gjrj6tz/rlsm
IHXFr2EWCG+g0EXI0rRjbiAMcEA4uLD99OOa2Xv6w1Hq0qK1lqmx/W8CgXvOHXZnviGpn1a/Bdir
AOLhlByQ7WY24QqzbFD7qWZuNj9fhVi/OKRAT9GP65x93al+PGHSZ4B9scQltxfDE6uSPNlEC2Up
cXvHpolxbn0BUr1pmaOn4oU0TuhYmykicbl0jY/roc+L7v6jDsj+AtCsAVu/+aMDsIBISoK37Uuw
+ko1yXhsDGyaKGS9Ca/BO2HTgsHIadCp1ouWYuom5YHJx/VekOWe+PrtuHc9Yv5PckW7qbbf37PA
ku9kN9/Xy1JvWhtvyDyYpjaXyjRujMU+ekDCQ57ZXeK6AMD7c0xt5w1unSofsuK75+ubAU712CNR
t/grf30Tcl9ZAmCgrouDPTT0IRDBFigIadPHmPi+7j65y75DWqVDEIDZqa3Z3gl/SH8nhJxm0rYv
rqTLAffKo+5/RJnUpmGQlYs4AUYCPHK2SxFIKPS60Gn0WK/GhdWDBFiebzwawuOomAULxj0OVgJC
U0wqhWsy/2qUdmWJ18cU/qnzPjNt9qMWGdZMlYo/0jnHiLMxXpOhHi50aaddnHgK8XfqGgHNiPpm
dZTxesMVHUEvA57owSHG3xIoENvNQDOGoFh2tVhaX8sGbPp19LY1Rz3OoWBLR8pQVWtVL/U+7+n2
/dlCCUGx10KmB6e0jJjhrcgHTGmZsvLMd95zZ7vVgzOva6DslBbOK7p/4dBmH/dfYmrNLnu7io3e
MwDa7X8JrRCXD4bsLjUUdXNkZisQIekNcnynO/wO23tgyxtlCyhDDDLJd5PZfDXodLguVHc7cRGt
39UNRCG/x0a8PM0Z48+PaetMYXtbM3V/XlgRYDVJxt4e41S3v27rL2SiGMVbgnVX2iAXrtcfQVl2
3MS7ZBYO1RT2gBoxgI+oS8M21Dn4LzbLCn49ZVYiyatoSztkcHuN925ncgnUjbglK1BkCzGZjaY+
pjzBF3pz5w39CqJn82XW8UYpQh7aQeNWrlIPDFU2FcHFwEUF7BDJheDNNW5jmbBVUcrYX2X64vnC
1x6t84TbBhznG+hDuiwy1zanRKBOzuIvn+coc1Hu5zicp/AV6iO/N/qG6BrCwJjOkKjE8elt+Dcf
t2xxC8gKbbOWuXp96e9g4h6op3gqt2819rIutUpt3WfYBU788nvAAcooeEdZLkadvDnuVUWCbKP4
xEhPCqi/KCAMvtbVqgcVr4Y4GGvVWJ/tZFu6ZCAP0Zt0d1UpjA/VankYv0z6k68IIz9sUlYi2N6k
MvgvR1gGws44g4w46MMUkHpuzDllHbtCbhz0ppVydE+Rxh2VW6Y8wPmZgM6BoXAVwFcWBwgSxSME
/4CVnSnNH+F89aXomqsmTq8UJHfTNvxPthr0WM/wAAADAACvjzN/cZNcIg6GbBd+Vi0CcY9+bAIm
D//obzLRs4SW5HYn+nl3mFRc5H/3dNn4cLoDkOQb4A01xOmK4WLA5UVcILq7UzFbQvWnuV7/60Yf
d1IUPp/IKhCIErCpQdhaP8XF75A/AempYYVAdV6aMR55g71Ujjp98DQwg0AAAAMABEhbcXHxNwxJ
aQLCTAGPQdgR+kPS4KKE9xGwpi8YVnh92ILEyBhTpeSIVj+XT0APOBPBJaTuIRZx0uUkwIaewkWE
mDV3W/1R/ItSNAI1z5Cxm0+0iZgAL3LKBKviTSUpv8w8DkphqvvyX13ImB+2wEzr0Ls41epViOtv
mzCeqYM4al5UN5emmoR/NHoOwAAABwa3xVwsmpCWMO0Q4zRRxUW1mmQhySLWpPYtvpZ8x8sJd9lN
tuLShjCLNTPL2h+N+JP82JAT35BGBTup55+bAkm7jGuXgQ62l4wkhq+x7TcEzmd0I/cxHXWNvb+e
yOrP5Jiu0OxCjmFzs55BYUCBVq3/j0NFqYW1TdE2fuUNOafJngrti1wGq5tD8c0//80QXX1hMR8q
0f+pda7zH0JMZLDfzt/XAwPbTaqlTpGU57xkJ3WmmK3jGV2/SB9Qdir2u+CqKYFihP81XxYbkssw
DhPuSjHL1feE4JsFP3guEACN6yO60GjPKoqCz2OscNMbFarAz9VV0abAjR1LGAJZDPntFUYHLQQA
WJP/YDK8UdR+BER/BwrK3AwG1T0mnkxN9xbFoLD5WtKmCo2iN2+9/461FsRcIU/oyvUjwv8k8Sty
bqezbr8SzXxc82B1crywaUJdNP97TQ+p85DpvSF72QCs/TEp6nyC07Xrs//k5IqvNkCiWWMslwNL
5+yL/3qxi5g2Acl6OaGPw+pPjkdtIiNzkhOxlS1uQPwjYePsmRrIadDRfK/NST49B2JMJk9Pu6+r
OEwdBhbydQgdKnBLWJwwSp2w4gaqgFmrQWN6DE1187RGrqcZJDMH9KnAWdWjKUgHaQAyK7QjC5Ke
66AY+0BmMl8xkNk6YqVlzaBluZC/odEFYpHy9GOcg+2pejicKTJDyA9kN8QfHpDsmxiE4fBv4J9u
2FF2uA9qemlTMkDjJHEBKj9LJL1XP8PtXhAkVbDyI/Y1PlLrG86kKyFrr/zX2YPpzzpfTSRXjEd7
3Q2q7I6NJZM4zYYWjHXygYnG90E1KNZ/oP+BhEFsKFJfiK7pqH5C6sddCVpqpEcYOtxVzbeuL84h
FX7X7o+2hYhaiQ2UJGMHbOxd3uJbOb/U9EomuaivXboh1GcRxhXIr2iCNLwEqQDCwFYNKe44RoQw
Gf6Ewzntdoa7Rg4WyQjWN14v4o0kEAz6Rf3MBqYgihZDS9PRL8i0USm61ojjZums5dKAwj2YAbvC
qeDwMi8sVUfq4tkFutEZZ9/LYc1TIM9sytcojzxZZ7o3rs32x2xed1/uf9MKZkzQt+UaXeybZUdk
/I2FsUyOUKUaaQJ99jafqkYuEdIUNMmt1adn8pguQ6Mxdyn8K8heEB8kcC7+9wesDjUPipnq0uwl
i5kYJ7/QMXLujKJYuxxIZTRsBm/oIKT23/twzjfD43Juct4AePfkIPb7zInnPAnsKJp3Nlm43kiX
K9Y5hgR5rehSxmRzus+HI7Old9zzoAydRveZcOpkd4QP+BmZwCryFLwUA0uYr112Upsi/1Lscv76
P653EWacooGO3bJQnj+zdyDQRzRJud9WwBovQ9wU1rw/kCPbu4dl44ioH1uhvr8eKY29fgm4KUaW
hHne/9QSbxSeGD/YLamMBtnj3hjTiQCi0/Oyd7ii7uVPKk/nkUxmXdbNWJkGzMYT7ctfT6rYmubu
tJLVT2ACR6hHuAAl4QAAAVNBmiRsQT/+tSqAB0lyNSZm0qwA4zjvBXHB1SeObxu2tBddlktiPM6+
h1u8SS+OxJVcmpcJaLvGbOtupfckV0UUm1a+POzcR3SPDOWEmDaoCTKn1gWHQ3YTDSNZ1+NIWzCY
7UQGwUeVUgcJXLGLOUpDruKfksYwzwvItLWDYQopz+pQHm4ClI3/iMh8afFSTQPADWI8N2xFB2uR
gsa4lRZmWVoJXX3SC7koxlrRFQvyRA9rdMUGYlsdXEcHcuXPe8R64Wpp02HWX8xcq5BQVQiGYoMd
3X+H29AC6eNh2cbj3MONg90CQ7mLa8S3AtNolCrq1yBV3kWQgkchandsFVwqhRrgbkm11Ss3k1N0
qtFogdSge0eZRW+R82Vq0BA3NGfqItSTgRKFRZGNY+dwUg+rZD5yvSgffNwl+INUHr4v4Kk6Ph1/
WpYceiP4E/u+DSEPg8AAAAC/QZ5CeIf/ABRyc+xAdLQAQp8iV3GpIzLIKvFl5spTk28VydPsVzLr
YHrdpNygRTdGbR6iaOmXevBPhnB7ieIhbB3dKSDGJfRN2/Oh0GP5jhUoraTs3MWnZ/l//rLitOOO
cXqDKdBe0VWiVr4ttygCEUZg+SGQsO8reGkTV9XHbJnaq1FDqlIMXXuC+55MeIWyowKv9C4x0oBX
ST4gWcLBGu4DGHOcng/VXUY92/zujKKAJ7Eh4E9pWmNQtnkriTkAAADMAZ5hdEO/ABx67XCKkmKx
QwAbVVDx/HSNPzqR6Dg4SknCZGPfU4/F3pfBFMvSu/pg5ilzK3UF99/BOVHx2P6FqBfoN6hlYJMc
8Ha+tR2HKBYmymgTe1vNQ82+IxbEv9+rr2cQsFAoI3Kd0FnFdFJn7QBcdHCjy56Erxc/EsE8j3J4
A/HywRTRGfhZCmC0b8Gf1hmNQC5jaOiEjwGeKZXbisAlYrHqIxYNmtZ52iCqICLHDtElK4vUHZh/
1dVV6fb4E9tGBIqJYQqEPB/gAAAAbAGeY2pDvwAaPTCq9eLSADZyCzh98j5Ose83dW4U3KduS45x
NDO1oc9rrK93/mm0Tx0eU0MwS7hJaghTnZgcMIYyVmYDYp61S1mapMiXXdPdeW0Xi4NVyiwgBfcO
jtpzu9FzocrBgTqQoAABTQAAANdBmmhJqEFomUwIJ//+tSqABtAsWDAsAJYPGc5JWhT9PltlY62B
c9VEihtAS0kbzwxPZRUj3/VZieBSj1+9yq9em76qH540MiGP3PpJbq2GjVV19I0jkSuCUcYAGLum
PThmcXNXyuqPHVr6cqtoFTKzCLaeCZyitI1ESovfq9tdYrAT5av4mHXPR2vtp9vO545wKGul/nGb
WFHALpXZWgTpC98ltk79jguGl6rkZ1FB4GjAXEZYkxUBOpXyx6oO/LNCaKLKNd8OL4uGtvHIz2Uu
wAAAAwBUwQAAALFBnoZFESw//wASzh2ZgAvqSxTZWi9Pqr8/GG6qd+fzLi38GgutWBEL9/Elf5EG
Bt2MgzU7KEQH/7tjq86hWPRIOoCGnL9uuQxLhIaudo5BC5mQgIoPosHzlOlJscl9BYvlVKcWdN60
xmSIG1hph3taulSolLFXHJwE0F8qap78SidYrH27aiVZVSv/cPhxLN3QDGERAcBzWaMciUWI5H7e
iGo4ocs7p265G8gAAAMAAfcAAACPAZ6ldEO/ABo+3kSBHSacADaot4/f6CFf33+oFSk9RxjFW7Gd
D0idMau+q5+TT9Cr0IXN4RVpoi/rf2rr3fJBaeB4zX6T1JNkWmwXSf9INLudiHmZ8EgTAo4YJ3Q8
d7m//m6cpmHhx2uJLfr9AEYvHZgLhwr8yD+0BWGVOgFrP3Bd81xaezA3/NAAAAMAA+8AAACCAZ6n
akO/ABqNj/8BrEP5wzABfXwzNGhmniiecx59v61P4Pcvu1OC1Oa6OUEOgNjKWCcV0TqGrInD/9y+
OuFnJtqqhP10gnWzzk980byPActi8ONxbqcRnd5s/tlsAKo6oZIJFgmM5KrCUyeJ/bwbI1YZ5mZ1
H5p7Je8YAAADAACRgAAAALVBmqxJqEFsmUwIJ//+tSqABst0uQAcdSzOclECv8l9NKknZ4LBF4oB
m4hLD8v/sxPf4NYhERi1swDfZwLec+9fDKgf4DXAgI0CN9OQjUxvzpaAdEP2s3GcCfdAyowZ7CWV
DbE8P6A//5YGfpjLkWvEou6tY1bgJP4Nwfds6QS6rT8GZLxq7fttfQoR2L6iOtUfaZRRfxqLyPIv
Xocq4Yrxozp3JnEHVY1pxJzFDopAAAADACFgAAAAbEGeykUVLD//ABLWgvuVON1tB7cjpYANqux2
s2FeVgoy7OWd2NahOJKebbpca2Ji0fPYHl/pyKUjxGhN3vuhZKb/vJuqqnfmHkCJZKw1sBzOk1tE
adEfP9g9mzPhYc9y3mHVZikMAAADAACtgQAAAGkBnul0Q78AGoxtmXxu4SWKlAA7S3ffqZCpOi8c
VxKx/mbLmGOGm8k8PnIepSjNzAyJrZcHoTEGBiqZ3TJ45SyArau4G9OSA+IGYfqB9poY+exfH9pY
/a4hqA9iHBHwTliLmgAAAwAAfcAAAABmAZ7rakO/ABoMJgAvrtl/YNisCvOlI1U4W3FGnNb51pn4
QDHFZxf0NvFoAKgAP/YSj6J78Z+HD4uDQtVH/xv81c9NinQmGs9Ps5xnUxxOEb0NTBzz+xmhT9CZ
kM8E7pfeAAADAAGBAAAAp0Ga8EmoQWyZTAgn//61KoAGywpcgBGLjvBXT4a+4ZltLVG40XWu1zL1
HglY3pDYqrzuIWhcbit176vL3JusUqD8b6tvSPPbrVgF1z2r4CtBFCNjrGjj1adf9QyeCisszSN5
xjKJ+Xb4B5SqpHycW5g1dcjs4iR2fiFps8sPmG7hFAAPGR7LmlppElkLug2OzjQPNUoTzxrfk0zd
jOD5s0fgAAADAAXdAAAAdEGfDkUVLD//ABLWgdEBRPVxsAF9fIOl3uNhlwfT7XVCGQshaSm3Eut4
/6ygcbry1RQyvcTlEDor7VmgrO4rs1cy9wVEGTDOKwMA6TthHeauKnXuAg7FJ2M39rBlxaneI70c
VdrBAoOOlM7fLMywAAADAAK3AAAAgwGfLXRDvwAakT9ae2v5AfF91AA7SfjDLaRJR61WbUnORNon
6eaw61Dw69rZ8/FmzcuGa7b8DF9ZPuzVd8neyDxnAyWjlrR3uYnOOYCHNrUMfg1I1/Job0plX5kd
F7KBzHkhaRGByMyJ/gH+jWGNoQWwUBilsDnTp5Tn600AAAMAAD7hAAAAawGfL2pDvwAZmrQ1uupV
ilf8mAC+vQI+YHz6X/W+xKbTUXQ3OQzkTMiaysMSPIKhGZqv3/BjnW766BeK8wkmdP7GdtQXvqjl
XgGLmvhlXyMwLumEAVx0bUzGQ15fCg3O9e4LDDzgAAADACDgAAAAmEGbNEmoQWyZTAgn//61KoAG
omn1WlhZ84ADLjFTqfbfo7BcFkshr2SGiXGC4s6oIb+x73FqJnwQsH722a5k3RIKeJF7GqKj2AVq
MdENrU69uVli9jXuKEguLL0jmxfWpbsPaHkJOAzuHTVIzAegM4ENenDvD7o4N/H/dVWrmmNFJJ5b
YuudV1WvMUsQCAu31Rpf3oAAAG1AAAAAbkGfUkUVLD//ABIZueJPMQRsADRbPC7rm7+4G3JtoEet
B1Mek4qRl8h7cVBDwAlc0n5XB/gnU7cV7wnLGGLckKQIuqyQB9EwxoY8DhBCvfd+XSHQXVpS1Iyz
04hTNPkYeJ5k6Vm39HFQAAADAA45AAAANgGfcXRDvwAZ8aFwAL6+GlchAhxx4rkquHvyeKz5mija
2sBuRb+vm52AeaU/NIunvAAAAwADAgAAAEABn3NqQ78AGe4YcADtLn+xrphcGQA0AMEPJP7/O4IJ
5Q2tb55EuETqT548mRtHMQCrTQrAw0/WIpyAAAADAE3AAAAAl0GbeEmoQWyZTAgn//61KoAGo/N7
EABCqkl3zW4Rmi+mtQdyTwvgbD2dL7BGmbodOS0KCVmoEEov9c9fnQuRCmLxnglTgOABEnNnWsR3
h5BdeI+8FlSRJtVHqUd9cVd4k0yfYLAqeS5qIL6E1LMPKWlFQv9KIMKyjkCNsl0v+rlk1KpE49sj
j00Rf/sHqS8C3tAAAAMACLkAAABpQZ+WRRUsP/8AEhpjYI0Qm4ANpRErvwR6CxxIEVWmQPeNPRUG
mJPDvs5nviCYj54jNtEgKN3Jbv7Rij8rK4a6RixApdjCpA+3+Zokd2yZvDwvgjXe5Gury6tQBbzj
eIqmcjLgAAADACDgAAAAOAGftXRDvwAZ7LI4AHaXP9jXTC4MgBoAYIeUDGTpJCdOOQ1Qw1FyQrlJ
IsPRmW+hUV5wAAADABBxAAAASAGft2pDvwAZ7hhwAO0iWNljB86lp+zdaEUwzXk/24isQOhPRlkF
8SxcI6Bo2vkt3xj20twMGL3bzro9N4omUBVdkAAAAwAOOQAAAIFBm7xJqEFsmUwIJ//+tSqABqJW
9EwlwdbAAhVSvG0vGvRgeaezkc9OA/d3sndLE5OdMWQEiW1b9OrfQcrw9BKoilRiKK3numWXfhoi
rTwJjPOPkZF/0HgQb+zGH87n21XdKeb72LRUv89Q3lQQYtroYmKkWI/Edf/w0WAAAAMADFgAAAA4
QZ/aRRUsP/8AElhNZgAvr5EHTgeHuBOaqq3p6bbh/v9CKTc4VOy1o6C3fDJDyi9n2y4AAAMAAg8A
AABZAZ/5dEO/ABoDSQAX11wv4ErKWdGO2hONCFtAP+0ZnsyaDUCfVgNaYKMlr/xwjSMhCE4Hndya
4G3u5Qh+MOpG9kIi9xRuRMhQ7XLILhIIV52KXZAAAAMADjgAAABIAZ/7akO/ABnuGHAA7S6AX3NN
BRhdv1BoRW2HnRRKMsuYZfrGWsz4QSimGElA6ZFOob7t7zTBZMfst72RG97XvfzgAAADACDhAAAA
WUGb4EmoQWyZTAgn//61KoAGo3S5ABO6kl33dhjjvz7TXvCwWV8ETOt88B+/vn/CM3GmKf61b0Pq
5vmIkXj9quTZ5hYuwR/C5f+FiTaQa0M18NFgAAADAAxZAAAAQEGeHkUVLD//ABJYPPoAL2K8glJi
un3ss1TxgxKIlr/Kzy+7bYWIy2m/WHAPCx4SmUy0G8zbvi9HFQAAAwAA44AAAAAyAZ49dEO/ABnx
oXAAvr4aVyI2CJzKQM14sm9tinX00zdNvf4l7pKa3kG/f5wAAAMABBwAAABZAZ4/akO/ABmfcFqS
AC+u6RA91jYSmcSWh14VPOvr1j9mN6rtVaNaVgmuANtEkF9BybDKr4QHrUNIk6Wtbs/NaxSdOqHK
IVPq3Rg86Fl7PvQS7IAAAAMAccEAAABHQZokSahBbJlMCCf//rUqgAajbJQADl4d4KsD2R+PIYQT
+K641taiRBpzmUP67x2PWZAoa76PBmUq2ilFa2VDRYAAAAMAMWAAAABGQZ5CRRUsP/8AElaiOACE
FCV3zDy/k60GERXnuXod+n+yfGwmSim/QXaYDG3xFaevGfAJTHGXT0gReYZwuH2YqAAAAwAHHQAA
ABMBnmF0Q78AGex4kFOQAAADAAm4AAAAEwGeY2pDvwAZ7aW3TkAAAAMAJuEAAACjQZpoSahBbJlM
CCf//rUqgAajhBgAbtudNB///xVGTuHk4NyVf+AjoYZlTew8nN/DhqyJc2zg/Npg2AVOx+yX8pG6
ICG3QAhr6r9wmHrksYyoEn5CZf+yhMpc+B7oneGMR2dbBplbIJ5Z3Il1EObn5vEKYdNU8qs9v0qP
kBbbjpRl9oSZ+bGPi4Bsz4UjYdqYeWMpkBAiqgfW69sNgAAAAwAh4QAAADtBnoZFFSw//wASWWOr
AB/SGR/GfnfmE6Cb13cwh7Ugyh/BV2NGyUEvDPjvqF4rk7oMd3JXOAAAAwAGBQAAABMBnqV0Q78A
Gex4kFOQAAADAAm5AAAAEwGep2pDvwAZ8Ef9qcgAAAMABNwAAABSQZqsSahBbJlMCCf//rUqgAaj
dLkAHHUsznL8uV4HIryoY24eT82b/WSz6quC0iHG1QLBz9Z2fmhDjUhT7Vf080d9bV6P1pGYy+pQ
wAAAAwAU0AAAABVBnspFFSw//wASWDW5v84AAAMAAYEAAAAQAZ7pdEO/AAADAAADAAAPCAAAABMB
nutqQ78AGfBH/anIAAADAATcAAAAGUGa8EmoQWyZTAgn//61KoAAAAMAAAMAA9cAAAASQZ8ORRUs
P/8AAAMAAAMAAArZAAAAEAGfLXRDvwAAAwAAAwAADwkAAAAQAZ8vakO/AAADAAADAAAPCAAAABlB
mzRJqEFsmUwIJf/+tSqAAAADAAADAAPWAAAAEkGfUkUVLD//AAADAAADAAAK2QAAABABn3F0Q78A
AAMAAAMAAA8IAAAAEAGfc2pDvwAAAwAAAwAADwgAAAAZQZt4SahBbJlMCCX//rUqgAAAAwAAAwAD
1wAAABJBn5ZFFSw//wAAAwAAAwAACtgAAAAQAZ+1dEO/AAADAAADAAAPCQAAABABn7dqQ78AAAMA
AAMAAA8JAAAAGUGbvEmoQWyZTAgl//61KoAAAAMAAAMAA9YAAAASQZ/aRRUsP/8AAAMAAAMAAArZ
AAAAEAGf+XRDvwAAAwAAAwAADwgAAAAQAZ/7akO/AAADAAADAAAPCQAAAGVBm+BJqEFsmUwII//+
tSqABqNtiwAuq3gVrYlNrofQNU8pp/+KDmltsBrTjoW+EOvUd+Er7Btx7Lg2gBfpFUgKD2a6vCHK
MxTHGesZ67q6nDTBI/8oTK8MSE9im/bDIAAAAwAI+QAAABdBnh5FFSw//wA4XY/hTXG8gAAAAwAH
3AAAABMBnj10Q78AGfFQRqcgAAADABNwAAAAEgGeP2pDvwBQVX+rAAADAABDwQAAABlBmiRJqEFs
mUwIf//+qZYAAAMAAAMAAB4QAAAAFEGeQkUVLD//ADhhfdhAAAADAA/JAAAAEgGeYXRDvwBQR6NK
AAADAABxwAAAABIBnmNqQ78AUFV/qwAAAwAAQ8EAAAAZQZplSahBbJlMCHf//qmWAAADAAADAAAe
EQAABnptb292AAAAbG12aGQAAAAAAAAAAAAAAAAAAAPoAAApBAABAAABAAAAAAAAAAAAAAAAAQAA
AAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAACAAAFpHRyYWsAAABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAApBAAAAAAAAAAAAAAAAAAA
AAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAC0AAAAbAAAAAAACRlZHRzAAAA
HGVsc3QAAAAAAAAAAQAAKQQAAAwAAAEAAAAABRxtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAACgA
AAGkAFXEAAAAAAAtaGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAATH
bWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAA
AAABAAAEh3N0YmwAAAC3c3RzZAAAAAAAAAABAAAAp2F2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAA
AAAC0AGwAEgAAABIAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8A
AAA1YXZjQwFkABb/4QAYZ2QAFqzZQLQ3oQAAAwADAAADACgPFi2WAQAGaOvjyyLA/fj4AAAAABx1
dWlka2hA8l8kT8W6OaUbzwMj8wAAAAAAAAAYc3R0cwAAAAAAAAABAAAARgAABgAAAAAUc3RzcwAA
AAAAAAABAAAAAQAAAkBjdHRzAAAAAAAAAEYAAAABAAAMAAAAAAEAAB4AAAAAAQAADAAAAAABAAAA
AAAAAAEAAAYAAAAAAQAAHgAAAAABAAAMAAAAAAEAAAAAAAAAAQAABgAAAAABAAAeAAAAAAEAAAwA
AAAAAQAAAAAAAAABAAAGAAAAAAEAAB4AAAAAAQAADAAAAAABAAAAAAAAAAEAAAYAAAAAAQAAHgAA
AAABAAAMAAAAAAEAAAAAAAAAAQAABgAAAAABAAAeAAAAAAEAAAwAAAAAAQAAAAAAAAABAAAGAAAA
AAEAAB4AAAAAAQAADAAAAAABAAAAAAAAAAEAAAYAAAAAAQAAHgAAAAABAAAMAAAAAAEAAAAAAAAA
AQAABgAAAAABAAAeAAAAAAEAAAwAAAAAAQAAAAAAAAABAAAGAAAAAAEAAB4AAAAAAQAADAAAAAAB
AAAAAAAAAAEAAAYAAAAAAQAAHgAAAAABAAAMAAAAAAEAAAAAAAAAAQAABgAAAAABAAAeAAAAAAEA
AAwAAAAAAQAAAAAAAAABAAAGAAAAAAEAAB4AAAAAAQAADAAAAAABAAAAAAAAAAEAAAYAAAAAAQAA
HgAAAAABAAAMAAAAAAEAAAAAAAAAAQAABgAAAAABAAAeAAAAAAEAAAwAAAAAAQAAAAAAAAABAAAG
AAAAAAEAAB4AAAAAAQAADAAAAAABAAAAAAAAAAEAAAYAAAAAAQAAHgAAAAABAAAMAAAAAAEAAAAA
AAAAAQAABgAAAAABAAAMAAAAABxzdHNjAAAAAAAAAAEAAAABAAAARgAAAAEAAAEsc3RzegAAAAAA
AAAAAAAARgAADeQAAAFXAAAAwwAAANAAAABwAAAA2wAAALUAAACTAAAAhgAAALkAAABwAAAAbQAA
AGoAAACrAAAAeAAAAIcAAABvAAAAnAAAAHIAAAA6AAAARAAAAJsAAABtAAAAPAAAAEwAAACFAAAA
PAAAAF0AAABMAAAAXQAAAEQAAAA2AAAAXQAAAEsAAABKAAAAFwAAABcAAACnAAAAPwAAABcAAAAX
AAAAVgAAABkAAAAUAAAAFwAAAB0AAAAWAAAAFAAAABQAAAAdAAAAFgAAABQAAAAUAAAAHQAAABYA
AAAUAAAAFAAAAB0AAAAWAAAAFAAAABQAAABpAAAAGwAAABcAAAAWAAAAHQAAABgAAAAWAAAAFgAA
AB0AAAAUc3RjbwAAAAAAAAABAAAAMAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhkbHIAAAAAAAAA
AG1kaXJhcHBsAAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAAAABMYXZmNTku
MTYuMTAw
">
  Your browser does not support the video tag.
</video></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;figs/Gradient_descent2.png&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">700</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_29_0.png" src="../_images/gradient_decent_29_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">update2</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">losses</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iternation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Loss&#39;)
</pre></div>
</div>
<img alt="../_images/gradient_decent_30_1.png" src="../_images/gradient_decent_30_1.png" />
</div>
</div>
</div>
<div class="section" id="in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent">
<h4>In Deep learning, we use a variation of gradient descent called <a class="reference external" href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/">mini-batch gradient descent</a>:<a class="headerlink" href="#in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent" title="Permalink to this headline">¶</a></h4>
<p>Instead of calculating the gradient over the whole training data before changing model weights (coefficients), we take a subset (batch) of our data, and change the values of the weights after we calculated the gradient over this subset.</p>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Neural_network_for_Lorenz96.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Using neural networks for L96 parameterization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Learning-DA-increments.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Learning Data Assimilation Increments</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By The M2LinES Community<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>