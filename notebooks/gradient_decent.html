
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural networks &#8212; Learning Machine Learning with Lorenz-96</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Learning Data Assimilation Increments" href="Learning-DA-increments.html" />
    <link rel="prev" title="Using neural networks for L96 parameterization" href="Neural_network_for_Lorenz96.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/newlogo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning Machine Learning with Lorenz-96</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L96-two-scale-description.html">
   The Lorenz-96 Two-Timescale System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gcm-analogue.html">
   The Lorenz-96 GCM Analog
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gcm-parameterization-problem.html">
   Key aspects of GCMs parameterizations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="estimating-gcm-parameters.html">
   Tuning GCM Parameterizations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DA_demo_L96.html">
   Data Assimilation demo in the Lorenz 96 (L96) two time-scale model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Neural_network_for_Lorenz96.html">
   Using neural networks for L96 parameterization
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Learning-DA-increments.html">
   Learning Data Assimilation Increments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="random_forest_parameterization.html">
   Random Forest
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Neural-Network-Saliency-Maps.html">
   Generating saliency maps for neural networks trained on L96
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Neural-Network-Advection.html">
   Using neural networks to parameterize advection in L96
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bibliography.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/gradient_decent.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/m2lines/L96_demo"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/m2lines/L96_demo/issues/new?title=Issue%20on%20page%20%2Fnotebooks/gradient_decent.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/m2lines/L96_demo/main?urlpath=tree/notebooks/gradient_decent.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#universal-approximation-theorem-nns-can-approximate-any-continuous-function">
   Universal approximation theorem - NNs can approximate any continuous function.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-gradient-descent-in-linear-regression">
   Using gradient descent in Linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression-from-scratch">
   Linear regression from scratch
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-pytorch-as-defined-at-https-pytorch-org">
     What is pytorch? (as defined at https://pytorch.org/)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chose-the-true-parameters-we-want-to-learn">
     Chose the true parameters we want to learn.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-some-data-points-x-and-y-which-lie-on-the-line">
     Create some data points x and y which lie on the line
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2">
     <span class="math notranslate nohighlight">
      \(L = \frac{1}{n}\sum_{i=1}^n [y_i - (w_0 + w_1 x_i)]^2\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression">
       So far, we have specified the
       <em>
        model
       </em>
       (linear regression) and the
       <em>
        evaluation criteria
       </em>
       (or
       <em>
        loss function
       </em>
       ). Now we need to handle
       <em>
        optimization
       </em>
       ; that is, how do we find the best values for weights (
       <span class="math notranslate nohighlight">
        \(w_0\)
       </span>
       ,
       <span class="math notranslate nohighlight">
        \(w_1\)
       </span>
       )? How do we find the best
       <em>
        fitting
       </em>
       linear regression.
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate">
       If we know those we can iteratively take little steps down the gradient to reduce the loss, aka,
       <em>
        gradient descent
       </em>
       . How big our steps are is determined by the
       <em>
        learning rate
       </em>
       .
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent">
       In Deep learning, we use a variation of gradient descent called mini-batch gradient descent:
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Neural networks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#universal-approximation-theorem-nns-can-approximate-any-continuous-function">
   Universal approximation theorem - NNs can approximate any continuous function.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-gradient-descent-in-linear-regression">
   Using gradient descent in Linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression-from-scratch">
   Linear regression from scratch
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-pytorch-as-defined-at-https-pytorch-org">
     What is pytorch? (as defined at https://pytorch.org/)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chose-the-true-parameters-we-want-to-learn">
     Chose the true parameters we want to learn.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-some-data-points-x-and-y-which-lie-on-the-line">
     Create some data points x and y which lie on the line
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2">
     <span class="math notranslate nohighlight">
      \(L = \frac{1}{n}\sum_{i=1}^n [y_i - (w_0 + w_1 x_i)]^2\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression">
       So far, we have specified the
       <em>
        model
       </em>
       (linear regression) and the
       <em>
        evaluation criteria
       </em>
       (or
       <em>
        loss function
       </em>
       ). Now we need to handle
       <em>
        optimization
       </em>
       ; that is, how do we find the best values for weights (
       <span class="math notranslate nohighlight">
        \(w_0\)
       </span>
       ,
       <span class="math notranslate nohighlight">
        \(w_1\)
       </span>
       )? How do we find the best
       <em>
        fitting
       </em>
       linear regression.
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate">
       If we know those we can iteratively take little steps down the gradient to reduce the loss, aka,
       <em>
        gradient descent
       </em>
       . How big our steps are is determined by the
       <em>
        learning rate
       </em>
       .
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent">
       In Deep learning, we use a variation of gradient descent called mini-batch gradient descent:
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="neural-networks">
<h1>Neural networks<a class="headerlink" href="#neural-networks" title="Permalink to this headline">¶</a></h1>
<div class="section" id="universal-approximation-theorem-nns-can-approximate-any-continuous-function">
<h2>Universal approximation theorem - NNs can approximate any continuous function.<a class="headerlink" href="#universal-approximation-theorem-nns-can-approximate-any-continuous-function" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>A visual demonstration that neural nets can compute any function: http://neuralnetworksanddeeplearning.com/chap4.html</p></li>
<li><p>Like any ML algorithm, training a neural netwoek requires minimizing some loss function (for a given structure that maps inputs to outputs).</p></li>
<li><p>The minimization is done using an algorithm called gradient descent, or a variation called stochastic/minibach gradient descent.</p></li>
</ul>
</div>
<div class="section" id="using-gradient-descent-in-linear-regression">
<h2>Using gradient descent in Linear regression<a class="headerlink" href="#using-gradient-descent-in-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>The simplest machine learning algorithm is linear regression. We will code up linear regression from scratch with a twist: we will use gradient descent, which is also how neural networks learn. Most of this lesson is pretty much stolen from Jeremy Howard’s fast.ai <a class="reference external" href="https://www.youtube.com/watch?v=ACU-T9L4_lI">lesson zero</a></p>
<ul class="simple">
<li><p>In linear regression, we assume that <span class="math notranslate nohighlight">\(y = w_0 + w_1 x \)</span>.</p></li>
<li><p>We look for the <span class="math notranslate nohighlight">\(w\)</span> coefficients that give the ‘best’ prediction for the output (<span class="math notranslate nohighlight">\(y\)</span>). The best prediction is defined by minimizing some cost function. For linear regression in machine learning task it is usually the mean square error.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;figs/linear_regression_as_neural_network2.png&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="c1"># Image is taken from https://blog.insightdatascience.com/a-quick-introduction-to-vanilla-neural-networks-b0998c6216a1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_5_0.png" src="../_images/gradient_decent_5_0.png" />
</div>
</div>
</div>
<div class="section" id="linear-regression-from-scratch">
<h2>Linear regression from scratch<a class="headerlink" href="#linear-regression-from-scratch" title="Permalink to this headline">¶</a></h2>
<p>We will learn the parameters <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> of a line.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import plotting packages</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Import machine-learning packages</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">Data</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>

<span class="c1"># from fastai.basics import *</span>
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Set random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="what-is-pytorch-as-defined-at-https-pytorch-org">
<h3>What is pytorch? (as defined at https://pytorch.org/)<a class="headerlink" href="#what-is-pytorch-as-defined-at-https-pytorch-org" title="Permalink to this headline">¶</a></h3>
<p>It’s a Python-based scientific computing package targeted at two sets of audiences:</p>
<ul class="simple">
<li><p>A replacement for NumPy to use the power of GPUs</p></li>
<li><p>A deep learning research platform that provides flexibility and speed</p></li>
</ul>
</div>
<div class="section" id="chose-the-true-parameters-we-want-to-learn">
<h3>Chose the true parameters we want to learn.<a class="headerlink" href="#chose-the-true-parameters-we-want-to-learn" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">w</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([3., 2.])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="create-some-data-points-x-and-y-which-lie-on-the-line">
<h3>Create some data points x and y which lie on the line<a class="headerlink" href="#create-some-data-points-x-and-y-which-lie-on-the-line" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span>
    <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">1</span>
<span class="p">)</span>  <span class="c1"># Underscore functions in pytorch means replace the value (update)</span>
<span class="n">x</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 1.0000,  0.8386],
        [ 1.0000,  0.4724],
        [ 1.0000, -0.1087],
        [ 1.0000, -0.6441],
        [ 1.0000, -0.3719]])
</pre></div>
</div>
</div>
</div>
<p>Tensor is a data structure which is a fundamental building block of PyTorch. Tensors are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU
and more importantly for us - they can keep track of its gradients.</p>
<p>For further reading, see <a class="reference external" href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">here</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># @ is a matrix product (similar to matmul)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_15_0.png" src="../_images/gradient_decent_15_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_real</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>If we could find a way to fit our guess for the coefficients the weights (<span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span>), we could use the exact same method for very complicated tasks (as in image recognition).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Written in terms of <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(w_1\)</span>, our <strong>loss function</strong> is:</p>
</div>
<div class="section" id="l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2">
<h3><span class="math notranslate nohighlight">\(L = \frac{1}{n}\sum_{i=1}^n [y_i - (w_0 + w_1 x_i)]^2\)</span><a class="headerlink" href="#l-frac-1-n-sum-i-1-n-y-i-w-0-w-1-x-i-2" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_real</span>
<span class="n">mse</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># Initial mean-squared error</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(56.7406)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_hat</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_21_0.png" src="../_images/gradient_decent_21_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w_real</span><span class="p">)</span>
<span class="n">w</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([-3., -5.], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<div class="section" id="so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression">
<h4>So far, we have specified the <em>model</em> (linear regression) and the <em>evaluation criteria</em> (or <em>loss function</em>). Now we need to handle <em>optimization</em>; that is, how do we find the best values for weights (<span class="math notranslate nohighlight">\(w_0\)</span>, <span class="math notranslate nohighlight">\(w_1\)</span>)? How do we find the best <em>fitting</em> linear regression.<a class="headerlink" href="#so-far-we-have-specified-the-model-linear-regression-and-the-evaluation-criteria-or-loss-function-now-we-need-to-handle-optimization-that-is-how-do-we-find-the-best-values-for-weights-w-0-w-1-how-do-we-find-the-best-fitting-linear-regression" title="Permalink to this headline">¶</a></h4>
<p>To know how to change <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(w_1\)</span> to reduce the loss, we compute the derivatives (or gradients).</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial L}{\partial w_0} = \frac{1}{n}\sum_i -2[y_i - (w_0 + w_1 x_i)]\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial L}{\partial w_1} = \frac{1}{n}\sum_i -2[y_i - (w_0 + w_1 x_i)]x_i\)</span></p>
</div>
<div class="section" id="if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate">
<h4>If we know those we can iteratively take little steps down the gradient to reduce the loss, aka, <em>gradient descent</em>. How big our steps are is determined by the <em>learning rate</em>.<a class="headerlink" href="#if-we-know-those-we-can-iteratively-take-little-steps-down-the-gradient-to-reduce-the-loss-aka-gradient-descent-how-big-our-steps-are-is-determined-by-the-learning-rate" title="Permalink to this headline">¶</a></h4>
<p><span class="math notranslate nohighlight">\(w_0^{new} = w_0^{old}\)</span> - Learning-Rate <span class="math notranslate nohighlight">\(*  \frac{\partial L}{\partial w_0}\)</span></p>
<p><span class="math notranslate nohighlight">\(w_1^{new} = w_1^{old}\)</span> - Learning-Rate <span class="math notranslate nohighlight">\(*  \frac{\partial L}{\partial w_1}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update2</span><span class="p">(</span><span class="n">iteration</span><span class="p">):</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mse</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># calculate the gradient of a tensor! It is now stored at w.grad</span>

    <span class="c1"># To prevent tracking history and using memory</span>
    <span class="c1"># (code block where we don&#39;t need to track the gradients but only modify the values of tensors)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">w</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
        <span class="c1"># Under score means inplace.</span>
        <span class="c1"># lr is the learning rate. Good learning rate is a key part of Neural Networks.</span>

        <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="c1"># We want to zero the gradient before we are re-evaluate it.</span>

    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<p>In PyTorch, we need to set the gradients to zero before starting to do back propragation because PyTorch accumulates the gradients on subsequent backward passes. This is convenient while training RNNs. So, the default action is to accumulate (i.e. sum) the gradients on every loss.backward() call.
Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the minimum (or maximum, in case of maximization objectives).</p>
<p>Explanations about how PyTorch calculates the gradients can be found here (and in many other sources) - https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lin</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">w0</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="p">(</span><span class="n">line</span><span class="p">,)</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">lin</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">]),</span>
    <span class="n">c</span><span class="o">=</span><span class="s2">&quot;firebrick&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># line, = ax.plot(x[:,0], y, c=&#39;firebrick&#39;)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Loss = 0.00&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">update2</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Loss = </span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">l</span><span class="p">)</span>
    <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span>
        <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">lin</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">]),</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">line</span><span class="p">,)</span>


<span class="n">anim</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># You might have some difficulties running this cell without importing certain packages.</span>
<span class="c1"># might need to install: conda install -c conda-forge ffmpeg</span>
<span class="n">HTML</span><span class="p">(</span><span class="n">anim</span><span class="o">.</span><span class="n">to_html5_video</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><video width="720" height="432" controls autoplay loop>
  <source type="video/mp4" src="data:video/mp4;base64,AAAAIGZ0eXBNNFYgAAACAE00ViBpc29taXNvMmF2YzEAAAAIZnJlZQAAJW5tZGF0AAACrgYF//+q
3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE2MSByMzAzME0gOGJkNmQyOCAtIEguMjY0L01Q
RUctNCBBVkMgY29kZWMgLSBDb3B5bGVmdCAyMDAzLTIwMjAgLSBodHRwOi8vd3d3LnZpZGVvbGFu
Lm9yZy94MjY0Lmh0bWwgLSBvcHRpb25zOiBjYWJhYz0xIHJlZj0zIGRlYmxvY2s9MTowOjAgYW5h
bHlzZT0weDM6MHgxMTMgbWU9aGV4IHN1Ym1lPTcgcHN5PTEgcHN5X3JkPTEuMDA6MC4wMCBtaXhl
ZF9yZWY9MSBtZV9yYW5nZT0xNiBjaHJvbWFfbWU9MSB0cmVsbGlzPTEgOHg4ZGN0PTEgY3FtPTAg
ZGVhZHpvbmU9MjEsMTEgZmFzdF9wc2tpcD0xIGNocm9tYV9xcF9vZmZzZXQ9LTIgdGhyZWFkcz0z
IGxvb2thaGVhZF90aHJlYWRzPTEgc2xpY2VkX3RocmVhZHM9MCBucj0wIGRlY2ltYXRlPTEgaW50
ZXJsYWNlZD0wIGJsdXJheV9jb21wYXQ9MCBjb25zdHJhaW5lZF9pbnRyYT0wIGJmcmFtZXM9MyBi
X3B5cmFtaWQ9MiBiX2FkYXB0PTEgYl9iaWFzPTAgZGlyZWN0PTEgd2VpZ2h0Yj0xIG9wZW5fZ29w
PTAgd2VpZ2h0cD0yIGtleWludD0yNTAga2V5aW50X21pbj02IHNjZW5lY3V0PTQwIGludHJhX3Jl
ZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAu
NjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAAL
aGWIhAAT//73sY+BTcgADZc6inof4RWx9JBRerHZoGTqAAADAAADAAADABCOhyi2JfVxcZaAAAAK
aMYJOI+wxmPgAC7uAxKfClhXdEiiDs5SKd9S/4jjH0k4KTcA7K/Dv/ZKYTDB9HsK035GbnDB1dAJ
zCHLEByzWJ6o3Zj8q282zzjIp6irzKq1/t6RzYB/eFkvtToaWT+GaISV3vMU6Orpp3ap8CUKuNP2
fQhNuIgvY4ywAJWpoD519FMOTfg8dsmmwBWtwMVdlW+v02OzSQMDOhhBBlmK/3oxkI9B4d8278L5
k6UbxeQS0VfVo4ubehFvJE6p1VGQx5OFoo+lxARKEUJVTLQMb7QZEtt+dzXVwQW9Gfq+sbwHgjCB
R9wGjfM5FBhbmu997OVcTYEh+3N59SIta8jLHyBfyVGAtcbij9p2p+2J/eKc+h9nd1YN8Fbsizlt
WOw50w3QcF7mXLvpr1b0NnwUAXCDOzxGO8bSxR+HRS0vvGXEQ3/pc6v0kfOX1SJ6UM3P5vXbXZUU
GzNiwQ30Fib62C7tJ0dCVWfu2dIOX12ux1dBbaYcQCk/TCRr1sdMsskDQ/Gd1Z8VnY/ZTO/LdgEA
GhPJavdSRONWv7g+xcYydCbN+CeupICB6/yGx1fD9D6Im2rQMha7E3HLnhPg98pygCRgqXVw23PQ
l1zxf4Wi1EPWh+gdhGRpCED06AcUMqdvQ72mBFiP+KBvHETQhJ76q7mKyCCMQLZot5DVxDOr+PmQ
mftRxLNSCgYxC5mQojJmJvG13pJzH/UJTrERIaDW7o3SpEdp/nZxGlWpN60NqCgL3WCm7RvocP2y
OO6Z3EsQ5bfc9KROCmGmESut6FJeUT0DLwpYZIC0sgqTWGJNymjEdM2b231acW9MMv/fZYb7m4fx
CRJ3Ho/FG8FfnRIWitFTsQ6oU5JvzwUtDR8df3tW90VK4YBSqDrDZaq2jR7iWPKrCdWzdF11fkR+
XkGGiIX/1Ta9q7kqoRdbh10fcT3ERlUlkUUYl4ZdTCgD0D1zq3WjT6RZwrwuBLUblJR1W57MXUpc
OPbwZlcs3vE7JKRTCudHDGXyvs0bEgzKgANWDtM4Paw6TM0lYYXpM7XpvdGfTo6fiMr6vrDqnMvl
EYPI/+bXZndTguP3uLQTXxnyfTaL/QpXkj6sg9YrkLXqbB9PQiLDPz/ZN5u54HqBzkZ2hkpq+074
IaqgOMPEr6guoZuuR3KmhnfPDKWgTlH5iWFUWHbGY9CGhmHbJO6Bu652XlA8ivf5mig8qdfG07e/
7gEpngg4tBAlHed3eFJVKcSXGOvULClA3rV4I28jqNMDj/+ejpeJw0h4bMiPaaz76XlUP3aBbzbN
fOk3AV2sn6B7VQg1kFO/HefEf/IiWQaTZ9ylNvBLj2px74w/yNsG2WaI3zWgG9rjTkjeqC3TQTqg
Pmhtu1QaVzC/HvOEfq8o2MLR6i42aDpB+M7uKak8N+A8ufWxjQBnq3IJWG9WLLr25hjZzsXVwMQL
WS0r3sZdaGgxzv0FyDpLM0hbDukRzWYZpEqqgCNJLdxxAYM1eC0rKOnHr7w7V5GHgkMUxJ7CcWx+
lROZkKuI8u+DYXb9Bhvm32Uc2m5IMSC/7215L3BSf3m6qRp3c/QlTgk7KC6RWBhLamhcyAv93XH3
yxVk+zgnqYb17mXMmLBK+sxHuvrkH7l+EijPEQlVWDqAlJOt9oF8lOHPfxUqifmlHlgoGwSLh/5y
cS/izKsFBtIjF7b2hhR82e4bGYcuin08R+GZkFUpXg5wZIyDcSslMsr/8WiXZNlB/cOmCSnIIRwr
wAJqS0F2HrEGwmDeigAHyABP9KH3vivk/xlY3gZa1njWgKkRdpiQfVvkNtRC5rOhTQ9mVtdjEokv
1TCHXQqDzBmsEffgH+bj5wQvYVFAAAgAzsBtHI6GO6UV92RvsV1iYDDGBc3T91O/CnE1uGe1weFD
z9yUhytgUo6cK16F2SSKRb4+okiY4WYXFgaJRx7+f5cCfavuXPt6nrrFsUPNaUMnjE4sAAT9XAAN
hwgK54feiQ28zHupbuIrsojRjaZJ6JUnwwwz2ZoP30M/VFdTAZMN66gabCl5hozEyRCvnMf//vQh
jzezs+i0x1YAqQdzFBzLpCXLFkZkpDAIojO/lh9Ojv3RgXEEixj0PHl2ixiRBDJ1aOVghhYUiKDj
EovSCx98C5cQNXVcAAIzBioOuTAB17ab3fbWec2G86GeUfHOtsHNY66TUX7Y8DOukpaJYAgQPSKm
4is+Vzuv53XL0opadS7dYGxoGx6fXdVNYaJrIX8pIKptnaXEBjAT96LqwSMQAc9HX8PUPPIf5hiY
UYbfSPeEr6Ggg6mlyPJOKNFFREQ3IBAPFCACg52Cj/48qNxaCO7Bs+1XEW0KgYv/2C7Lm/gJvDJf
gbwUmGbyeVa2HCyKh6CsNUrGX7kmGRd1vWHuej1hl/T1qI4XTr8JCWFU9j0LORWBIm2R89+9S4t7
m2j21JSBehu+wUQ9PUWyyA42rGPJ3y6sNCUZJbanT/uSkw6hSAc0hwsiqX20mVMPh8ykLxoJqTUJ
aldguDsbGSnh3qvwkJYVT2PQohHREiX/07kmW7Tq8kJYVT2PQsgQAAT4YmPcp8hHnqMfNaIh2Xkw
5vOBdaln+n7tfwZZ/Oq33s8MTXa1Vlasjr2OscJTmXBe3SqDIV9Jlcq6BfKVGKAOQ0PCgwOWggAs
ToXJ9bOWAe4BRH8HCsgsDAxs6UxalSIeS79SihkXrtaG84QQLfe/9g/AnObPtGYSbiiI7AmF3m6A
Lt1sG4S+DSJI7FwcBhPM1GtJ0WFW1TozD5dVIXvZYGNs6N3qvcvLSIoX//6It2mHoYfMyRliOifT
RIYz9KnNHCOkE1zfnUiuy0bcsoL7aREbnJCdjKlrdLZfwf9kynNNLwyvFk8GCyT49B2JMJk9QQcm
nnjkeNIgWttYqfLnn9HXqgJ+BW+tqFrMKQMn6DCz7DBm/F1OMkhmD+mME6L2BhGP7xHSeeUhOmv+
vddAMfaAzGUNeCKK7LAu5tEUOPxFIXO1ik4bPD1X5B9tS9HE4bqowgD2BbYOxhhkeQvtTh8HZ8n2
7YUXZgoTZI1ugn3oJHEBKj9LJL1XP9chXhAkVbDyI/Y1PlW9FqFoNkfer/zX2YP+jGFXROjJvSnD
MvSJ9ZHzK3pq9TYYbnXygYnG90E1J0Fn2/7dwURsOdG2vMbtnDu8yxanVYIhqkRxg63FXNt64vzi
VUGYdF2miFiFqJDZQkTnrxwhm1hxjZv9T0Sia5wwAdHvcMgiD1t6JJjnj+54apgb1JLFVYjdN7Sh
E2FR6x+9C5tic8P+YupYrLAQtUcQ39g+kX9zAamIOEE3kmtMsYZJKXE1rNEcbN01nLpQGEezZ33X
xAseu9v3ICv6uLZBbrRGWffy4INO1IdLkk9lPakJ6sWWe6N67N9sdsXndf73/Z67F5CYmWBEQRRL
LZPyNhbFMjlClGwMiffY2n6pGMbZ2FDTJrdWnZ/KYMU8rMXcp/CvIXhAmTHQu/vcHrA41hISuerS
7CWLmRgnv9Axb+6Moli7HEhlNGwGj+uT4xTlkoDbN8Pjcm5y3gB499YeOCpuTD30vW8mTUObLNxv
JEuV6xzDAjzYNCljMjndZ8OR2dK76moAAB1G95j4amSmHED/gbT+WBVm4c0KKdD1XhG9NBD3z5Cu
1bhaSVQecuAqWyCEYnrQZm7kGgjmiUM76tgDReh7gorhqfIEe3d1xTtRUD9Yy+R+PFMbevwZcFKN
LQjzvf+oJNg5CMkKEBciBd7t9qHeWfuAVGXsItU8CABLsigV5kc+fauqyh40/ZAFHEKhWrYm1H/g
1Zbzn4AQA+3mXrAAH9EAAAGuQZokbEE//rUqgAdJcjUmZtKsAOM47wVxwdUnjm8btrQXXZZLYjzO
vodbvEkvjsSVXJqXCWi7xmzrbqX3JFdFFJtWvjzs3Ed0jwzlhJg2qAkyp9YFh0N2Ew0jWdfjSFYO
UvlMq6jyqkDhK5YxZylIddxT8ljGGPTm/D7PkEACAsADUgmknNydDMo+g/b42Y+0L3cs64T2nYdN
TJJq4ARualbW+crm8G/Mvz12FNLu5XyRGip8RcYzoJxyp1ywJfY4L4afmGwwxi+poI1xQjcY+sRg
ZxbQDfWsU0DZHjoSoX0J9XVahBXlqC1f9+u3XhgbETymAu5C8NTjex0jzIgq52K+Gkw977pNzxhN
6n/7ubLzoi+h61U023UAFXX2W+ifpNCuZ9t5hEfcuWJS8GSBSIDlW/ZnyMt7eegOjPYCudTaaPfN
IKlJTLvVZ6kd4WafeISX3qltp4FP/7sj6UYEyF2pUMPdEWDWK/ONKH1Xp9QEvVYrSK+xPOJEs6sB
rsfL5ehStzQ8imp+LgQCV34F+5eK+mQJ171VESwPoW2p+Lw0QZPmf6WNAWC9d/CzgAAAAMJBnkJ4
h/8AFHJz7EB0tABCnyJXcakjMsgq8WXmylOTbxXJ0+xXMutget2k3KBFN0ZtHqJo6ZlhPLCEMXAb
G93KQUTvFsxxNUV8MP4+zDtEQDnIrpA128E/rb+lvkQLe5GfTa8wT+qnzdRoUOIV1zt4W/Okfs03
KrI7Qfmk7paPEDbiZaHWOIyQmpeujUqlVsy0fszNUhrDxwjjoh0avQYOIbfvTc1ti5OEChK5FvKq
L6EC5bjGZfPdMVQ7cph0ldyiNwAAAK8BnmF0Q78AHHrtcIqSYrFDABtVz/Y10znotbcxBhQjsytz
O166lCwu1iQi7/ZhNdxoemMVfK24VtrhF1qRCtSIxUxs8OSyIyPUCChfuRfbefKemuYwaFT47eQg
MBAOuK2n5UbpSPW4k7xukFyz18CcR/zAWHcGQjPTPvuVpGeVg41ddnjeED73umE2k90qOKKgBQbI
SlZvygfmppwghChAFLa6pvIlOOfpZEHW4LiAAAAAtwGeY2pDvwAaPTCq9eLSADZyCzh98j5Ose83
dW4U3KduS45xNDO1oc9rrK93/mm0QS43s940UNaFJS4LbkEeFIBax66T9A7dCw/BF7P2KA+9IRDn
5JiIu0TdjH70lIIhtO2LOAS0kh6Ia9pPusZd7F6WomDpPrueZcsihDza3GXBOh8j139zze5ylPi1
RotNx6ogUOHbwFT4MD9ELf9DKkcE3nfxRNja9VblYBMbl9MgANLshdQoxwAAAN1BmmhJqEFomUwI
J//+tSqABtAsWDAsAJYPGc5JWhT9PltlY62Bc9VEihtAS0kbzwxPZRUj3/VZieBSj1+9yq9em76q
H540MiGP3PpOS6EYd0ZSXsimeqVOHgSNrO8cMrDB40fj498ZID5+uwOCm/81qYFhHeJjR00K4sxU
ymJSepwqtzEq8wK6zJBdbm0LVYFra1mX9/6naLw+AwfOg7drM6RX93YjBW4FMcJWTHMCLKCBblAd
sMI/RUp9ORP5Bo+CxkTMz5+pEgNSNKgeQttGLsAAACoN8bmj55J/1QAAAIlBnoZFESw//wASzh2Z
gAvqSxTZWi9Pqr8/GG6qd+fzLi38GgutWBEL9/Elf5EGBt2MgzU7KERUqwdw67d4q+kpvgO6pOsC
ko/PzInvj4xUUH7t0gnB7/e9jVA+ImTfEzdBa39SU3SY+y48HV+yOEYbAcOUuoxnmve2jr1bhFzj
BdgHl/vD0AAFtQAAAJgBnqV0Q78AGj7eRIEdJpwANqi3j9/oIV/ff6gVKT1HGMVbsZ0PSJ0xq76r
n5NP0KvQhc3hFWmiL5GRtuQ/bnQWmXCByZU1Ux+uGrG9avxYvvb86AecZ74kM/UJ8333wMaX73mj
xst+HjYlIWGEnHEJJJgkboYDv/eZ2fggH4nUwfXpTuHM7d8DHXphj5ouyI8YAAADAACRgQAAAIcB
nqdqQ78AGo2PDYyHJtBSWogAc2aFtI2YyRfZYeah/Vy8zdgHULwSJP99T/zTwdu+ARimx5fdq0qn
v1AIGzNUe4+sjpVWB6q/5S7FJMtpn0JNYBnHF/+KtLZc7d/sPxAMJTMS+n4pR9Al9DYnmac8pYkG
Qj1qErl0vFo45gPuVgAAAwAANaAAAACQQZqsSahBbJlMCCf//rUqgAbLdLkAIy3nYnLGzQP9vmsn
+jUwtO0GZvlqu5xyPw2GRrcIbdWxSQdNORbuG6ZumOBqi7wzm8/9sw4sAoE18uDhpU6Xl+SuELgZ
44WaHyHUtSEfVACN4XR7qP2MAZS4z1cfRHLx+jusqxsYoak4wdv/Ys8iQGajxAeQgAAAAwPGAAAA
dUGeykUVLD//ABLWgv7CbEJMVPkZYcwAC+vkSu+9HRJpyazm45ck2QsSCPPSamla7EXwoeGgVwDf
kFqRxjwYDhL3XoCI+FmPxDQwomzAOky5+bYkn3Wc++YvmMerLpqlK7yMOTANJgns6ZAmkCHoYAAA
AwAFbQAAAGUBnul0Q78AGpFDxQnCtXDBoAL5rvVOBSNHrKVlFmRa8rcEpoBDadv0v7AQ54FfDpk0
8YNzQ9QHNrXUXdykVZC7qUW0ADM4BCmmpvZYK/gr5qpgP2Mh/hxmJobLlrOqaAAAAwAB9wAAAHwB
nutqQ78AGZut+C7+8gAAvqGGE3JlmoSlGLSGKwwzNXGPcPw4A3gDfs0U+TFy7D2HyMYEBn+gR5CF
N/BxtuAAHvSCoF01huuIj4FljWFx2S8HCVA6Li8l2vPrsJDYsWUJNaVzP/jR60tIkIuBHvLSFqsT
yXZAAAADADjgAAAAvkGa8EmoQWyZTAgn//61KoAGywpcgBGLjvBXT4a+4ZltLVG40XWu1zLXeCVj
0Ad0jOOKcDjcg0DHQ/QL3JusUqD8b6tvSPP6zULQw2r2YtDkM773yY3HEl/7vuIiE164iGG8rwbB
NjnDhdpU3qoeyWKSgDEqFk5NWfhWhLDh8RNBXLDH9i6645tLgyobnLOWipe74/a+2rZumbuPQWFZ
5MKL7K1DvAclr9/LKIcAAAMAMn9I5Kr6svnLAwv/4YEAAACPQZ8ORRUsP/8AEtgzg/1ZW1aVqgA2
qosT7bD3MOp4h6A/5tmkHyQUTyJPCD3iuDRXzjP7GTk1SA7hEFv6yghoBLptVHgADK8BaLQm6rFE
mulCIWE+9xSP81STKggnH8OsAexCvbPqyoZtCD/3q2YZaf/IQkYd337ER3YN4FJmDi7AAAADArIx
fGX/TpRPg/kAAACLAZ8tdEO/ABmcc77URSPY7yGAB2WJM6j4HwPMykj/LacPIs4FT2+wsw8y9q8b
fiRofSiRGHMJKWkr4xPnL5HlmU8cQZaCFoocoI3vV5tvS40/D3RHZsh2rfu4d5CmYVQ7MtUH6duZ
C5HGlYptZSN4e4Y4M2Z862sW7/q1h2AAmmcAAAMAQWCTzFIZgQAAAHMBny9qQ78AGo2PT1qOx8/1
UADsz+qXtijFOUxvJ1B99h9dmJDSkcvgrSozNkt1D/aPwE+TPCV6zHxji/YI+pjx7ISOAdijbqcp
kx3UQqPk1WJ/7hc4ss/miw3JRWl/q1WtptqnWzUhwAAAAwDdG7ry24HgAAAAckGbNEmoQWyZTAgn
//61KoAGo4QYADL1LM5y+C+ElepIZVzBajcvldSlCUncfa5MU6lNG58PM4ECE3My1J+23UwWqO0g
2C8c61G9Z6YzNL3ngnR1jVlQNGRLZVoqdNjZZRQcyrN3AAADAAn4ryziPUqGSwAAAGxBn1JFFSw/
/wASJPg8AEKejyCU3/mw1AbBiTEUS/cbLxGG+oIsbiY13c5Y/0uvyrZvpJgsjzFVBufonM2LBIIR
yf6CXuURtwI8BNDYt8PwvOj+y7c87BMzEagXuL0MeXanioAAABJVUFILqRsAAAA/AZ9xdEO/ABns
jWgAdeF6pw00FGFExNwFd13kUx7gv6VwgLGaRkXhaH3qLi1OLhCRHceXZAAAAwCSvgWeMIAeAAAA
dQGfc2pDvwAZm6qC3R4ypmvvABzCUAXZv1At/YNzjrCjA9d+2eDvHq18JuoFiXttKZx9q6VtwDW0
YK3V03o2gKs71QvtU2oYnXpftq+4Wp10TGR/u3INq0Ce0/crNaXM2tIORSnY4tpPFGgaqdC7IAAA
AwAccAAAAI5Bm3hJqEFsmUwIJ//+tSqABqJujAXABtW7sn3dhsdaa6JMVZOn//uAI7a6GV6JTAS8
R3f9Z4M5mEJuaZ4+/DapV8Z9uz0Gea14AF+L04j45Zet0APiq3Z8hu+kgassKCnW8eMbb/RJ3nkp
5C8HlvmsiJPgJKLZOR+KpS3FR4GprahhQuPfJyYAAAMAAKaBAAAAikGflkUVLD//ABJPrlBMAEQf
FcPA3BpzcEjXMbIyjzq4EfQ6xREP4mU/YfznAkDpEnJ3A9kkL0Fn+pRCg65q5Jx/wE0oX4OEYsYb
Zz0hMY6KMd/MrSFa4SXGDYadJBTSRsfkobrT1Zb4lukXDesAT5/MZTaYN9L4Dr/aBdB1cMr3188A
gAAAAwBNwAAAADYBn7V0Q78AGfGhcAC+vfxAgVSnkoNw7eXfipmGgKwwIFOZesKWQ4wYeLGo7uzP
84AAAAMAg4EAAABUAZ+3akO/ABnwcLgAX18M8plWbxcFJX9ITa6ws0S2hM89sJMb7rxAHSN7B/xt
9vms1EWk/VlJgTglucfxN0NVBwsKYL3iookJb1bxildkAAADAAOPAAAAckGbvEmoQWyZTAgn//61
KoAGo3iWAA5ozgd2ZvC/vOObDt4VKCtv9id0H3uFxD0VVRfwZ85UVaQk3m0tsUoNKkr0l4N4T/dB
zADhCEqW+O+q54fqW+dp43LkhZLtipaxTwnSyipcaWFhDNShgAAAAwApoAAAADVBn9pFFSw//wAS
WE1mAC+viR/c9vDCCftix8f8RMlDzLAUGgQQ3QHS+L6UK/8MuAAAAwAIOQAAADoBn/l0Q78AGd48
o0AF4Tw1inhNZ1qsGaQeJU71n2rABsn6SqDVJr/uFkqrgMEkx4UsMuyAAAADAHHAAAAATgGf+2pD
vwAZ7hhwAO0iWNli0KEgnDrdUIphqqqR5aF4CCGu7LIL56+eHsw0xw/ugguFxWP/0mUs69TYVDiW
WhgAVkM85M7sgAAAAwBxwQAAAJJBm+BJqEFsmUwIJf/+tSqABqKlQOAC6XHJBIb4qcUsaHzFpXBT
UpSyDeQLi+0YfQngGBFbBlHrHVwVvghV1SWmqFHLwcQjhVZR3ZjWUXnsbNV2Ui3WA09wOttWpqfq
BawDRsllWFK3fGQ9m8D3ndN85sO+OH4zz+J+50Xbcsl2m0l52yonk89kZpo5MAAAAwAFNQAAAFhB
nh5FFSw//wASWE1mAD+kQg6cWuTJocJt+jJBFsUoa0406vYLeEWB/GSWYMm7AQ/ALXD9VRm3k7vZ
PWUWt4bSWAn0XKcerC2upX2EDfCkbwCAAAADAE3AAAAAXAGePXRDvwAaA0kAF9e6dcoNY6vFDHXM
IOsmNU/UrS6wRvJsJF5dhXy73faRSYrHuShxWjEDsG2kWUsa04R5DlK11tEtVQDvJF1ZZsYFpaNV
jkk8HNTeAAADAAGBAAAANwGeP2pDvwAZ8FTUADrwvVMhNk6rGUvSbI5uSevBeeiivaF5QG+dDXl4
zBntPQfRpyAAAAMAE3EAAABfQZokSahBbJlMCCX//rUqgAajbQcAH9bnJB5rAPWhB0jfT2oSuUc7
udlwMolqBkIgIMcEaMhIwfHGRHwGksACLlAVm9k7ppbAEzR/LK5HbWbpEkwRC9F1+WAAAAMACHgA
AABQQZ5CRRUsP/8AElbT2AC+vR5BKTDP+vZZcNWkkKABr7+tUIPOVWPYqNh8IyJb+qWkTkJlyF03
mbZnWrFenVXlnF4x+mFLqJbE/ioAAAMAAccAAAA8AZ5hdEO/ABnu+0QAL2KEgl+wYZv25M9te9vz
q2BrAiLf5+neI42gw/Ud9Vh6Ad/WfDq6ef5wAAADABBwAAAAEwGeY2pDvwAZ7aW3TkAAAAMAJuEA
AABoQZpmSahBbJlMFEwT//61KoAGonFTuLABtW7sn3dhsdT9MoIsKvf2QRM7nK1xMcSvczKTkOO3
iGFTAdQrwxBU9W4lBOeikCmcmPvXIhobSvhveHnquLOgHHxDtMFAWPwaOTAAAAMABTUAAABUAZ6F
akO/ABoDSQAX13SItDEL6fhVGG9At++ecl8IdlXEdnWRyWsw0hH4DMAbBy39WHaEJ2mI2AdYGLaM
T4IkQh4dtDCcNuXC/qdhYv84AAADAAg5AAAAVUGaiknhClJlMCCf//61KoAGo2yUAA5eHeCtxQhy
86PtBo1KI41tZ2Rx9vM4WklULGAo0yoQKPb+O67VKiAb15+J2rqvMXZzW0i9N0athsAAAAMAEPEA
AABHQZ6oRTRMP/8AElaiOACEFCV3zDy7zklanCZ4ahrVPClzSQmZhk/Cb59o2Wneq3RU/srqxWbC
GUQ2SFegCBaHioAAAAMAccAAAAATAZ7HdEO/ABnseJBTkAAAAwAJuAAAABMBnslqQ78AGe2lt05A
AAADACbhAAAAm0GazkmoQWiZTAgn//61KoAGo4QYAFvbzlGu/OrcFjAV5eTkK0pk1mVN6UWPzzvc
oaaqLVEz93lvPgAA9DtMq/+79ZcCuTw9tYbGiz4SQcOb4i7NpcDbdaPCEl/QvI2TjIjarAyoXsb0
yKsJ3EJcQovIHFzJ0gLbWxuotiAkG1gjgvsj4u3MUdRH0/KX1HnQlxHL5OTAAAADABTQAAAAO0Ge
7EURLD//ABJZY6sAH9IZH8bsFZ2J0E3ru5hD2pBk0hjpsLY1rv5671x5w2sF0kyKHcMuAAADAAIO
AAAAEwGfC3RDvwAZ7HiQU5AAAAMACbkAAAATAZ8NakO/ABnwR/2pyAAAAwAE3QAAAFhBmxJJqEFs
mUwIJ//+tSqABqN0uQAXVvOxOXwPxuA/ZYfan8fgm+frJaF1c6g+9dsE16lbT+D6+qT7Vf62c1v1
heRuVXS9ktFfJ7rqA9GfBw2AAAADACHhAAAAFUGfMEUVLD//ABJYNbm/zgAAAwABgQAAABABn090
Q78AAAMAAAMAAA8IAAAAEwGfUWpDvwAZ8Ef9qcgAAAMABN0AAAAcQZtWSahBbJlMCCX//rUqgAIA
hkSnAAADAAAz4AAAABJBn3RFFSw//wAAAwAAAwAACtgAAAAQAZ+TdEO/AAADAAADAAAPCQAAABAB
n5VqQ78AAAMAAAMAAA8IAAAAaUGbmkmoQWyZTAgl//61KoAGo20HAC2tYaM7HIvKnsoJg5mXan/8
AsoYJhdjgUU9Is+xh0yh/GorBIUT+IpvGV7Hw1PWAwRAoidgJ4U3H2BRKKR8u1UveNBSX4b4a1BX
YzedEAAAAwAHLQAAABVBn7hFFSw//wASWDW5v84AAAMAAYEAAAAQAZ/XdEO/AAADAAADAAAPCAAA
ABMBn9lqQ78AGfBH/anIAAADAATdAAAAGUGb3kmoQWyZTAgl//61KoAAAAMAAAMAA9YAAAASQZ/8
RRUsP/8AAAMAAAMAAArZAAAAEAGeG3RDvwAAAwAAAwAADwkAAAAQAZ4dakO/AAADAAADAAAPCAAA
ABlBmgJJqEFsmUwII//+tSqAAAADAAADAAPWAAAAEkGeIEUVLD//AAADAAADAAAK2QAAABABnl90
Q78AAAMAAAMAAA8IAAAAEAGeQWpDvwAAAwAAAwAADwkAAAAZQZpFSahBbJlMCHf//qmWAAADAAAD
AAAeEAAAABJBnmNFFSw//wAAAwAAAwAACtkAAAAQAZ6EakO/AAADAAADAAAPCQAABnJtb292AAAA
bG12aGQAAAAAAAAAAAAAAAAAAAPoAAApBAABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAA
AAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAFnHRyYWsA
AABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAApBAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAA
AAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAC0AAAAbAAAAAAACRlZHRzAAAAHGVsc3QAAAAAAAAA
AQAAKQQAAAwAAAEAAAAABRRtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAACgAAAGkAFXEAAAAAAAt
aGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAAS/bWluZgAAABR2bWhk
AAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAAEf3N0YmwA
AAC3c3RzZAAAAAAAAAABAAAAp2F2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAC0AGwAEgAAABI
AAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8AAAA1YXZjQwFkABb/
4QAYZ2QAFqzZQLQ3oQAAAwADAAADACgPFi2WAQAGaOvjyyLA/fj4AAAAABx1dWlka2hA8l8kT8W6
OaUbzwMj8wAAAAAAAAAYc3R0cwAAAAAAAAABAAAARgAABgAAAAAUc3RzcwAAAAAAAAABAAAAAQAA
AjhjdHRzAAAAAAAAAEUAAAABAAAMAAAAAAEAAB4AAAAAAQAADAAAAAABAAAAAAAAAAEAAAYAAAAA
AQAAHgAAAAABAAAMAAAAAAEAAAAAAAAAAQAABgAAAAABAAAeAAAAAAEAAAwAAAAAAQAAAAAAAAAB
AAAGAAAAAAEAAB4AAAAAAQAADAAAAAABAAAAAAAAAAEAAAYAAAAAAQAAHgAAAAABAAAMAAAAAAEA
AAAAAAAAAQAABgAAAAABAAAeAAAAAAEAAAwAAAAAAQAAAAAAAAABAAAGAAAAAAEAAB4AAAAAAQAA
DAAAAAABAAAAAAAAAAEAAAYAAAAAAQAAHgAAAAABAAAMAAAAAAEAAAAAAAAAAQAABgAAAAABAAAe
AAAAAAEAAAwAAAAAAQAAAAAAAAABAAAGAAAAAAEAABIAAAAAAQAABgAAAAABAAAeAAAAAAEAAAwA
AAAAAQAAAAAAAAABAAAGAAAAAAEAAB4AAAAAAQAADAAAAAABAAAAAAAAAAEAAAYAAAAAAQAAHgAA
AAABAAAMAAAAAAEAAAAAAAAAAQAABgAAAAABAAAeAAAAAAEAAAwAAAAAAQAAAAAAAAABAAAGAAAA
AAEAAB4AAAAAAQAADAAAAAABAAAAAAAAAAEAAAYAAAAAAQAAHgAAAAABAAAMAAAAAAEAAAAAAAAA
AQAABgAAAAABAAAeAAAAAAEAAAwAAAAAAQAAAAAAAAABAAAGAAAAAAEAABgAAAAAAgAABgAAAAAc
c3RzYwAAAAAAAAABAAAAAQAAAEYAAAABAAABLHN0c3oAAAAAAAAAAAAAAEYAAA4eAAABsgAAAMYA
AACzAAAAuwAAAOEAAACNAAAAnAAAAIsAAACUAAAAeQAAAGkAAACAAAAAwgAAAJMAAACPAAAAdwAA
AHYAAABwAAAAQwAAAHkAAACSAAAAjgAAADoAAABYAAAAdgAAADkAAAA+AAAAUgAAAJYAAABcAAAA
YAAAADsAAABjAAAAVAAAAEAAAAAXAAAAbAAAAFgAAABZAAAASwAAABcAAAAXAAAAnwAAAD8AAAAX
AAAAFwAAAFwAAAAZAAAAFAAAABcAAAAgAAAAFgAAABQAAAAUAAAAbQAAABkAAAAUAAAAFwAAAB0A
AAAWAAAAFAAAABQAAAAdAAAAFgAAABQAAAAUAAAAHQAAABYAAAAUAAAAFHN0Y28AAAAAAAAAAQAA
ADAAAABidWR0YQAAAFptZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAA
AC1pbHN0AAAAJal0b28AAAAdZGF0YQAAAAEAAAAATGF2ZjU5LjE2LjEwMA==
">
  Your browser does not support the video tag.
</video></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;figs/Gradient_descent2.png&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">700</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_decent_29_0.png" src="../_images/gradient_decent_29_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">update2</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">losses</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iternation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Loss&#39;)
</pre></div>
</div>
<img alt="../_images/gradient_decent_30_1.png" src="../_images/gradient_decent_30_1.png" />
</div>
</div>
</div>
<div class="section" id="in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent">
<h4>In Deep learning, we use a variation of gradient descent called <a class="reference external" href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/">mini-batch gradient descent</a>:<a class="headerlink" href="#in-deep-learning-we-use-a-variation-of-gradient-descent-called-mini-batch-gradient-descent" title="Permalink to this headline">¶</a></h4>
<p>Instead of calculating the gradient over the whole training data before changing model weights (coefficients), we take a subset (batch) of our data, and change the values of the weights after we calculated the gradient over this subset.</p>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Neural_network_for_Lorenz96.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Using neural networks for L96 parameterization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Learning-DA-increments.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Learning Data Assimilation Increments</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By The M2LinES Community<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>