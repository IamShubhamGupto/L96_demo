{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Universal approximation theorem: Neural Networks can approximate any continuous function.**\n",
    "\n",
    "## Outline:\n",
    "\n",
    "- A visual [demonstration](http://neuralnetworksanddeeplearning.com/chap4.html) that neural networks (NNs) can compute any function.\n",
    "- Like any Machine Learning (ML) algorithm, training a neural network requires minimizing some loss function (defined as the distance between the predicted value and target value).\n",
    "- The minimization is done using an algorithm called gradient descent, or a variation called stochastic/minibach gradient descent. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using gradient descent on linear regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest machine learning algorithm is [linear regression](https://en.wikipedia.org/wiki/Linear_regression). We will code up linear regression from scratch with a twist: we will use gradient descent, which is also how neural networks learn. Most of this lesson is pretty much stolen from Jeremy Howard's fast.ai [lesson zero](https://www.youtube.com/watch?v=ACU-T9L4_lI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In linear regression, we assume that $y = w_0 + w_1 x $, where $y$ is the expected output.\n",
    "- We look for the $w$ coefficients that give the 'best' prediction ($\\tilde{y}$) for the output. The best prediction is defined by minimizing some cost function. For linear regression, we generally minimize the mean sequare error between the expected output $y$ and the prediction ($\\tilde{y}$)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression from scratch\n",
    "\n",
    "We will learn the parameters $w_0$ and $\\mathbf{w}_1$ of a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting packages\n",
    "from IPython.display import Image, HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import machine-learning packages\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1032/1*WswH2fPx0bf_JFRMm8V-HA.gif' width=500/>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image is taken from [here](https://blog.insightdatascience.com/a-quick-introduction-to-vanilla-neural-networks-b0998c6216a1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is [PyTorch](https://pytorch.org/)?\n",
    "\n",
    "PyTorch is a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- A replacement for NumPy to use the power of GPUs\n",
    "- A deep learning research platform that provides flexibility and speed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a [Tensor](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)?\n",
    "Tensor is a data structure which is a fundamental building block of PyTorch. Tensors are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU\n",
    "and more importantly for us - they can keep track of its gradients.\n",
    "\n",
    "Further reading [here](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chose the true parameters we want to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.as_tensor([3.0, 2])\n",
    "w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Create some data points x and y which lie on the line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Documentation for:\n",
    "- [ones()](https://pytorch.org/docs/stable/generated/torch.ones.html)\n",
    "- [uniform_()](https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = torch.ones(n, 2)\n",
    "# Underscore functions in pytorch means replace the value (update)\n",
    "x[:, 1].uniform_(-1.0, 1)\n",
    "\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x @ w + torch.rand(n)  # @ is a matrix product (similar to matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.scatter(x[:, 1], y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_real = torch.as_tensor([-3.0, -5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "If we could find a way to fit our guess for the coefficients the weights ($w_0$ and $\\mathbf{w}_1$), we could use the exact same method for very complicated tasks (as in image recognition). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our loss function $L$ as Mean Square Error loss as:\n",
    "\\begin{equation*}\n",
    "L_{MSE} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} (y_i - \\tilde y_i)^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written in terms of $w_0$ and $w_1$, our **loss function** $L$ is:\n",
    "\n",
    "\\begin{equation*}\n",
    "L_{MSE} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} (y_i - (w_0 + \\mathbf{w_1} \\cdot x_i))^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = x @ w_real\n",
    "# Initial mean-squared error\n",
    "mse(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.scatter(x[:, 1], y, label=\"y\")\n",
    "plt.scatter(x[:, 1], y_hat, label=\"$\\\\tilde{y}$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = nn.Parameter(w_real)\n",
    "w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have specified the *model* (linear regression) and the *evaluation criteria* (or *loss function*). Now we need to handle *optimization*; that is, how do we find the best values for weights ($w_0$, $w_1$) such that they best fit the linear regression line?\n",
    "\n",
    "To know how to change $w_0$ and $w_1$ to reduce the loss, we compute the derivatives (or gradients:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L_{MSE}}{\\partial w_0} = \\frac{1}{n}\\sum_{i=1}^{n} -2\\cdot [y_i - (w_0 + \\mathbf{w_1}\\cdot x_i)]\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L_{MSE}}{\\partial \\mathbf{w_1}} = \\frac{1}{n}\\sum_{i=1}^{n} -2\\cdot [y_i - (w_0 + \\mathbf{w_1}\\cdot x_i)] \\cdot x_i\n",
    "\\end{equation*}\n",
    "\n",
    "Since we know that we can iteratively take little steps down along the gradient to reduce the loss, aka, *gradient descent*, the size of the step is determined by the learning rate ($\\eta$):\n",
    "\n",
    "\\begin{equation*}\n",
    "w_0^{new} = w_0^{current} -   \\eta \\cdot \\frac{\\partial L_{MSE}}{\\partial w_0}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{w_1^{new}} = \\mathbf{w_1^{current}} -  \\eta \\cdot \\frac{\\partial L_{MSE}}{\\partial \\mathbf{w_1}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(lr):\n",
    "    y_hat = x @ w\n",
    "    loss = mse(y, y_hat)\n",
    "    # calculate the gradient of a tensor! It is now stored at w.grad\n",
    "    loss.backward()\n",
    "\n",
    "    # To prevent tracking history and using memory\n",
    "    # (code block where we don't need to track the gradients but only modify the values of tensors)\n",
    "    with torch.no_grad():\n",
    "        # lr is the learning rate. Good learning rate is a key part of Neural Networks.\n",
    "        w.sub_(lr * w.grad)\n",
    "        # We want to zero the gradient before we are re-evaluate it.\n",
    "        w.grad.zero_()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, we need to set the gradients to zero before starting to do back propragation because PyTorch accumulates the gradients on subsequent backward passes. This is convenient while training Recurrent Neural Networks (RNNs). So, the default action is to accumulate or sum the gradients on every `loss.backward()` call.\n",
    "Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the minimum (or maximum, in case of maximization objectives).\n",
    "\n",
    "Explanations about how PyTorch calculates the gradients can be found [here](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(w1, w0, x):\n",
    "    return w1 * x + w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.as_tensor([-2.0, -3])\n",
    "w = nn.Parameter(w)\n",
    "lr = 0.1\n",
    "losses = []\n",
    "epoch = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(x[:, 0], y)\n",
    "(line,) = ax.plot(\n",
    "    x[:, 0],\n",
    "    lin(w.detach().numpy()[0], w.detach().numpy()[1], x.detach().numpy()[:, 0]),\n",
    "    c=\"firebrick\",\n",
    ")\n",
    "# line, = ax.plot(x[:,0], y, c='firebrick')\n",
    "ax.set_title(\"Loss = 0.00\")\n",
    "plt.close()\n",
    "\n",
    "# Train model and perform gradient descent\n",
    "for _ in range(epoch):\n",
    "    loss = step(lr)\n",
    "    losses.append(loss.detach().numpy())\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    ax.set_title(\"Loss = %.2f\" % losses[i])\n",
    "    line.set_data(\n",
    "        x.detach().numpy()[:, 0],\n",
    "        lin(w.detach().numpy()[0], w.detach().numpy()[1], x.detach().numpy()[:, 0]),\n",
    "    )\n",
    "    return (line,)\n",
    "\n",
    "\n",
    "animation = FuncAnimation(fig, animate, frames=70, interval=150, blit=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you have some difficulties running the cell below without importing certain packages. Run the following code in a terminal in `L96M2lines` environment.\n",
    "```shell\n",
    "conda install -c conda-forge ffmpeg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(animation.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"figs/Gradient_descent2.png\", width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(np.array(losses))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Iteration\")\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "In Deep learning, we use a variation of gradient descent called [mini-batch gradient descent](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/). Instead of calculating the gradient over the whole training data before changing model weights (coefficients), we take a subset (batch) of our data, and change the values of the weights after we calculated the gradient over this subset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
