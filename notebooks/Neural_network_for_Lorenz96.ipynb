{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using neural networks for L96 parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "from IPython.display import HTML, Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from L96_model import L96, RK2, RK4, EulerFwd, L96_eq1_xdot, integrate_L96_2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring reproducibility\n",
    "np.random.seed(14)\n",
    "torch.manual_seed(14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, [Lorenz (1996)](https://www.ecmwf.int/en/elibrary/10829-predictability-problem-partly-solved) describes a two-time scale dynamical system using two equations which are:\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F - \\left( \\frac{hc}{b} \\right) \\sum_{j=0}^{J-1} Y_{j,k}\n",
    "\\\\\n",
    "\\frac{d}{dt} Y_{j,k}\n",
    "&= - cbY_{j+1,k} \\left( Y_{j+2,k} - Y_{j-1,k} \\right) - c Y_{j,k} + \\frac{hc}{b} X_k\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the GCM Classes\n",
    "\n",
    "In all the GCMs, we set time stepping using the fourth order Runge-Kutta method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Without* Neural Network Parameterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCM_without_parameterization:\n",
    "    \"\"\"GCM without parameterization\n",
    "\n",
    "    Args:\n",
    "        F: Forcing term\n",
    "        time_stepping: Time stepping method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F, time_stepping=RK4):\n",
    "        self.F = F\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, _):\n",
    "        \"\"\"Compute right hand side of the the GCM equations\"\"\"\n",
    "        return L96_eq1_xdot(X, self.F)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"Run GCM\n",
    "\n",
    "        Args:\n",
    "            X0: Initial conditions of X\n",
    "            dt: Time increment\n",
    "            nt: Number of forward steps to take\n",
    "            param: Parameters of closure\n",
    "\n",
    "        Returns:\n",
    "            Model output for all variables of X at each timestep\n",
    "            along with the corresponding time units\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Parameterization in RHS of Equation for Tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCM_linear_parameterization:\n",
    "    \"\"\"GCM with linear parameterization\n",
    "\n",
    "    Args:\n",
    "        F: Forcing term\n",
    "        parameterization: Parameterization function\n",
    "        time_stepping: Time stepping method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F, parameterization, time_stepping=RK4):\n",
    "        self.F = F\n",
    "        self.parameterization = parameterization\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        \"\"\"Compute right hand side of the the GCM equations\"\"\"\n",
    "        return L96_eq1_xdot(X, self.F) - self.parameterization(param, X)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"Run GCM\n",
    "\n",
    "        Args:\n",
    "            X0: Initial conditions of X\n",
    "            dt: Time increment\n",
    "            nt: Number of forward steps to take\n",
    "            param: Parameters of closure\n",
    "\n",
    "        Returns:\n",
    "            Model output for all variables of X at each timestep\n",
    "            along with the corresponding time units\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *With* Neural Network Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCM_network:\n",
    "    \"\"\"GCM with neural network parameterization\n",
    "\n",
    "    Args:\n",
    "        F: Forcing term\n",
    "        network: Neural network\n",
    "        time_stepping: Time stepping method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F, network, time_stepping=RK4):\n",
    "        self.F = F\n",
    "        self.network = network\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, _):\n",
    "        \"\"\"Compute right hand side of the the GCM equations\"\"\"\n",
    "        if self.network.linear1.in_features == 1:\n",
    "            X_torch = torch.from_numpy(X)\n",
    "            X_torch = torch.unsqueeze(X_torch, 1)\n",
    "        else:\n",
    "            X_torch = torch.from_numpy(np.expand_dims(X, 0))\n",
    "\n",
    "        # Adding NN parameterization\n",
    "        return L96_eq1_xdot(X, self.F) + np.squeeze(self.network(X_torch).data.numpy())\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"Run GCM\n",
    "\n",
    "        Args:\n",
    "            X0: Initial conditions of X\n",
    "            dt: Time increment\n",
    "            nt: Number of forward steps to take\n",
    "            param: Parameters of closure\n",
    "\n",
    "        Returns:\n",
    "            Model output for all variables of X at each timestep\n",
    "            along with the corresponding time units\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the *Real World* to Generate the Ground Truth Dataset\n",
    "\n",
    "We initialise the L96 two time-scale model using $K$ (set to 8) values of $X$ and $J$ (set to 32) values of $Y$ for each $X$. The model is run for 20,000 timesteps to generate the dataset for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 20000\n",
    "forcing, dt, T = 18, 0.01, 0.01 * time_steps\n",
    "\n",
    "# Create a \"real world\" with K=8 and J=32\n",
    "W = L96(8, 32, F=forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Training Data\n",
    "\n",
    "Using the *real world* model created above we generate the training data (input and output pairs) for the neural network by running the true state and outputting subgrid tendencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The effect of Y on X is `xy_true`\n",
    "X_true, _, _, xy_true = W.run(dt, T, store=True, return_coupling=True)\n",
    "\n",
    "# Change the data type to `float32` in order to avoid doing type conversions later on\n",
    "X_true, xy_true = X_true.astype(np.float32), xy_true.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data to obtain the Training and Test (Validation) Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time steps for validation\n",
    "val_size = 4000\n",
    "\n",
    "# Training Data\n",
    "X_true_train = X_true[\n",
    "    :-val_size, :\n",
    "]  # Flatten because we first use single input as a sample\n",
    "subgrid_tend_train = xy_true[:-val_size, :]\n",
    "\n",
    "# Test Data\n",
    "X_true_test = X_true[-val_size:, :]\n",
    "subgrid_tend_test = xy_true[-val_size:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Loaders \n",
    "\n",
    "- `Dataset` and `Dataloader` classes provide a very convenient way of iterating over a dataset while training a deep learning model.\n",
    "\n",
    "- We need to iterate over the data because it is very slow and memory-intensive to hold all the data and to use gradient decent over all the data simultaneously (see more details [here](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) and [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of sample in each batch\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the X (state), Y (subgrid tendency) pairs for the linear regression local network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataset = Data.TensorDataset(\n",
    "    torch.from_numpy(np.reshape(X_true_train, -1)),\n",
    "    torch.from_numpy(np.reshape(subgrid_tend_train, -1)),\n",
    ")\n",
    "\n",
    "local_loader = Data.DataLoader(\n",
    "    dataset=local_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dataloader for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(np.reshape(X_true_test, -1)),\n",
    "    torch.from_numpy(np.reshape(subgrid_tend_test, -1)),\n",
    ")\n",
    "\n",
    "local_loader_test = Data.DataLoader(\n",
    "    dataset=local_dataset_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a batch of samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over the data to get one batch\n",
    "data_iterator = iter(local_loader)\n",
    "X_iter, subgrid_tend_iter = next(data_iterator)\n",
    "\n",
    "print(\"X (State):\\n\", X_iter)\n",
    "print(\"\\nY (Subgrid Tendency):\\n\", subgrid_tend_iter)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(X_iter, subgrid_tend_iter, \".\")\n",
    "plt.xlabel(\"State\", fontsize=20)\n",
    "plt.ylabel(\"Subgrid tendency\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Networks in PyTorch\n",
    "\n",
    "The **Universal Approximation Theorm** states that neural networks can approximate any continuous function. A visual demonstration that neural nets can compute any function can be seen in [this page](http://neuralnetworksanddeeplearning.com/chap4.html).\n",
    "\n",
    "In this notebook, we give a brief overview of neural networks and how to build them using PyTorch. If you want to go through it in depth, check out these resources:\n",
    "- [Deep Learning With Pytorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "- [Neural Networks](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architectures\n",
    "\n",
    "We will try to understand the fully connected networks with the help of Linear regression (and gradient descent).\n",
    "\n",
    "<center>\n",
    "  <img\n",
    "    src=\"https://miro.medium.com/max/720/1*VHOUViL8dHGfvxCsswPv-Q.png\"\n",
    "    width=400\n",
    "  />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Linear Regression Network\n",
    "\n",
    "First, we will build a linear regression \"network\" and later see how to generalize the linear regression in order to use fully connected neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(1, 1)  # A single input and a single output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This method is automatically executed when\n",
    "        # we call a object of this class\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_network = LinearRegression()\n",
    "linear_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Predictions from the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_input = torch.randn(1, 1)\n",
    "out = linear_network(net_input)\n",
    "print(f\"The output of the random input is: {out.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Loss Function\n",
    "\n",
    "In order to check how well our network is modeling the dataset, we need to define a loss function. For our task, we choose the *Mean Squared Error* metric as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Load the input and output pair from the data loader\n",
    "X_tmp = next(iter(local_loader))\n",
    "\n",
    "# Predict the output\n",
    "y_tmp = linear_network(torch.unsqueeze(X_tmp[0], 1))\n",
    "\n",
    "# Calculate the MSE loss\n",
    "loss = criterion(y_tmp, torch.unsqueeze(X_tmp[1], 1))\n",
    "print(f\"MSE Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero the gradient buffers of all parameters\n",
    "linear_network.zero_grad()\n",
    "\n",
    "print(\"Gradients before backward:\")\n",
    "print(linear_network.linear1.bias.grad)\n",
    "\n",
    "# Compute the gradients\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "print(\"\\nGradients after backward:\")\n",
    "print(linear_network.linear1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the Weights using an Optimizer\n",
    "\n",
    "Now in order to make the network learn, we need an algorithm that will update its weights depending on the loss function. This is achieved by using an optimizer. The implementation of almost every optimizer that we'll ever need can be found in PyTorch itself. The choice of which optimizer we choose might be very important as it will determine how fast the network will be able to learn.\n",
    "\n",
    "In the example below, we show one of the popular optimizers `SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(linear_network.parameters(), lr=0.003, momentum=0.9)\n",
    "print(\"Before backward pass: \\n\", list(linear_network.parameters())[0].data.numpy())\n",
    "\n",
    "loss.backward(retain_graph=True)\n",
    "optimizer.step()\n",
    "\n",
    "print(\"\\nAfter backward pass: \\n\", list(linear_network.parameters())[0].data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimizer usually consists of two major hyperparameters called the **learning rate** and **momentum**. The **learning rate** determines the magnitude with which the weights of the network update thus making it crucial to choose the correct learning rate ($LR$) otherwise the network will either fail to train, or take much longer to converge. To read about **momentum**, check out this [blog post](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d).\n",
    "\n",
    "The  effective value of the gradient $V$ at step $t$ in SGD with momentum ($\\beta$) is determined by\n",
    "\n",
    "\\begin{equation}\n",
    "V_t = \\beta V_{t-1} + (1-\\beta) \\nabla_w L(W,X,y)\n",
    "\\end{equation}\n",
    "\n",
    "and the updates to the weights will be\n",
    "\n",
    "\\begin{equation}\n",
    "w^{new} = w^{old} - LR * V_t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam Optimizer\n",
    "\n",
    "Another popular optimizer that is used in many neural networks is the Adam optimizer. It is an adaptive learning rate method that computes individual learning rates for different parameters. For further reading, check out this [post](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c) about Adam, and this [post](https://www.ruder.io/optimizing-gradient-descent/) about other optimizers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining it all Together: Training the Whole Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Training and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, criterion, loader, optimizer):\n",
    "    \"\"\"Train the network for one epoch\"\"\"\n",
    "    network.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        # Get predictions\n",
    "        if len(batch_x.shape) == 1:\n",
    "            # This if block is needed to add a dummy dimension if our inputs are 1D\n",
    "            # (where each number is a different sample)\n",
    "            prediction = torch.squeeze(network(torch.unsqueeze(batch_x, 1)))\n",
    "        else:\n",
    "            prediction = network(batch_x)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(prediction, batch_y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backpropagation to compute the gradients and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(network, criterion, loader):\n",
    "    \"\"\"Test the network\"\"\"\n",
    "    network.eval()  # Evaluation mode (important when having dropout layers)\n",
    "\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            # Get predictions\n",
    "            if len(batch_x.shape) == 1:\n",
    "                # This if block is needed to add a dummy dimension if our inputs are 1D\n",
    "                # (where each number is a different sample)\n",
    "                prediction = torch.squeeze(network(torch.unsqueeze(batch_x, 1)))\n",
    "            else:\n",
    "                prediction = network(batch_x)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(prediction, batch_y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        # Get an average loss for the entire dataset\n",
    "        test_loss /= len(loader)\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_model(network, criterion, optimizer, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"Train and validate the network\"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        train_loss = train_model(network, criterion, train_loader, optimizer)\n",
    "        val_loss = test_model(network, criterion, val_loader)\n",
    "        print(f\"Training Loss: {train_loss:.6f} | Validation Loss: {val_loss:.6f}\\n\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Number of Epochs and the Optimizer\n",
    "\n",
    "Epochs refer to the number of times we iterate over the entire training data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "optimizer = optim.Adam(linear_network.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, _ = fit_model(\n",
    "    linear_network, criterion, optimizer, local_loader, local_loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the Weights of the Trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array(\n",
    "    [\n",
    "        linear_network.linear1.weight.data.numpy()[0][0],\n",
    "        linear_network.linear1.bias.data.numpy()[0],\n",
    "    ]\n",
    ")\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Predictions with Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = linear_network(\n",
    "    torch.unsqueeze(torch.from_numpy(np.reshape(X_true_test[:, 1], -1)), 1)\n",
    ")\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(predictions.detach().numpy()[0:1000], label=\"Predicted Values\")\n",
    "plt.plot(subgrid_tend_test[:1000, 1], label=\"True Values\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the Simple Linear Parameterization back to the GCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test = 10\n",
    "\n",
    "# Full L96 model\n",
    "X_full, _, _ = W.run(dt, T_test)\n",
    "X_full = X_full.astype(np.float32)\n",
    "\n",
    "init_conditions = X_true[-1, :]\n",
    "\n",
    "# GCM parameterized by the linear network\n",
    "gcm_net = GCM_network(forcing, linear_network)\n",
    "Xnn_1layer, t = gcm_net(init_conditions, dt, int(T_test / dt), linear_network)\n",
    "\n",
    "# GCM parameterized without parameterization\n",
    "gcm_no_param = GCM_without_parameterization(forcing)\n",
    "X_no_param, t = gcm_no_param(init_conditions, dt, int(T_test / dt))\n",
    "\n",
    "# GCM with naive parameterization\n",
    "naive_parameterization = lambda param, X: np.polyval(param, X)\n",
    "gcm = GCM_linear_parameterization(forcing, naive_parameterization)\n",
    "X_param, t = gcm(init_conditions, dt, int(T / dt), param=-weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_i = 200\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(t[:time_i], X_full[:time_i, 4], label=\"Full L96\")\n",
    "plt.plot(t[:time_i], Xnn_1layer[:time_i, 4], \".\", label=\"NN 1 layer\")\n",
    "plt.plot(t[:time_i], X_no_param[:time_i, 4], label=\"No parameterization\")\n",
    "plt.plot(t[:time_i], X_param[:time_i, 4], label=\"linear parameterization\")\n",
    "plt.legend(loc=\"upper left\", fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deeper Networks for Lorenz 96 (with Non-Local Features)\n",
    "\n",
    "Now we'll increase the complexity of our neural network by adding a few more linear layers to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <img\n",
    "    src=\"https://www.researchgate.net/publication/319201436/figure/fig1/AS:869115023589376@1584224577926/Visualisation-of-a-two-scale-Lorenz-96-system-with-J-8-and-K-6-Global-scale-values.png\"\n",
    "    width=400\n",
    "  />\n",
    "</center>\n",
    "\n",
    "<span> <center> *Fig. 1: Visualisation of a two-scale Lorenz '96 system with J = 8 and K = 6. Global-scale variables ($X_k$) are updated based on neighbouring variables and on the local-scale variables ($Y_{j,k}$) associated with the corresponding global-scale variable. Local-scale variabless are updated based on neighbouring variables and the associated global-scale variable. The neighbourhood topology of both local and global-scale variables is circular. Image from [Exploiting the chaotic behaviour of atmospheric models with reconfigurable architectures - Scientific Figure on ResearchGate.](https://www.researchgate.net/figure/Visualisation-of-a-two-scale-Lorenz-96-system-with-J-8-and-K-6-Global-scale-values_fig1_319201436)* </center> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create non-local Training and Test Dataset\n",
    "\n",
    "The datasets will have *8 inputs* and *8 outputs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of sample in each batch\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlocal_data = Data.TensorDataset(\n",
    "    torch.from_numpy(X_true_train),\n",
    "    torch.from_numpy(subgrid_tend_train),\n",
    ")\n",
    "\n",
    "loader = Data.DataLoader(dataset=nlocal_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlocal_data_test = Data.TensorDataset(\n",
    "    torch.from_numpy(X_true_test), torch.from_numpy(subgrid_tend_test)\n",
    ")\n",
    "\n",
    "loader_test = Data.DataLoader(\n",
    "    dataset=nlocal_data_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a class of a 3 layer fully-connected network with ReLU\n",
    "\n",
    "We now build a 3 layer neural network consisting of two hidden layers and an output layer. This time we use an activation function called `ReLU` (a very common choice) after every hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs\n",
    "        self.linear2 = nn.Linear(16, 16)\n",
    "        self.linear3 = nn.Linear(16, 8)  # 8 outputs\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need for Activation Functions\n",
    "\n",
    "**If layers contain only matrix multiplications, everything would be linear.**\n",
    "\n",
    "For example, if we have an input $x$ along with 2 layers of weight matrices $A$ and $B$ then the neural network would compute the output as $A(Bx)$, which is linear (in $x$). Thus, in order to introduce some non-linearity we use activation functions.\n",
    "\n",
    "Now the same neural network as above with an activation function $\\phi$ would compute the output as $A(\\phi(Bx))$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ReLU` activation function used in the `NetANN` network above is just a $max(0,X)$ function. Even a function as simple as this enables a typical NN to be a nonlinear function of the inputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curve of ReLU\n",
    "x = np.linspace(-2, 2, 50)\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(x, np.maximum(x, 0))\n",
    "plt.title(\"ReLU\", fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the network\n",
    "nn_3l = NetANN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Number of Epochs and the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "optimizer = optim.Adam(nn_3l.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss, val_loss = fit_model(\n",
    "    nn_3l, criterion, optimizer, loader, loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"Training loss\")\n",
    "plt.plot(val_loss, \"r\", label=\"Validation loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Predictions with the Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_3l(torch.from_numpy(X_true_test[:, :]))\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(predictions.detach().numpy()[0:1000, 1], label=\"NN Predicted values\")\n",
    "plt.plot(subgrid_tend_test[:1000, 1], label=\"True values\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the Simple Linear Parameterization back to the GCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test = 5\n",
    "\n",
    "# GCM parameterized by the global 3-layer network\n",
    "gcm_net_3layers = GCM_network(forcing, nn_3l)\n",
    "Xnn_3layer, t = gcm_net_3layers(init_conditions, dt, int(T_test / dt), nn_3l)\n",
    "\n",
    "# GCM parameterized by the linear network\n",
    "gcm_net_1layers = GCM_network(forcing, linear_network)\n",
    "Xnn_1layer, t = gcm_net_1layers(init_conditions, dt, int(T_test / dt), linear_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_i = 240\n",
    "channel = 1\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(t[:time_i], X_full[:time_i, channel], label=\"Full L96\")\n",
    "plt.plot(t[:time_i], Xnn_1layer[:time_i, channel], \".\", label=\"NN 1 layer local\")\n",
    "plt.plot(t[:time_i], Xnn_3layer[:time_i, channel], \".\", label=\"NN 3 layer global\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking over 100 Different Initial Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "err_1l, err_3l = [], []\n",
    "T_test = 1\n",
    "for i in range(100):\n",
    "    init_conditions_i = X_true[i * 10, :]\n",
    "\n",
    "    # GCM parameterized by the global 3-layer network\n",
    "    gcm_net_3layers = GCM_network(forcing, nn_3l)\n",
    "    Xnn_3layer_i, t = gcm_net_3layers(init_conditions_i, dt, int(T_test / dt), nn_3l)\n",
    "\n",
    "    # GCM parameterized by the linear network\n",
    "    gcm_net_1layers = GCM_network(forcing, linear_network)\n",
    "    Xnn_1layer_i, t = gcm_net_1layers(\n",
    "        init_conditions_i, dt, int(T_test / dt), linear_network\n",
    "    )\n",
    "\n",
    "    err_1l.append(\n",
    "        np.sum(np.abs(X_true[i * 10 : i * 10 + T_test * 100 + 1] - Xnn_1layer_i))\n",
    "    )\n",
    "    err_3l.append(\n",
    "        np.sum(np.abs(X_true[i * 10 : i * 10 + T_test * 100 + 1] - Xnn_3layer_i))\n",
    "    )\n",
    "\n",
    "print(f\"Sum of errors for 1 layer local: {sum(err_1l):.2f}\")\n",
    "print(f\"Sum of errors for 3 layer global: {sum(err_3l):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training further to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "train_loss, val_loss = fit_model(\n",
    "    nn_3l, criterion, optimizer, loader, loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Training and Validation Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"Training loss\")\n",
    "plt.plot(val_loss, \"r\", label=\"Validation loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Network\n",
    "\n",
    "Let's save the weights of the trained network so that we don't have to train it again if we want to use it in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save network\n",
    "save_path = \"./networks/network_3_layers_100_epoches.pt\"\n",
    "torch.save(nn_3l.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Overfitting\n",
    "\n",
    "One of the most common issues that happen while training a neural network is when the model memorizes the training dataset. It causes the model to perform very accurately on the training set but shows very poor performance on the validation set. This phenomenon is termed overfitting. One of the ways to prevent overfitting is to add regularization to our model as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The figure below is taken from Python Machine Learning book by Sebastian Raschka\n",
    "Image(filename=\"figs/overfitting.png\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curve on the far right of the plot above predicts perfectly on the given set, yet it's not the best choice. This is because if you were to gather some new data points, they most likely would not be on that curve. Instead, those new points would be closer to the curve in the middle graph since it generalizes better to the dataset.\n",
    "\n",
    "All ML algorithms have some form of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regularization Intuition\n",
    "\n",
    "Regularization can be thought of as **putting constraints on the model** to obtain better generalizability i.e. *avoiding remembering* the training data.\n",
    "\n",
    "One of the ways to achieve this can be by adding a term to the loss function such that:\n",
    "> Loss = Training Loss + Regularization\n",
    "\n",
    "This puts a penalty for making the model more complex.\n",
    "\n",
    "Very braodly speaking (just to gain intuition) - if we want to reduce the training loss (reduce bias) we should try using a more complex model (if we have enough data) and if we want to reduce overfitting (reduce variace) we should simplify or constraint the model (increase regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the ways to add regularization in neural networks are\n",
    "\n",
    "- Dropout (added in the definition of the network). \n",
    "- Early stopping\n",
    "- Weight decay (added in the optimizer part - see `optim.Adam` in PyTorch)\n",
    "- Data augmentation (usually for images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight decay (L2 norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight decay is usually defined as a term that’s added directly to the update rule.\n",
    "Namely, to update a certain weight $w$ in the $i+1$ iteration, we would use a modified rule:\n",
    "\n",
    "$w_{i+1} = w_{i} - \\gamma ( \\frac{\\partial L}{\\partial w} + A w_{i})$\n",
    "\n",
    "In practice, this is almost identical to L2 regularization, though there is some difference (e.g., see [here](https://bbabenko.github.io/weight-decay/))\n",
    "\n",
    "Weight decay is one of the parameters of the optimizer - see `torch.optim.SGD`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Weight Decay\n",
    "\n",
    "Now we try to train our `NetANN` model again but this time by adding a weight decay to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_3l_decay = NetANN()\n",
    "\n",
    "n_epochs = 10\n",
    "optimizer = optim.Adam(nn_3l_decay.parameters(), lr=0.003, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss, val_loss = fit_model(\n",
    "    nn_3l_decay, criterion, optimizer, loader, loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the training and validation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"Training loss\")\n",
    "plt.plot(val_loss, \"r\", label=\"Validation loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout means randomly deactivating or temporarily removing some units from a layer of the network while training, along with all its incoming and outgoing connections. See more details [here](http://jmlr.org/papers/v15/srivastava14a.html).\n",
    "It is usually the most useful regularization that we can do in fully connected layers.\n",
    "\n",
    "In convolutional layers dropout makes less sense - see more discussion [here](https://www.kdnuggets.com/2018/09/dropout-convolutional-networks.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image taken from: http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "Image(filename=\"figs/Dropout_layer.png\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the network defined below, we add dropout to with a probability of 20% to each layer. This means that during each training step, random 20% of the units within each layer will be deactivated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetANNDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)\n",
    "        self.linear2 = nn.Linear(16, 16)\n",
    "        self.linear3 = nn.Linear(16, 8)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout regularization\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network with very high dropout\n",
    "nn_3l_drop = NetANNDropout(dropout_rate=0.8)\n",
    "\n",
    "n_epochs = 10\n",
    "optimizer = optim.Adam(nn_3l_drop.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss, val_loss = fit_model(\n",
    "    nn_3l_drop, criterion, optimizer, loader, loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the training and validation loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"Training loss\")\n",
    "plt.plot(val_loss, \"r\", label=\"Validation loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the Learning Rate\n",
    "\n",
    "While training a neural network **selecting a good learning rate (LR) is essential for both fast convergence and a lower error**. A high learning rate can cause the training loss to never converge while a too small learning rate will cause the model to converge extremely slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Optimal Learning Rate\n",
    "\n",
    "To choose the optimal learning rate for our network, we can use an LR finding algorithm. The objective of a LR Finder is to find the highest LR which still minimises the loss and does not make the loss diverge/explode. This is done by first starting with an extremely small LR and then increasing the LR after each batch until the corresponding loss starts to explode. To read more about learning rate finders, read [this blog](https://towardsdatascience.com/speeding-up-neural-net-training-with-lr-finder-c3b401a116d0).\n",
    "\n",
    "For our use case, we use the LR finder from the `torch-lr-finder` package to find the best learning rate for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model and the optimizer. The optimizer is **initialized with a very small learning rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_3l_lr = NetANN()\n",
    "optimizer = optim.Adam(nn_3l_lr.parameters(), lr=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup the LR finder and make it run for 200 iterations during which the learning rate varies from 1e-7 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_finder = LRFinder(nn_3l_lr, optimizer, criterion)\n",
    "lr_finder.range_test(loader, end_lr=100, num_iter=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the LR vs the loss curve to find the best learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the lr vs the loss curve\n",
    "lr_finder.plot()\n",
    "\n",
    "# Reset the model and optimizer to their initial state\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the curve, we see that at the learning of approximately 0.01 we get the steepest gradient. So we choose 0.01 as the learning rate for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "optimizer = optim.Adam(nn_3l_lr.parameters(), lr=0.01)\n",
    "\n",
    "train_loss, val_loss = fit_model(\n",
    "    nn_3l_lr, criterion, optimizer, loader, loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the training and validation loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"Training loss\")\n",
    "plt.plot(val_loss, \"r\", label=\"Validation loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the loss curves we can see that **the loss has converged much faster** than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNormalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the activation values such that the hidden representation don’t vary drastically and also helps to get improvement in the training speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclic learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cyclic learning rate policy, introduced in [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186), cycles the learning rate between two boundaries with a constant frequency in a triangular fashion. To read more about the cyclic learning rates and the one cycle policy, read [here](https://sgugger.github.io/the-1cycle-policy.html).\n",
    "\n",
    "In PyTorch, cyclic learning rate can be used from `optim.lr_scheduler.CyclicLR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image taken from - https://pyimagesearch.com/2019/07/29/cyclical-learning-rates-with-keras-and-deep-learning/\n",
    "Image(filename=\"figs/cyclic_lr.png\", width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
