{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33938dd5-f864-4642-8ce0-c44bb81bd60d",
   "metadata": {},
   "source": [
    "# Tuning GCM Parameterizations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "686988e8",
   "metadata": {},
   "source": [
    "*The objective of this notebook is to show how GCM closures can be tuned in practice. We will assume a specific formulation of a closure and estimate its parameters through a standard optimisation procedure with DA.* \n",
    "\n",
    "What is DA?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cb1f6aa",
   "metadata": {},
   "source": [
    "**Resources** : We have used material from Emmanuel Cosme's nice GitHub [repository](https://github.com/ecosme38/Data-Assimilation-Notebooks). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa836f4",
   "metadata": {},
   "source": [
    "## The GCM Paramterization Problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a03d0b",
   "metadata": {},
   "source": [
    "Our starting point is {doc}`gcm-parameterization-problem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6ac7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "# L96 provides the \"real world\", L96_eq1_xdot is the beginning of rhs of X tendency\n",
    "from L96_model import L96, RK2, RK4, EulerFwd, L96_eq1_xdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed gives us reproducible results\n",
    "np.random.seed(13)\n",
    "\n",
    "# Create a \"real world\" with K=8 and J=32\n",
    "W = L96(8, 32, F=18)\n",
    "\n",
    "# Run \"real world\" for 3 days to forget initial conditons\n",
    "# (store=True save the final state as an initial condition for the next run)\n",
    "W.run(0.05, 3.0, store=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "777fef0d",
   "metadata": {},
   "source": [
    "From here on we can use `W.X` as perfect initial conditions for a model and sample the real world using `W.run(dt,T)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCM:\n",
    "    def __init__(self, F, parameterization, time_stepping=EulerFwd):\n",
    "        self.F = F\n",
    "        self.parameterization = parameterization\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        return L96_eq1_xdot(X, self.F) - self.parameterization(param, X)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66a2385b",
   "metadata": {},
   "source": [
    "As a first step, we illustrate introducing a polynomial parameterization to GCM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec03cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_parameterization = lambda param, X: np.polyval(param, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18baf519",
   "metadata": {},
   "outputs": [],
   "source": [
    "F, dt, T = 18, 0.01, 5.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "# where did param values come from?\n",
    "X, t = gcm(W.X, dt, int(T / dt), param=[0.85439536, 1.75218026])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "235aed7d",
   "metadata": {},
   "source": [
    "### Comparing model and true trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de79307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This samples the real world with the same time interval as \"dt\" used by the model\n",
    "X_true, _, _ = W.run(dt, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d260623",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(t, X_true[:, 4], label=\"Truth\")\n",
    "plt.plot(t, X[:, 4], label=\"Model\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend(fontsize=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b2947",
   "metadata": {},
   "source": [
    "We also copy the distance metrics used in the gcm-parameterization-problem notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8989a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - pointwise distance :\n",
    "def pointwise(X1, X2, L=1.0):  # computed over some window t<L.\n",
    "    D = (X1 - X2)[np.where(t < L)]\n",
    "    return np.sqrt(D**2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca2ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - mean state metric :\n",
    "def dist_mean(X1, X2, L=1.0):\n",
    "    _X1 = X1[np.where(t < L)]\n",
    "    _X2 = X2[np.where(t < L)]\n",
    "    return np.sqrt((_X1.mean(axis=0) - _X2.mean(axis=0)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba15eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_initial_tendency(X1, X2):\n",
    "    T1 = X1[1, :] - X1[0, :]\n",
    "    T2 = X2[1, :] - X2[0, :]\n",
    "    return np.sqrt((T1 - T2) ** 2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3200004c",
   "metadata": {},
   "source": [
    "## Variational estimation of optimal parameters for a predefined closure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73bf060",
   "metadata": {},
   "source": [
    "We will try here to estimate the parameters of `naive_parameterization` with a variational approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5141f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - assuming the formulation of the parameterization\n",
    "gcm = GCM(F, naive_parameterization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b14903",
   "metadata": {},
   "source": [
    "### Estimating parameters based on one initial condition and one time step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57669f4",
   "metadata": {},
   "source": [
    "#### Cost function "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91466a50",
   "metadata": {},
   "source": [
    "What we will be doing here is very close to what is done with classical variational data assimilation, where we try to estimate the state of the parameters of a model through the minimization of a cost function $J$. This is also very close to what is done when parameterizations are encoded as neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9ab28",
   "metadata": {},
   "source": [
    "We introduce a cost function $J(p)$ which depends on the parameters of the closure. \n",
    "\n",
    "$$J(p) = ||X_p - X_{true}||_{d}$$\n",
    "\n",
    "where $p=[p1,p2]$, $X_p$ is GCM solution computed with with parameters $p$ and $||\\cdot ||_{d}$ is one of the distances above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce939e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(param):\n",
    "    F, dt, T = 18, 0.01, 0.01\n",
    "    X_gcm, t = gcm(W.X, dt, int(T / dt), param=param)\n",
    "    return norm_initial_tendency(X_true, X_gcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6198df7",
   "metadata": {},
   "source": [
    "#### Minimization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d38b2b",
   "metadata": {},
   "source": [
    "The problem dimension being small enough (2 parameters to find), one can use efficient derivative-free optimization methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b23971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = np.array([0.85439536, 1.75218026])  #  prior\n",
    "res = opt.minimize(cost_function, prior, method=\"Powell\")\n",
    "opt_param = res[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdffeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_param\n",
    "# Do we need this line?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6b43c9e",
   "metadata": {},
   "source": [
    "##### Let's test our closure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c65cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "F, dt, T = 18, 0.01, 100.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "X_optimized, t = gcm(W.X, dt, int(T / dt), param=opt_param)\n",
    "X_prior, t = gcm(W.X, dt, int(T / dt), param=prior)\n",
    "\n",
    "# - ... the true state\n",
    "X_true, _, _ = W.run(dt, T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9172f27e",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(t[:500], X_true[:500, 4], label=\"Truth\")\n",
    "plt.plot(t[:500], X_prior[:500, 4], label=\"Initial GCM\")\n",
    "plt.plot(t[:500], X_optimized[:500, 4], label=\"Optimized GCM\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcc951db",
   "metadata": {},
   "source": [
    "The results are better but not great. This problem is related to the question of *a priori* versus *a posteriori* skill in LES closures. \n",
    "\n",
    "What is LES closures? Add link maybe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de43a461",
   "metadata": {},
   "source": [
    "### Estimating parameters which optimize longer trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "F, dt, T = 18, 0.01, 5.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "X_true, _, _ = W.run(dt, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gcm, t = gcm(W.X, dt, int(T / dt), param=[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(param):\n",
    "    F, dt, T = 18, 0.01, 5\n",
    "    X_gcm, t = gcm(W.X, dt, int(T / dt), param=param)\n",
    "    return pointwise(X_true, X_gcm, L=5.0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b152335",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = np.array([0.85439536, 1.75218026])  #  prior\n",
    "res = opt.minimize(cost_function, prior, method=\"Powell\")\n",
    "opt_param = res[\"x\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5613d4a",
   "metadata": {},
   "source": [
    "##### Let's test our closure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a96080",
   "metadata": {},
   "outputs": [],
   "source": [
    "F, dt, T = 18, 0.01, 100.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "X_optimized, t = gcm(W.X, dt, int(T / dt), param=opt_param)\n",
    "X_prior, t = gcm(W.X, dt, int(T / dt), param=prior)\n",
    "\n",
    "X_true, _, _ = W.run(dt, T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a44d0463",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(t[:500], X_true[:500, 4], label=\"Truth\")\n",
    "plt.plot(t[:500], X_prior[:500, 4], label=\"Initial GCM\")\n",
    "plt.plot(t[:500], X_optimized[:500, 4], label=\"Optimized GCM\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47764aff",
   "metadata": {},
   "source": [
    "Our closure produces better results but it is not clear how this would generalize to unseen initial conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814ad48",
   "metadata": {},
   "source": [
    "## Discussion and Possible next steps\n",
    "\n",
    " - Estimating parameters over one time-step but with an ensemble of initial conditions. \n",
    " - Encode the closure with neural networks.\n",
    " - The need for differentiable GCMs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
