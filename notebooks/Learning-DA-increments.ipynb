{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f25c823",
   "metadata": {},
   "source": [
    "# Learning Data Assimilation Increments\n",
    "\n",
    "This notebook is a derived from that in the \"Learning neural networks for Lorenz 96\" and \"DA demo L96\".\n",
    "- We've restricted it to only using the 3-layer network (not the linear regression model)\n",
    "- We use parameters of the Lorenz 1996 model that match those of Wilks, 2005; K=8, J=32, F=18.\n",
    "- In the first part of this notebook, and for the purposes of this illustration, we removed all the unused options and parameters in the DA algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53133bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn\n",
    "import torch.nn.functional\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "\n",
    "# The following imports are from local module files\n",
    "import DA_methods\n",
    "from L96_model import L96, L96_eq1_xdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "rng = np.random.default_rng(3210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96737f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to represent the GCM, and many other convenience functions used within the DA algorithm\n",
    "\n",
    "\n",
    "def GCM(X0, F, dt, nt, param=[0]):\n",
    "    time, hist, X = (\n",
    "        dt * np.arange(nt + 1),\n",
    "        np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "        X0.copy(),\n",
    "    )\n",
    "    hist[0] = X\n",
    "\n",
    "    for n in range(nt):\n",
    "        X = X + dt * (L96_eq1_xdot(X, F) - np.polyval(param, X))\n",
    "\n",
    "        hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "    return hist, time\n",
    "\n",
    "\n",
    "def s(k, K):\n",
    "    \"\"\"A non-dimension coordinate from -1..+1 corresponding to k=0..K\"\"\"\n",
    "    return 2 * (0.5 + k) / K - 1\n",
    "\n",
    "\n",
    "def get_dist(i, j, K):\n",
    "    return abs(i - j) if abs(i - j) <= 0.5 * K / 2 else K - abs(i - j)\n",
    "\n",
    "\n",
    "# Generate observation operator, assuming linearity and model space observations\n",
    "def ObsOp(K, l_obs, t_obs, i_t):\n",
    "    nobs = l_obs.shape[-1]\n",
    "    H = np.zeros((nobs, K))\n",
    "    H[range(nobs), l_obs[t_obs == i_t]] = 1\n",
    "    return H\n",
    "\n",
    "\n",
    "# localize covariance matrix based on the Gaspari-Cohn function\n",
    "def cov_loc(B, loc=0):\n",
    "    M, N = B.shape\n",
    "    X, Y = np.ix_(np.arange(M), np.arange(N))\n",
    "    dist = np.vectorize(get_dist)(X, Y, M)\n",
    "    W = np.vectorize(gaspari_cohn)(dist, loc)\n",
    "    return B * W, W\n",
    "\n",
    "\n",
    "def gaspari_cohn(distance, radius):\n",
    "    if distance == 0:\n",
    "        weight = 1.0\n",
    "    else:\n",
    "        if radius == 0:\n",
    "            weight = 0.0\n",
    "        else:\n",
    "            ratio = abs(distance / radius)\n",
    "            weight = 0.0\n",
    "            if ratio <= 1:\n",
    "                weight = (\n",
    "                    -(ratio**5) / 4\n",
    "                    + ratio**4 / 2\n",
    "                    + 5 * ratio**3 / 8\n",
    "                    - 5 * ratio**2 / 3\n",
    "                    + 1\n",
    "                )\n",
    "            elif ratio <= 2:\n",
    "                weight = (\n",
    "                    ratio**5 / 12\n",
    "                    - ratio**4 / 2\n",
    "                    + 5 * ratio**3 / 8\n",
    "                    + 5 * ratio**2 / 3\n",
    "                    - 5 * ratio\n",
    "                    + 4\n",
    "                    - 2 / 3 / ratio\n",
    "                )\n",
    "    return weight\n",
    "\n",
    "\n",
    "def find_obs(loc, obs, t_obs, l_obs, period):\n",
    "    t_period = np.where((t_obs[:, 0] >= period[0]) & (t_obs[:, 0] < period[1]))\n",
    "    obs_period = np.zeros(t_period[0].shape)\n",
    "    obs_period[:] = np.nan\n",
    "    for i in np.arange(len(obs_period)):\n",
    "        if np.any(l_obs[t_period[0][i]] == loc):\n",
    "            obs_period[i] = obs[t_period[0][i]][l_obs[t_period[0][i]] == loc]\n",
    "    return obs_period\n",
    "\n",
    "\n",
    "def running_ave(X, N):\n",
    "    if N % 2 == 0:\n",
    "        N1, N2 = -N / 2, N / 2\n",
    "    else:\n",
    "        N1, N2 = -(N - 1) / 2, (N + 1) / 2\n",
    "\n",
    "    X_sum = np.zeros(X.shape)\n",
    "    for i in np.arange(N1, N2):\n",
    "        X_sum = X_sum + np.roll(X, int(i), axis=0)\n",
    "    return X_sum / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goldilocks settings\n",
    "config = dict(\n",
    "    K=8,  # Dimension of L96 \"X\" variables\n",
    "    J=32,  # Dimension of L96 \"Y\" variables\n",
    "    obs_freq=10,  # observation frequency (number of sampling intervals (si) per observation)\n",
    "    F_truth=18,  # F for truth signal\n",
    "    F_fcst=18,  # F for forecast (DA) model\n",
    "    GCM_param=np.array(\n",
    "        [0, 0, 0, 0]\n",
    "    ),  # polynomial coefficicents for GCM parameterization\n",
    "    ns_da=4000,  # number of time samples for DA\n",
    "    ns=4000,  # number of time samples for truth signal\n",
    "    ns_spinup=200,  # number of time samples for spin up\n",
    "    dt=0.005,  # model timestep\n",
    "    si=0.005,  # truth sampling interval\n",
    "    B_loc=0.0,  # spatial localization radius for DA\n",
    "    DA=\"EnKF\",  # DA method\n",
    "    nens=50,  # number of ensemble members for DA\n",
    "    inflate_opt=\"relaxation\",  # method for DA model covariance inflation\n",
    "    inflate_factor=0.86,  # inflation factor\n",
    "    obs_density=1.0,  # fraction of spatial gridpoints where observations are collected\n",
    "    DA_freq=10,  # assimilation frequency (number of sampling intervals (si) per assimilation step)\n",
    "    obs_sigma=0.1,  # observational error standard deviation\n",
    "    initial_spread=0.1,  # Initial spread added to initial conditions\n",
    ")\n",
    "\n",
    "# Uncomment blocks below to perturb the above settings\n",
    "\n",
    "# # Less certain observations\n",
    "# config['obs_sigma'] = 1.0\n",
    "# config['initial_spread'] = 1.0\n",
    "# config['inflate_factor'] = 0.5\n",
    "\n",
    "# # Less frequent observations\n",
    "# config['obs_freq'] = 50\n",
    "# config['DA_freq'] = 50\n",
    "# config['inflate_factor'] = 0.4\n",
    "\n",
    "# # Very infrequent observations\n",
    "# config['obs_freq'] = 200\n",
    "# config['DA_freq'] = 200\n",
    "# config['inflate_factor'] = 0.5\n",
    "\n",
    "# # More frequent observations\n",
    "# config['obs_freq'] = 5\n",
    "# config['DA_freq'] = 5\n",
    "# config['inflate_factor'] = 0.9\n",
    "\n",
    "# # Very frequent observations\n",
    "# config['obs_freq'] = 1\n",
    "# config['DA_freq'] = 1\n",
    "# config['inflate_factor'] = 0.98\n",
    "\n",
    "# # Very frequent observations but less accurate\n",
    "# config['obs_freq'] = 1\n",
    "# config['DA_freq'] = 1\n",
    "# config['obs_sigma'] = 1.0\n",
    "# config['initial_spread'] = 1.0\n",
    "# config['inflate_factor'] = 0.9\n",
    "\n",
    "# # Different time-scale\n",
    "# config['F_fcst'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e4ef9",
   "metadata": {},
   "source": [
    "The 'real world\" is the Lorenz '96 model:\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F - \\left( \\frac{hc}{b} \\right) \\sum_{j=0}^{J-1} Y_{j,k}\n",
    "\\\\\n",
    "\\frac{d}{dt} Y_{j,k}\n",
    "&= - cbY_{j+1,k} \\left( Y_{j+2,k} - X_{j-1,k} \\right) - c Y_{j,k} + \\frac{hc}{b} X_k\n",
    "\\end{align}\n",
    "\n",
    "The cell below spins-up a state and then records a series of $X$ and $Y$ at time $t$ in arrays `X_truth`, `Y_truth` and `t_truth` respectively. The initial state $X(t=0$ is recorded in `X_init` (and equal to `X_truth[0]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the \"truth\" 2-scale L96 model and generate initial conditions from a short spinup\n",
    "M_truth = L96(config[\"K\"], config[\"J\"], F=config[\"F_truth\"], dt=config[\"dt\"])\n",
    "M_truth.set_state(rng.standard_normal((config[\"K\"])), 0 * M_truth.j)\n",
    "X_init, Y_init, _ = M_truth.run(config[\"si\"], config[\"si\"] * config[\"ns_spinup\"])\n",
    "X_init, Y_init = X_init[-1, :], Y_init[-1, :]\n",
    "M_truth.set_state(X_init, Y_init)\n",
    "\n",
    "# Run L96 to generate the \"truth\"\n",
    "X_truth, Y_truth, t_truth = M_truth.run(config[\"si\"], config[\"si\"] * config[\"ns\"])\n",
    "\n",
    "del Y_init  # Not used below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7ce87",
   "metadata": {},
   "source": [
    "Now we create some \"observations\" of the \"real world\" by sampling at `obs_freq` intervals and adding some noise (observational error). `X_obs` are the observations at `k=l_obs` positions and `t=t_truth[t_obs]` times. Note that `t_obs` is an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff525d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the \"truth\" to generate observations at certain times (t_obs) and locations (l_obs)\n",
    "t_obs = np.tile(\n",
    "    config[\"obs_freq\"] + np.arange(0, config[\"ns_da\"], config[\"obs_freq\"]),\n",
    "    [int(config[\"K\"] * config[\"obs_density\"]), 1],\n",
    ").T\n",
    "l_obs = np.zeros(t_obs.shape, dtype=\"int\")\n",
    "for i in range(l_obs.shape[0]):\n",
    "    l_obs[i, :] = rng.choice(\n",
    "        config[\"K\"], int(config[\"K\"] * config[\"obs_density\"]), replace=False\n",
    "    )\n",
    "X_obs = X_truth[t_obs, l_obs] + config[\"obs_sigma\"] * rng.standard_normal(l_obs.shape)\n",
    "\n",
    "# Calculated observation covariance matrix, assuming independent observations\n",
    "R = config[\"obs_sigma\"] ** 2 * np.eye(int(config[\"K\"] * config[\"obs_density\"]))\n",
    "\n",
    "plt.figure(figsize=[10, 6])\n",
    "plt.plot(t_truth[:], X_truth[:, 0], label=\"truth\")\n",
    "plt.scatter(\n",
    "    t_truth[t_obs[1:, 0]],\n",
    "    find_obs(0, X_obs, t_obs, l_obs, [t_obs[0, 0], t_obs[-1, 0]]),\n",
    "    color=\"k\",\n",
    "    label=\"obs\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time t\")\n",
    "plt.ylabel(\"X(t)\")\n",
    "plt.title(\"Observations at k=0\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f2c954",
   "metadata": {},
   "source": [
    "We run the model in forward mode for 5000 steps to calculate the background covariance. The model is the \"GCM\" function defined above which integrates forward\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F\n",
    "\\end{align}\n",
    "The absence of the coupling term to the $Y$ equations makes this a model with \"missing physics\" that we hope the Ensemble Kalman Filter will correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate climatological background covariance for 1-scale L96 model\n",
    "X1_clim, _ = GCM(X_init, config[\"F_fcst\"], config[\"dt\"], 5000)\n",
    "B_clim1 = np.cov(X1_clim.T)\n",
    "del X1_clim\n",
    "\n",
    "# load pre-calculated climatological background covariance matrix from a long simulation\n",
    "# B_clim1=np.load('B_clim_L96s.npy')\n",
    "B_loc, W_clim = cov_loc(B_clim1, loc=config[\"B_loc\"])\n",
    "\n",
    "B_corr1 = np.zeros(B_clim1.shape)\n",
    "for i in range(B_clim1.shape[0]):\n",
    "    for j in range(B_clim1.shape[1]):\n",
    "        B_corr1[i, j] = B_clim1[i, j] / np.sqrt(B_clim1[i, i] * B_clim1[j, j])\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(131)\n",
    "plt.contourf(B_corr1, cmap=\"bwr\", extend=\"both\", levels=np.linspace(-0.95, 0.95, 20))\n",
    "plt.colorbar()\n",
    "plt.title(\"Background correlation matrix 1-scale L96\")\n",
    "plt.subplot(132)\n",
    "plt.contourf(B_loc)\n",
    "plt.colorbar()\n",
    "plt.title(\"B_loc\")\n",
    "plt.subplot(133)\n",
    "plt.contourf(W_clim)\n",
    "plt.colorbar()\n",
    "plt.title(\"W_clim\")\n",
    "\n",
    "del B_loc  # not used below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502fcb30",
   "metadata": {},
   "source": [
    "This is the actual DA algorithm. It steps through segments of time (\"DA cycles\"), launching an ensemble of short forecasts from the posterior estimate of the preceding segment, each perturbed by noise in their initial condition (inflation).\n",
    "\n",
    "Each ensemble trajectory is stored in `ensX`. The increment added to correct the prior is in `X_inc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdecb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up array to store DA increments\n",
    "X_inc = np.zeros(\n",
    "    (int(config[\"ns_da\"] / config[\"DA_freq\"]), config[\"K\"], config[\"nens\"])\n",
    ")\n",
    "if config[\"DA\"] == \"3DVar\":\n",
    "    X_inc = np.squeeze(X_inc)\n",
    "t_DA = np.zeros(int(config[\"ns_da\"] / config[\"DA_freq\"]))\n",
    "\n",
    "# initialize ensemble with perturbations\n",
    "i_t = 0\n",
    "ensX = (\n",
    "    X_init[None, :, None]\n",
    "    + rng.standard_normal((1, config[\"K\"], config[\"nens\"])) * config[\"initial_spread\"]\n",
    ")\n",
    "X_post = ensX[0, ...]\n",
    "\n",
    "W = W_clim\n",
    "\n",
    "# DA cycles\n",
    "for cycle in np.arange(0, config[\"ns_da\"] / config[\"DA_freq\"], dtype=\"int\"):\n",
    "    # set up array to store model forecast for each DA cycle\n",
    "    ensX_fcst = np.zeros((config[\"DA_freq\"] + 1, config[\"K\"], config[\"nens\"]))\n",
    "\n",
    "    # model forecast for next DA cycle\n",
    "    for n in range(config[\"nens\"]):\n",
    "        ensX_fcst[..., n] = GCM(\n",
    "            X_post[0 : config[\"K\"], n],\n",
    "            config[\"F_fcst\"],\n",
    "            config[\"dt\"],\n",
    "            config[\"DA_freq\"],\n",
    "            config[\"GCM_param\"],\n",
    "        )[0]\n",
    "    i_t = i_t + config[\"DA_freq\"]\n",
    "\n",
    "    # get prior/background from the forecast\n",
    "    X_prior = ensX_fcst[-1, ...]  # get prior from model integration\n",
    "\n",
    "    # call DA\n",
    "    t_DA[cycle] = t_truth[i_t]\n",
    "    if config[\"DA\"] == \"EnKF\":\n",
    "        H = ObsOp(config[\"K\"], l_obs, t_obs, i_t)\n",
    "        # augment state vector with parameters when doing parameter estimation\n",
    "        B_ens = np.cov(X_prior)\n",
    "        B_ens_loc = B_ens * W[0 : config[\"K\"], 0 : config[\"K\"]]\n",
    "        X_post = DA_methods.EnKF(X_prior, X_obs[t_obs == i_t], H, R, B_ens_loc)\n",
    "        X_post[0 : config[\"K\"], :] = DA_methods.ens_inflate(\n",
    "            X_post[0 : config[\"K\"], :],\n",
    "            X_prior[0 : config[\"K\"], :],\n",
    "            config[\"inflate_opt\"],\n",
    "            config[\"inflate_factor\"],\n",
    "        )\n",
    "    elif config[\"DA\"] == \"None\":\n",
    "        X_post = X_prior\n",
    "\n",
    "    if not config[\"DA\"] == \"None\":\n",
    "        X_inc[cycle, ...] = (\n",
    "            np.squeeze(X_post[0 : config[\"K\"], ...]) - X_prior[0 : config[\"K\"], ...]\n",
    "        )  # get current increments\n",
    "        # get posterior info about the estimated parameters\n",
    "\n",
    "    # reset initial conditions for next DA cycle\n",
    "    ensX_fcst[-1, :, :] = X_post[0 : config[\"K\"], :]\n",
    "    ensX = np.concatenate((ensX, ensX_fcst[1:None, ...]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbff140",
   "metadata": {},
   "source": [
    "`meanX` is the ensemble mean forecast, averaging over all the ensemble members. It has discontinuities due to the increment add between each DA segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53048cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post processing and visualization\n",
    "meanX = np.mean(ensX, axis=-1)\n",
    "clim = np.max(np.abs(meanX - X_truth[0 : (config[\"ns_da\"] + 1), :]))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "ch = axes[0, 0].contourf(\n",
    "    M_truth.k,\n",
    "    t_truth[0 : (config[\"ns_da\"] + 1)],\n",
    "    meanX - X_truth[0 : (config[\"ns_da\"] + 1), :],\n",
    "    cmap=\"bwr\",\n",
    "    levels=np.arange(-6.5, 7, 1),\n",
    "    extend=\"both\",\n",
    ")\n",
    "plt.colorbar(ch, ax=axes[0, 0], orientation=\"horizontal\")\n",
    "axes[0, 0].set_xlabel(\"s\")\n",
    "axes[0, 0].set_ylabel(\"t\")\n",
    "axes[0, 0].set_title(\"X - X_truth\")\n",
    "axes[0, 1].plot(\n",
    "    t_truth[0 : (config[\"ns_da\"] + 1)],\n",
    "    np.sqrt(((meanX - X_truth[0 : (config[\"ns_da\"] + 1), :]) ** 2).mean(axis=-1)),\n",
    "    label=\"RMSE\",\n",
    ")\n",
    "axes[0, 1].plot(\n",
    "    t_truth[0 : (config[\"ns_da\"] + 1)],\n",
    "    np.mean(np.std(ensX, axis=-1), axis=-1),\n",
    "    label=\"Spread\",\n",
    ")\n",
    "axes[0, 1].plot(\n",
    "    t_truth[0 : (config[\"ns_da\"] + 1)],\n",
    "    config[\"obs_sigma\"] * np.ones((config[\"ns_da\"] + 1)),\n",
    "    label=\"Obs error\",\n",
    ")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_xlabel(\"time\")\n",
    "axes[0, 1].set_title(\"RMSE (X - X_truth)\")\n",
    "axes[0, 1].grid(which=\"both\", linestyle=\"--\")\n",
    "\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k,\n",
    "    np.sqrt(((meanX - X_truth[0 : (config[\"ns_da\"] + 1), :]) ** 2).mean(axis=0)),\n",
    "    label=\"RMSE\",\n",
    ")\n",
    "X_inc_ave = X_inc / config[\"DA_freq\"] / config[\"si\"]\n",
    "axes[0, 2].plot(M_truth.k, X_inc_ave.mean(axis=(0, -1)), label=\"Inc\")\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k, running_ave(X_inc_ave.mean(axis=(0, -1)), 7), label=\"Inc Ave\"\n",
    ")\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k,\n",
    "    np.ones(M_truth.k.shape) * (config[\"F_fcst\"] - config[\"F_truth\"]),\n",
    "    label=\"F_bias\",\n",
    ")\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k,\n",
    "    np.ones(M_truth.k.shape) * (X_inc / config[\"DA_freq\"] / config[\"si\"]).mean(),\n",
    "    \"k:\",\n",
    "    label=\"Ave Inc\",\n",
    ")\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].set_xlabel(\"s\")\n",
    "axes[0, 2].set_title(\"Increments\")\n",
    "axes[0, 2].grid(which=\"both\", linestyle=\"--\")\n",
    "\n",
    "# X_inc_ave=(X_inc/config['DA_freq']/config['si']).mean(axis=(1,2)).\\\n",
    "#         reshape(int(config['ns_da']/ann_period),int(ann_period/config['DA_freq'])).mean(axis=0)\n",
    "# axes[0,2].plot(np.arange(ann_period/config['DA_freq']),X_inc_ave,label='Inc')\n",
    "# axes[0,2].plot(np.arange(ann_period/config['DA_freq']),running_ave(X_inc_ave,10),label='Inc Ave');\n",
    "# axes[0,2].plot(np.arange(0,ann_period/config['DA_freq'],mon_period/config['DA_freq']),\n",
    "#                -2*np.sin(2*np.pi*np.arange(mon_per_ann)/mon_per_ann),label='F_bias')\n",
    "# axes[0,2].legend()\n",
    "# axes[0,2].set_xlabel('\"annual cycle\"'); axes[0,2].set_title('Increments');\n",
    "# axes[0,2].grid(which='both',linestyle='--')\n",
    "\n",
    "plot_start, plot_end = 200, 800\n",
    "plot_start_DA, plot_end_DA = int(plot_start / config[\"DA_freq\"]), int(\n",
    "    plot_end / config[\"DA_freq\"]\n",
    ")\n",
    "plot_x = 0\n",
    "axes[1, 0].plot(\n",
    "    t_truth[plot_start:plot_end], X_truth[plot_start:plot_end, plot_x], label=\"truth\"\n",
    ")\n",
    "axes[1, 0].plot(\n",
    "    t_truth[plot_start:plot_end], meanX[plot_start:plot_end, plot_x], label=\"forecast\"\n",
    ")\n",
    "axes[1, 0].scatter(\n",
    "    t_DA[plot_start_DA - 1 : plot_end_DA - 1],\n",
    "    find_obs(plot_x, X_obs, t_obs, l_obs, [plot_start, plot_end]),\n",
    "    label=\"obs\",\n",
    ")\n",
    "axes[1, 0].grid(which=\"both\", linestyle=\"--\")\n",
    "axes[1, 0].set_xlabel(\"time\")\n",
    "axes[1, 0].set_title(\"k=\" + str(plot_x + 1) + \" truth and forecast\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 2].text(\n",
    "    0.1,\n",
    "    0.1,\n",
    "    \"RMSE={:3f}\\nSpread={:3f}\\nGCM param={}\\nDA={},{}\\nDA_freq={}\\nB_loc={}\\ninflation={},{}\\nobs_density={}\\nobs_sigma={}\\nobs_freq={}\".format(\n",
    "        np.sqrt(((meanX - X_truth[0 : (config[\"ns_da\"] + 1), :]) ** 2).mean()),\n",
    "        np.mean(np.std(ensX, axis=-1)),\n",
    "        config[\"DA\"],\n",
    "        config[\"GCM_param\"],\n",
    "        config[\"nens\"],\n",
    "        config[\"DA_freq\"],\n",
    "        config[\"B_loc\"],\n",
    "        config[\"inflate_opt\"],\n",
    "        config[\"inflate_factor\"],\n",
    "        config[\"obs_density\"],\n",
    "        config[\"obs_sigma\"],\n",
    "        config[\"obs_freq\"],\n",
    "    ),\n",
    "    fontsize=15,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cdeb59",
   "metadata": {},
   "source": [
    "Converting the increment `X_inc` into a tendency, we can examine the relationship between the $\\dot{X}$ due to the missing physics and the state of the model at the beginning of each DA segment. If we are properly correcting the absence of the coupling term then this structure should look like the parameterization of the coupling term, as done in Wilks, 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = np.abs(X_inc_ave[0:, :].flatten()) > -1e-7\n",
    "\n",
    "x_input = ensX[t_obs[0:, 0] - config[\"DA_freq\"], :].flatten()[\n",
    "    jj\n",
    "]  # The offset by DA_freq looks at the previous posterior\n",
    "x_input = 0.5 * (\n",
    "    x_input + ensX[t_obs[0:, 0], :].flatten()[jj]\n",
    ")  # Mid-point of trajectory\n",
    "xinc_output = X_inc_ave[0:, :].flatten()[jj]\n",
    "\n",
    "x = np.linspace(-8, 15, 100)\n",
    "p = np.polyfit(x_input, xinc_output, 4)\n",
    "p18 = [0.000707, -0.0130, -0.0190, 1.59, 0.275]  # Polynomial from Wilks, 2005\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.suptitle(\"All time, all individial k, and all ensemble members\")\n",
    "plt.subplot(121)\n",
    "plt.plot(x_input, xinc_output, \"k.\")\n",
    "plt.grid()\n",
    "plt.plot(x, -np.polyval(p18, x), label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)$\")\n",
    "plt.xlabel(\"Ensemble member $X_i(k,t)$\")\n",
    "plt.ylabel(\"Ensemble member increment $\\dot{X}$\")\n",
    "plt.subplot(122)\n",
    "plt.hist2d(\n",
    "    x_input, xinc_output, bins=(np.linspace(-10, 15, 50), np.linspace(-25, 20, 150))\n",
    ")\n",
    "plt.plot(x, -np.polyval(p18, x), label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)$\")\n",
    "plt.xlabel(\"Ensemble member $X_i(k,t)$\")\n",
    "plt.ylabel(\"Ensemble member increment $\\dot{X}$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = np.abs(X_inc_ave.mean(axis=-1).flatten()) > -1e-7\n",
    "\n",
    "x_input = meanX[t_obs[0:, 0] - config[\"DA_freq\"]].flatten()[\n",
    "    jj\n",
    "]  # The offset by DA_freq looks at the previous posterior\n",
    "x_input = 0.5 * (x_input + meanX[t_obs[0:, 0]].flatten()[jj])  # Mid-point of trajectory\n",
    "xinc_output = X_inc_ave.mean(axis=-1).flatten()[jj]\n",
    "\n",
    "x = np.linspace(-8, 15, 100)\n",
    "p = np.polyfit(x_input, xinc_output, 4)\n",
    "p18 = [0.000707, -0.0130, -0.0190, 1.59, 0.275]  # Polynomial from Wilks, 2005\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.suptitle(\"All time, all individial k, mean over ensemble members\")\n",
    "plt.subplot(121)\n",
    "plt.plot(x_input, xinc_output, \"k.\")\n",
    "plt.grid()\n",
    "plt.plot(x, -np.polyval(p18, x), label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)\")\n",
    "plt.xlabel(\"Ensemble member $X_i(k,t)$\")\n",
    "plt.ylabel(\"Ensemble member increment $\\dot{X}$\")\n",
    "plt.subplot(122)\n",
    "plt.hist2d(\n",
    "    x_input, xinc_output, bins=(np.linspace(-10, 15, 50), np.linspace(-25, 20, 150))\n",
    ")\n",
    "plt.plot(x, -np.polyval(p18, x), label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)\")\n",
    "plt.xlabel(\"Ensemble member $X_i(k,t)$\")\n",
    "plt.ylabel(\"Ensemble member increment $\\dot{X}$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb4c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = 8, 10\n",
    "k = 0\n",
    "si = config[\"DA_freq\"]\n",
    "\n",
    "l = (l_obs == k).max(axis=1)  # True if observation at time t_obs for column k\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.suptitle(\"Ensemble mean, k = %i\" % (k))\n",
    "plt.plot(t_truth, X_truth[:, k], \"--\", label=\"Truth\")\n",
    "plt.fill_between(\n",
    "    t_truth,\n",
    "    meanX[:, k] - ensX[:, k, :].std(axis=-1),\n",
    "    meanX[:, k] + ensX[:, k, :].std(axis=-1),\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=\"Ensemble spread\",\n",
    ")\n",
    "plt.plot(t_truth, meanX[:, k], label=\"Ensemble mean forecast\")\n",
    "plt.plot(\n",
    "    t_truth[t_obs[l, 0]],\n",
    "    meanX[si::si, k][l] - X_inc.mean(axis=-1)[l, k],\n",
    "    \".\",\n",
    "    label=\"Ensemble mean prior\",\n",
    ")\n",
    "plt.plot(t_truth[t_obs[l, 0]], meanX[si::si, k][l], \".\", label=\"Ensemble mean post\")\n",
    "plt.xlim(xl)\n",
    "plt.xlabel(\"Time, t\")\n",
    "plt.ylabel(\"$X(t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a15ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = 8, 10\n",
    "k = 0\n",
    "e = 19\n",
    "si = config[\"DA_freq\"]\n",
    "\n",
    "l = (l_obs == k).max(axis=1)  # True if observation at time t_obs for column k\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.suptitle(\"Ensemble member %i , k = %i\" % (e, k))\n",
    "plt.plot(t_truth, X_truth[:, k], \"--\", label=\"Truth\")\n",
    "plt.plot(t_truth, ensX[:, k, e], label=\"Ensemble member forecast\")\n",
    "plt.plot(\n",
    "    t_truth[t_obs[l, 0]],\n",
    "    ensX[si::si, k, e][l] - X_inc[:, :, e][l, k],\n",
    "    \".\",\n",
    "    label=\"Ensemble member prior\",\n",
    ")\n",
    "plt.plot(t_truth[t_obs[l, 0]], ensX[si::si, k, e][l], \".\", label=\"Ensemble member post\")\n",
    "plt.xlim(xl)\n",
    "plt.xlabel(\"Time, t\")\n",
    "plt.ylabel(\"$X(t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380408db",
   "metadata": {},
   "source": [
    "The above has created the \"DA increments\". Below we wil learn them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ee34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_inc = np.diff(np.load(\"increments.npz\")[\"t_inc\"])[0]\n",
    "t_inc = t_truth[\n",
    "    t_obs[:, 0]\n",
    "]  # These are the times in the \"real world\" when observations were made\n",
    "dt_inc = np.diff(t_inc)[0]  # Time interval between increments\n",
    "# dt = np.diff(np.load(\"increments.npz\")[\"t_truth\"])[0]\n",
    "dt = np.diff(t_truth)[0]  # Time-step of \"real world\" model\n",
    "da_interval = int(dt_inc / dt)\n",
    "\n",
    "# Use only one of the following blocks to load the dataset to model with the NN.\n",
    "\n",
    "# Data from DA system (increments for individual ensemble members)\n",
    "x_input = ensX[:-1:da_interval]\n",
    "X_tend = X_inc / dt_inc\n",
    "\n",
    "# Ensemble mean increments from DA system (1/50th of data above)\n",
    "# x_input = ensX[:-1:da_interval].mean(axis=-1)\n",
    "# X_tend = X_inc.mean(axis=-1) / dt_inc\n",
    "\n",
    "# # Truth tendencies (proxy for LES)\n",
    "# x_input = X_truth\n",
    "# X_tend = -Y_truth.reshape((4001,8,32)).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af4eb68",
   "metadata": {},
   "source": [
    "As a sanity check, we look at the data for obvious structure. A polyfit to the data will compare well to Wilks, 2005, if the data is similar in distribution. We show Wilks, 2005, and the 4th order polyfit for reference, but neither are used or neededin the NN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf0c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple scatter plot of x_tend against x_input\n",
    "x = np.linspace(-7, 14, 100)\n",
    "p = np.polyfit(x_input.flatten(), X_tend.flatten(), 4)\n",
    "p18 = [0.000707, -0.0130, -0.0190, 1.59, 0.275]  # Polynomial from Wilks, 2005\n",
    "plt.plot(x_input.flatten(), X_tend.flatten(), \"k.\")\n",
    "plt.plot(x, -np.polyval(p18, x), label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PDf of x_tend against x_input\n",
    "plt.figure(dpi=150)\n",
    "plt.hist2d(\n",
    "    x_input.flatten(),\n",
    "    X_tend.flatten(),\n",
    "    bins=(np.linspace(-10, 15, 50), np.linspace(-25, 20, 150)),\n",
    "    cmap=plt.cm.Greys,\n",
    ")\n",
    "plt.plot(x, -np.polyval(p18, x), \"k--\", label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)$ lin. regr.\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X$_k$\")\n",
    "plt.ylabel(\"Missing X$_k$ tendency\")\n",
    "plt.title(\"Conventional linear regression\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79538913",
   "metadata": {},
   "source": [
    "Partition the dataset into \"training\" (seen by the network during optimization of the weights), and \"validation\" not seen by the network but used as an independent metric of fit (was called \"test\" in other notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea024c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_input.size // 2\n",
    "train_size = int(0.7 * x_input.size)\n",
    "print(\"Training set size = \", train_size, \" out of \", x_input.size)\n",
    "X_train = x_input.flatten()[:train_size]\n",
    "Y_train = X_tend.flatten()[:train_size]\n",
    "X_valid = x_input.flatten()[train_size:]\n",
    "Y_valid = X_tend.flatten()[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4517f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_torch_dataset_train = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(X_train).double(), torch.from_numpy(Y_train).double()\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 1024  # Number of sample in each batch\n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(\n",
    "    dataset=local_torch_dataset_train, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7178136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_torch_dataset_valid = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(X_valid).double(), torch.from_numpy(Y_valid).double()\n",
    ")\n",
    "\n",
    "loader_valid = torch.utils.data.DataLoader(\n",
    "    dataset=local_torch_dataset_valid, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec20b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network structure in pytorch\n",
    "class Net_ANN(torch.nn.Module):\n",
    "    def __init__(self, W=16):\n",
    "        super(Net_ANN, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(\n",
    "            1, W\n",
    "        )  # 1 inputs, W neurons for first hidden layer\n",
    "        self.linear2 = torch.nn.Linear(W, W)  # W neurons for second hidden layer\n",
    "        #         self.linear2a = torch.nn.Linear(W, W) # W neurons for second hidden layer\n",
    "        #         self.linear2b = torch.nn.Linear(W, W) # W neurons for second hidden layer\n",
    "        self.linear3 = torch.nn.Linear(W, 1)  # 1 outputs\n",
    "\n",
    "    #         self.lin_drop = nn.Dropout(0.1) # regularization method to prevent overfitting.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.linear1(x))\n",
    "        x = torch.nn.functional.relu(self.linear2(x))\n",
    "        #         x = torch.nn.functional.relu(self.linear2a(x))\n",
    "        #         x = torch.nn.functional.relu(self.linear2b(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816fe6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, criterion, trainloader, optimizer):\n",
    "    net.train()\n",
    "    test_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(trainloader):  # for each training step\n",
    "        b_x = torch.autograd.Variable(batch_x)  # Inputs\n",
    "        b_y = torch.autograd.Variable(batch_y)  # outputs\n",
    "        if (\n",
    "            len(b_x.shape) == 1\n",
    "        ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "            prediction = torch.squeeze(\n",
    "                net(torch.unsqueeze(b_x, 1))\n",
    "            )  # input x and predict based on x\n",
    "        else:\n",
    "            prediction = net(b_x)\n",
    "        loss = criterion(prediction, b_y)  # Calculating loss\n",
    "        optimizer.zero_grad()  # clear gradients for next train\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients to update weights\n",
    "\n",
    "\n",
    "def test_model(net, criterion, testloader, optimizer, text=\"validation\"):\n",
    "    net.eval()  # Evaluation mode (important when having dropout layers)\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(testloader):  # for each training step\n",
    "            b_x = torch.autograd.Variable(batch_x)  # Inputs\n",
    "            b_y = torch.autograd.Variable(batch_y)  # outputs\n",
    "            if (\n",
    "                len(b_x.shape) == 1\n",
    "            ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "                prediction = torch.squeeze(\n",
    "                    net(torch.unsqueeze(b_x, 1))\n",
    "                )  # input x and predict based on x\n",
    "            else:\n",
    "                prediction = net(b_x)\n",
    "            loss = criterion(prediction, b_y)  # Calculating loss\n",
    "            test_loss = test_loss + loss.data.numpy()  # Keep track of the loss\n",
    "        test_loss /= len(testloader)  # dividing by the number of batches\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()  # MSE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabfcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(14)  # For reproducibility\n",
    "nn_3l = Net_ANN(W=16).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3966ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 24  # Number of epocs\n",
    "optimizer = torch.optim.Adam(nn_3l.parameters(), lr=0.003)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# train_loss.append(test_model(nn_3l, criterion,loader_train, optimizer, 'train'))\n",
    "# validation_loss.append(test_model(nn_3l, criterion, loader_valid, optimizer))\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(nn_3l, criterion, loader_train, optimizer)\n",
    "    train_loss.append(test_model(nn_3l, criterion, loader_train, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model(nn_3l, criterion, loader_valid, optimizer))\n",
    "    print(\n",
    "        \"Epoch = %3i\" % (epoch),\n",
    "        \"Training loss =\",\n",
    "        train_loss[-1],\n",
    "        \"\\tValidation loss =\",\n",
    "        validation_loss[-1],\n",
    "    )\n",
    "plt.semilogy(train_loss, \"b\", label=\"training loss\")\n",
    "plt.semilogy(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b1e20b",
   "metadata": {},
   "source": [
    "The NN has one input and one output, so we can plot it as a function, $nn(X)$ (orange), and compare it to the polyfit (blue) and Wilks, 2005, polynomial (black dashed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0409ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.hist2d(\n",
    "    x_input.flatten(),\n",
    "    X_tend.flatten(),\n",
    "    bins=(np.linspace(-10, 15, 50), np.linspace(-25, 20, 150)),\n",
    "    cmap=plt.cm.Greys,\n",
    ")\n",
    "plt.plot(x, -np.polyval(p18, x), \"k--\", label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)$ lin. regr.\")\n",
    "plt.plot(\n",
    "    x, (nn_3l(torch.unsqueeze(torch.from_numpy(x), 1))).data.numpy(), label=\"NN-3L\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X$_k$\")\n",
    "plt.ylabel(\"Missing X$_k$ tendency\")\n",
    "plt.title(\"NN fit\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
