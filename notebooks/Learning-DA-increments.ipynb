{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53133bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn\n",
    "import torch.nn.functional\n",
    "import torch.optim\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380408db",
   "metadata": {},
   "source": [
    "This notebook is a derived from that in the `04Subgrid-parametrization-pytorch` directory.\n",
    "- We've restricted it to only using the 3-layer network (not the linear regression model)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74216b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(\"increments.npz\").files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ee34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_inc = np.diff(np.load(\"increments.npz\")[\"t_inc\"])[0]\n",
    "dt = np.diff(np.load(\"increments.npz\")[\"t_truth\"])[0]\n",
    "da_interval = int(dt_inc / dt)\n",
    "\n",
    "# Use only one of the following blocks to load the dataset to model with the NN.\n",
    "\n",
    "# Data from DA system (increments for individual ensemble members)\n",
    "x_input = np.load(\"increments.npz\")[\"ensX\"][:-1:da_interval]\n",
    "X_tend = np.load(\"increments.npz\")[\"X_inc\"] / dt_inc\n",
    "\n",
    "# Ensemble mean increments from DA system (1/50th of data above)\n",
    "# x_input = np.load('increments.npz')['ensX'][:-1:da_interval].mean(axis=-1)\n",
    "# X_tend = np.load('increments.npz')['X_inc'].mean(axis=-1) / dt_inc\n",
    "\n",
    "# # Truth tendencies (proxy for LES)\n",
    "# x_input = np.load('increments.npz')['X_truth']\n",
    "# X_tend = -np.load('increments.npz')['Y_truth'].reshape((4001,8,32)).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af4eb68",
   "metadata": {},
   "source": [
    "As a sanity check, we look at the data for obvious structure. A polyfit to the data will compare well to Wilks, 2005, if the data is similar in distribution. We show Wilks, 2005, and the 4th order polyfit for reference, but neither are used or neededin the NN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf0c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple scatter plot of x_tend against x_input\n",
    "x = np.linspace(-7, 14, 100)\n",
    "p = np.polyfit(x_input.flatten(), X_tend.flatten(), 4)\n",
    "p18 = [0.000707, -0.0130, -0.0190, 1.59, 0.275]  # Polynomial from Wilks, 2005\n",
    "plt.plot(x_input.flatten(), X_tend.flatten(), \"k.\")\n",
    "plt.plot(x, -np.polyval(p18, x), label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PDf of x_tend against x_input\n",
    "plt.figure(dpi=150)\n",
    "plt.hist2d(\n",
    "    x_input.flatten(),\n",
    "    X_tend.flatten(),\n",
    "    bins=(np.linspace(-10, 15, 50), np.linspace(-25, 20, 150)),\n",
    "    cmap=plt.cm.Greys,\n",
    ")\n",
    "plt.plot(x, -np.polyval(p18, x), \"k--\", label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)$ lin. regr.\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X$_k$\")\n",
    "plt.ylabel(\"Missing X$_k$ tendency\")\n",
    "plt.title(\"Conventional linear regression\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79538913",
   "metadata": {},
   "source": [
    "Partition the dataset into \"training\" (seen by the network during optimization of the weights), and \"validation\" not seen by the network but used as an independent metric of fit (was called \"test\" in other notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea024c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_input.size // 2\n",
    "train_size = int(0.7 * x_input.size)\n",
    "print(\"Training set size = \", train_size, \" out of \", x_input.size)\n",
    "X_train = x_input.flatten()[:train_size]\n",
    "Y_train = X_tend.flatten()[:train_size]\n",
    "X_valid = x_input.flatten()[train_size:]\n",
    "Y_valid = X_tend.flatten()[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4517f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_torch_dataset_train = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(X_train).double(), torch.from_numpy(Y_train).double()\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 1024  # Number of sample in each batch\n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(\n",
    "    dataset=local_torch_dataset_train, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7178136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_torch_dataset_valid = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(X_valid).double(), torch.from_numpy(Y_valid).double()\n",
    ")\n",
    "\n",
    "loader_valid = torch.utils.data.DataLoader(\n",
    "    dataset=local_torch_dataset_valid, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec20b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network structure in pytorch\n",
    "class Net_ANN(torch.nn.Module):\n",
    "    def __init__(self, W=16):\n",
    "        super(Net_ANN, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(\n",
    "            1, W\n",
    "        )  # 1 inputs, W neurons for first hidden layer\n",
    "        self.linear2 = torch.nn.Linear(W, W)  # W neurons for second hidden layer\n",
    "        #         self.linear2a = torch.nn.Linear(W, W) # W neurons for second hidden layer\n",
    "        #         self.linear2b = torch.nn.Linear(W, W) # W neurons for second hidden layer\n",
    "        self.linear3 = torch.nn.Linear(W, 1)  # 1 outputs\n",
    "\n",
    "    #         self.lin_drop = nn.Dropout(0.1) # regularization method to prevent overfitting.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.linear1(x))\n",
    "        x = torch.nn.functional.relu(self.linear2(x))\n",
    "        #         x = torch.nn.functional.relu(self.linear2a(x))\n",
    "        #         x = torch.nn.functional.relu(self.linear2b(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816fe6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, criterion, trainloader, optimizer):\n",
    "    net.train()\n",
    "    test_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(trainloader):  # for each training step\n",
    "        b_x = torch.autograd.Variable(batch_x)  # Inputs\n",
    "        b_y = torch.autograd.Variable(batch_y)  # outputs\n",
    "        if (\n",
    "            len(b_x.shape) == 1\n",
    "        ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "            prediction = torch.squeeze(\n",
    "                net(torch.unsqueeze(b_x, 1))\n",
    "            )  # input x and predict based on x\n",
    "        else:\n",
    "            prediction = net(b_x)\n",
    "        loss = criterion(prediction, b_y)  # Calculating loss\n",
    "        optimizer.zero_grad()  # clear gradients for next train\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients to update weights\n",
    "\n",
    "\n",
    "def test_model(net, criterion, testloader, optimizer, text=\"validation\"):\n",
    "    net.eval()  # Evaluation mode (important when having dropout layers)\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(testloader):  # for each training step\n",
    "            b_x = torch.autograd.Variable(batch_x)  # Inputs\n",
    "            b_y = torch.autograd.Variable(batch_y)  # outputs\n",
    "            if (\n",
    "                len(b_x.shape) == 1\n",
    "            ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "                prediction = torch.squeeze(\n",
    "                    net(torch.unsqueeze(b_x, 1))\n",
    "                )  # input x and predict based on x\n",
    "            else:\n",
    "                prediction = net(b_x)\n",
    "            loss = criterion(prediction, b_y)  # Calculating loss\n",
    "            test_loss = test_loss + loss.data.numpy()  # Keep track of the loss\n",
    "        test_loss /= len(testloader)  # dividing by the number of batches\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()  # MSE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabfcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(14)  # For reproducibility\n",
    "nn_3l = Net_ANN(W=16).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3966ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 24  # Number of epocs\n",
    "optimizer = torch.optim.Adam(nn_3l.parameters(), lr=0.003)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# train_loss.append(test_model(nn_3l, criterion,loader_train, optimizer, 'train'))\n",
    "# validation_loss.append(test_model(nn_3l, criterion, loader_valid, optimizer))\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(nn_3l, criterion, loader_train, optimizer)\n",
    "    train_loss.append(test_model(nn_3l, criterion, loader_train, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model(nn_3l, criterion, loader_valid, optimizer))\n",
    "    print(\n",
    "        \"Epoch = %3i\" % (epoch),\n",
    "        \"Training loss =\",\n",
    "        train_loss[-1],\n",
    "        \"\\tValidation loss =\",\n",
    "        validation_loss[-1],\n",
    "    )\n",
    "plt.semilogy(train_loss, \"b\", label=\"training loss\")\n",
    "plt.semilogy(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b1e20b",
   "metadata": {},
   "source": [
    "The NN has one input and one output, so we can plot it as a function, $nn(X)$ (orange), and compare it to the polyfit (blue) and Wilks, 2005, polynomial (black dashed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0409ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.hist2d(\n",
    "    x_input.flatten(),\n",
    "    X_tend.flatten(),\n",
    "    bins=(np.linspace(-10, 15, 50), np.linspace(-25, 20, 150)),\n",
    "    cmap=plt.cm.Greys,\n",
    ")\n",
    "plt.plot(x, -np.polyval(p18, x), \"k--\", label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)$ lin. regr.\")\n",
    "plt.plot(\n",
    "    x, (nn_3l(torch.unsqueeze(torch.from_numpy(x), 1))).data.numpy(), label=\"NN-3L\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X$_k$\")\n",
    "plt.ylabel(\"Missing X$_k$ tendency\")\n",
    "plt.title(\"NN fit\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
